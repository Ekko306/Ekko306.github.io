<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="CNN重读笔记因为后续实验重点要掌握应用图像识别，这里深入归纳一下CNN。 章节框架由上图可见这章，先介绍CNN起源，再介绍卷积神经网络的理论概念，跟着PyTorch的CNN实现，后面是两个拓展：实用的CNN实例与项目，图片增强方法与实例 4.1 主要任务及起源在卷积神经网络流行起来之前，图像处理使用的都是→些传统的方法.比如提取图像中的边缘、纹理、线条、边界等特征，依据这些特征再进行下一步的处理">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN重读笔记">
<meta property="og:url" content="http://yoursite.com/2019/10/18/CNN重读笔记/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="CNN重读笔记因为后续实验重点要掌握应用图像识别，这里深入归纳一下CNN。 章节框架由上图可见这章，先介绍CNN起源，再介绍卷积神经网络的理论概念，跟着PyTorch的CNN实现，后面是两个拓展：实用的CNN实例与项目，图片增强方法与实例 4.1 主要任务及起源在卷积神经网络流行起来之前，图像处理使用的都是→些传统的方法.比如提取图像中的边缘、纹理、线条、边界等特征，依据这些特征再进行下一步的处理">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/CNN%E6%A1%86%E6%9E%B6.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/diff.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/CNN%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/%E5%85%A8%E8%BF%9E%E6%8E%A5.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/%E6%BB%A4%E6%B3%A2%E5%99%A8.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/solution.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/%E6%80%BB%E7%BB%93.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/pooling.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/77v33.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/size.png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/nn.Conv2d().png">
<meta property="og:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/nn.MaxPool2d().png">
<meta property="og:updated_time" content="2019-10-19T07:41:12.211Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CNN重读笔记">
<meta name="twitter:description" content="CNN重读笔记因为后续实验重点要掌握应用图像识别，这里深入归纳一下CNN。 章节框架由上图可见这章，先介绍CNN起源，再介绍卷积神经网络的理论概念，跟着PyTorch的CNN实现，后面是两个拓展：实用的CNN实例与项目，图片增强方法与实例 4.1 主要任务及起源在卷积神经网络流行起来之前，图像处理使用的都是→些传统的方法.比如提取图像中的边缘、纹理、线条、边界等特征，依据这些特征再进行下一步的处理">
<meta name="twitter:image" content="http://yoursite.com/2019/10/18/CNN重读笔记/CNN%E6%A1%86%E6%9E%B6.png">
  <link rel="canonical" href="http://yoursite.com/2019/10/18/CNN重读笔记/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>CNN重读笔记 | Hexo</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/18/CNN重读笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="望星的太阳花">
      <meta itemprop="description" content="You are my JavaSript in my HTML.">
      <meta itemprop="image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1570163730249&di=dcd36b04d1066a90ddb1f132ae3a6bcc&imgtype=0&src=http%3A%2F%2Fhbimg.b0.upaiyun.com%2Ffe60497fd762440686b6d5702c2c9f19df71fb9911009-LVWEJj_fw658">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">CNN重读笔记

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-10-18 15:16:55" itemprop="dateCreated datePublished" datetime="2019-10-18T15:16:55+08:00">2019-10-18</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-19 15:41:12" itemprop="dateModified" datetime="2019-10-19T15:41:12+08:00">2019-10-19</time>
              </span>
            
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="CNN重读笔记"><a href="#CNN重读笔记" class="headerlink" title="CNN重读笔记"></a>CNN重读笔记</h1><p>因为后续实验重点要掌握应用图像识别，这里深入归纳一下CNN。</p>
<h2 id="章节框架"><a href="#章节框架" class="headerlink" title="章节框架"></a>章节框架</h2><p><img src="/2019/10/18/CNN重读笔记/CNN%E6%A1%86%E6%9E%B6.png" alt="CNN框架"><br>由上图可见这章，先介绍CNN起源，再介绍卷积神经网络的理论概念，跟着PyTorch的CNN实现，后面是两个拓展：实用的CNN实例与项目，图片增强方法与实例</p>
<h2 id="4-1-主要任务及起源"><a href="#4-1-主要任务及起源" class="headerlink" title="4.1 主要任务及起源"></a>4.1 主要任务及起源</h2><p>在卷积神经网络流行起来之前，图像处理使用的都是→些传统的方法.比如提取图像中的边缘、纹理、线条、边界等特征，依据这些特征再进行下一步的处理，这样的处理不仅效率特别低，准确率也不高。 归功于卷积神经网络。</p>
<h2 id="4-2-卷积神经网络的原理和结构"><a href="#4-2-卷积神经网络的原理和结构" class="headerlink" title="4.2 卷积神经网络的原理和结构"></a>4.2 卷积神经网络的原理和结构</h2><h3 id="4-2-0-概述"><a href="#4-2-0-概述" class="headerlink" title="4.2.0 概述"></a>4.2.0 概述</h3><p>三个观点让卷积神经网络真正起作用</p>
<ol>
<li>局部性：特征不是整张图片决定的，而是一些局部的特征决定的，如看鸟看鸟嘴。</li>
<li>相同性：特征出现在不同位置，但是决定相同的特征机制是一样的，如不同鸟都有鸟嘴。</li>
<li>不变性：一张大图片，进行图片缩小也不会影响图片的性质。<hr>
</li>
</ol>
<ul>
<li><strong>卷积神经网络与全连接神经网络不同：</strong><ul>
<li><strong>全连接网络神经网络</strong>由一系列隐藏层构成，每个隐藏层由若干神经元构成，其中每个神经元都和前一层的所有神经元相关联，但每一层的神经元是相互独立的。<br><img src="/2019/10/18/CNN重读笔记/diff.png" alt="diff"></li>
<li><strong>卷积神经网络</strong>不同于一般的全连接神经网络，卷积神经网络是一个3D容量的神经元，通过宽度，高度和深度来排列。主要层结构有三个:卷积层、池化层和全连接层，通过堆叠这些层结构形成了一个完整的卷积神经网络结构。<br><img src="/2019/10/18/CNN重读笔记/CNN%E7%BB%93%E6%9E%84.png" alt="CNN结构"></li>
</ul>
</li>
</ul>
<h3 id="4-2-1-卷积层"><a href="#4-2-1-卷积层" class="headerlink" title="4.2.1 卷积层"></a>4.2.1 卷积层</h3><p><strong><em>这里插入题外思考，想了半天不理解为啥卷积神经网络是这个样子，其实就是长宽只扫描3x3或者5x5实际上有30x30或者更大，然后零层可以让输入的个数等于输出的个数，这只是一层卷积层，这个里面包含了滤波器就是一小块，滤波器的参数是可以学习的，而全连接神经网络就是可以将所有的输出都输入给下一个层，当然层数越复杂 模型可以学习的参数也越多 拟合效果越好，时间越长。卷积神经网络只提取部分的，然后分析。阿哈 终于懂了</em></strong><br><img src="/2019/10/18/CNN重读笔记/%E5%85%A8%E8%BF%9E%E6%8E%A5.png" alt="全连接"></p>
<hr>

<ul>
<li><p>要点</p>
<ul>
<li>局部连接 只获取局部的参数（图像局部性）</li>
<li>空间排列 输出深度、滑动步长，以及边界填充控制着卷积层的空间排布。</li>
<li>零填充的使用 来让输入大小等于输出大小</li>
<li>步长 配合与零填充使用</li>
<li><strong>参数共享</strong>：单个滤波器可以扫描到相同的特征（图像相同性）,就可以把参数减少，如下。这里相同性也不总是有用的，比如人脸识别就需要某个位置有某个特征与位置有关了。<hr>
</li>
</ul>
<p> <strong><em>问题</em></strong></p>
<p> <img src="/2019/10/18/CNN重读笔记/%E6%BB%A4%E6%B3%A2%E5%99%A8.png" alt="滤波器"></p>
 <hr>

<p> <img src="/2019/10/18/CNN重读笔记/solution.png" alt="solution"><br> <strong><em>上面又想半天，滤波器就是提取激活学习的层数，32个层数是自己指定的，你可以任意增加滤波器的个数结果输出图片的深度会增加，然后3x3x10是单个滤波器扫描得到的参数，这个10要和输入图片的深度一致，如上图输入6x6x3，输出4x4x2，然后开头的20x20就是输入的图片大小可以指定3x3参数合适让输出也等于20x20</em></strong></p>
<hr>

</li>
</ul>
<p><img src="/2019/10/18/CNN重读笔记/%E6%80%BB%E7%BB%93.png" alt="总结"></p>
<h3 id="4-2-2-池化层"><a href="#4-2-2-池化层" class="headerlink" title="4.2.2 池化层"></a>4.2.2 池化层</h3><blockquote>
<p>池化层作用是逐渐降低数据体的空间尺 寸.这样就能够减少网络中参数的数量 .减少计算资服耗费， 同时也能够有效地控制过 拟合。</p>
</blockquote>
<p>依据图片的不变性，将一个层的大小变小也不会改变值。</p>
<p><strong>总结</strong><br><img src="/2019/10/18/CNN重读笔记/pooling.png" alt="pooling"></p>
<h3 id="4-2-3-全连接层"><a href="#4-2-3-全连接层" class="headerlink" title="4.2.3 全连接层"></a>4.2.3 全连接层</h3><p>一般经过卷积层和池化层之后再经过一个全连接层，将3D的立方体重新排列变成全全连接层，再经过几个隐藏层，最后得到结果。<br>一般会添加dropout防止过拟合</p>
<h3 id="4-2-4-卷积神经网络的基本形式"><a href="#4-2-4-卷积神经网络的基本形式" class="headerlink" title="4.2.4 卷积神经网络的基本形式"></a>4.2.4 卷积神经网络的基本形式</h3><p>基本由以上三个层构成，还会添加激活函数来增加非线性，也会增加批标准化等权重初始化的函数等等来缩小图像，最后全连接层展开，一般结构：<br><strong><em>卷积层&rarr;ReLU层&rarr;批标准化层&rarr;池化层&rarr;…&rarr;全连接层</em></strong><br><strong>1，小滤波器的有效性</strong><br><img src="/2019/10/18/CNN重读笔记/77v33.png" alt="77v33"><br>多个小卷积层比单个大卷积层效果好，上图同时感受野同是7x7，大的有7x7xCxC=49C<sup>2</sup>，小的有3x(3x3)xCxC=49C<sup>2</sup>，参数变小。同时多个小卷积层增加了复杂度。<br><strong>2，网络的尺寸</strong><br>根据经验会有一些规则无严格数学证明。<br><img src="/2019/10/18/CNN重读笔记/size.png" alt="size"></p>
<h2 id="4-3-PyTorch卷积模块"><a href="#4-3-PyTorch卷积模块" class="headerlink" title="4.3 PyTorch卷积模块"></a>4.3 PyTorch卷积模块</h2><p>在nn这个模块包中调用卷积神经的层结构</p>
<h3 id="4-3-1-卷积层"><a href="#4-3-1-卷积层" class="headerlink" title="4.3.1 卷积层"></a>4.3.1 卷积层</h3><p><img src="/2019/10/18/CNN重读笔记/nn.Conv2d().png" alt="nn.Conv2d()"></p>
<h3 id="4-3-2-池化层"><a href="#4-3-2-池化层" class="headerlink" title="4.3.2 池化层"></a>4.3.2 池化层</h3><p><img src="/2019/10/18/CNN重读笔记/nn.MaxPool2d().png" alt="nn.MaxPool2d()"><br>构造简单的卷积神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(SimpleCNN, self).__init__()</span><br><span class="line">        layer1 = nn.Sequential()</span><br><span class="line">        layer1.add_module(<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>)) <span class="comment"># b, 32, 32, 32</span></span><br><span class="line">        layer1.add_module(<span class="string">'relu1'</span>, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer1.add_module(<span class="string">'pool1'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#b, 32, 16, 16</span></span><br><span class="line">        self.layer1 = layer1</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        layer2 = nn.Sequential()</span><br><span class="line">        layer2.add_module(<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>)) <span class="comment">#b, 64, 16, 16</span></span><br><span class="line">        layer2.add_module(<span class="string">'relu2'</span>, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer2.add_module(<span class="string">'pool2'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#b, 64, 8, 8</span></span><br><span class="line">        self.layer2 = layer2</span><br><span class="line">        </span><br><span class="line">        layer3 = nn.Sequential()</span><br><span class="line">        layer3.add_module(<span class="string">'conv3'</span>, nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, padding=<span class="number">1</span>)) <span class="comment"># 128, 8, 8</span></span><br><span class="line">        layer3.add_module(<span class="string">'relu3'</span>, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer3.add_module(<span class="string">'pool3'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#b, 128, 4, 4</span></span><br><span class="line">        self.layer3 = layer3</span><br><span class="line">        </span><br><span class="line">        layer4 = nn.Sequential()</span><br><span class="line">        layer4.add_module(<span class="string">'fc1'</span>, nn.Linear(<span class="number">2048</span>, <span class="number">512</span>))</span><br><span class="line">        layer4.add_module(<span class="string">'fc_relu1'</span>, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer4.add_module(<span class="string">'fc2'</span>, nn.Linear(<span class="number">512</span>, <span class="number">64</span>))</span><br><span class="line">        layer4.add_module(<span class="string">'fc_relu2'</span>, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer4.add_module(<span class="string">'fc3'</span>, nn.Linear(<span class="number">64</span>, <span class="number">10</span>))</span><br><span class="line">        self.layer4 = layer4</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        conv1 = self.layer1(x)</span><br><span class="line">        conv2 = self.layer2(conv1)</span><br><span class="line">        conv3 = self.layer3(conv2)</span><br><span class="line">        fc_input = conv3.view(conv3.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        fc_out = self.layer4(fc_input)</span><br><span class="line">        <span class="keyword">return</span> fc_out</span><br><span class="line">model = SimpleCNN()</span><br></pre></td></tr></table></figure>

<p>概括上面：将卷积层、激活层和池化层组合乘一个层结构，定义3个这样层结构，最后定义全连接层，输出10.<br>也可以在forward添加输出中间层观察中间层输出。<br>还可以print(model)显示哪些层结构</p>
<h3 id="4-3-3-提取层结构"><a href="#4-3-3-提取层结构" class="headerlink" title="4.3.3 提取层结构"></a>4.3.3 提取层结构</h3><p>对于一个给定的模型，如果不想要模型中所有的层结构，只希望提取网络中某一层或者几层。</p>
<ul>
<li>nn.Module几个重要属性<ul>
<li>nn.Module.children() 返回下一级模块的迭代器，只会返回layer1,2,3,4</li>
<li>nn.Module.modules() 返回所有模块的迭代器，好处是能访问到最内层，比如self.layer1.conv1</li>
<li>与以上两个对应的下面两个，不仅会返回模块迭代器，还会返回网路层的名字<ul>
<li>nn.Module.named_children()</li>
<li>nn.Module.named_modules()<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">new_module = nn.Sequential(*list(model.children())[:<span class="number">2</span>]) <span class="comment">#提取模型前两层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.named_modules():</span><br><span class="line">    <span class="keyword">if</span> isinstance(layer[<span class="number">1</span>], nn.Conv2d): <span class="comment"># isinstance判断是不是需要的类型实例</span></span><br><span class="line">        conv_model.add_module(layer[<span class="number">0</span>], layer[<span class="number">1</span>])  <span class="comment">#提取除所有的卷积模块(内层的)</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-3-4-如何提取参数及自定义初始化"><a href="#4-3-4-如何提取参数及自定义初始化" class="headerlink" title="4.3.4 如何提取参数及自定义初始化"></a>4.3.4 如何提取参数及自定义初始化</h3><p>提取层结构不够还需要对<strong>参数初始化</strong></p>
<ul>
<li>nn.Module特别重要关于参数的属性<ul>
<li>nn.Module.named_parameters() 给出网络层的名字和参数的迭代器</li>
<li>nn.Module.parameters() 给出一个网络的全部参数的迭代器<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(param[<span class="number">0</span>]) <span class="comment">#得到每一层参数的名字</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p>对<strong>权重初始化</strong>，因为权重是一个Variable，只需要提取data属性再处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">    <span class="keyword">if</span> isinstane(m, nn.Conv2d):</span><br><span class="line">        init.normal(m.weight.data)</span><br><span class="line">        init.xavier_normal(m.weight.data)</span><br><span class="line">        init.kaiming_normal(m.weight.data)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> isinstane(m, nn.Linear):</span><br><span class="line">        m.weight.data.normal_()</span><br></pre></td></tr></table></figure>

<p><strong>上面参数就是权重，一种是parameter提取出名字，另一种是直接调用.weight属性修改</strong></p>
<h2 id="4-4-卷积神经网络案例分析"><a href="#4-4-卷积神经网络案例分析" class="headerlink" title="4.4 卷积神经网络案例分析"></a>4.4 卷积神经网络案例分析</h2><h3 id="4-4-1-LeNeT"><a href="#4-4-1-LeNeT" class="headerlink" title="4.4.1 LeNeT"></a>4.4.1 LeNeT</h3><blockquote>
<p>整个卷积神经网络的开山之作，1998年提出，结构简单，一共7层，其中2层卷积层和2层池化层教徒出现，最后三层全连接层得到整体的结果。层数很浅，没有添加激活层</p>
</blockquote>
<h3 id="4-4-2-AlexNet"><a href="#4-4-2-AlexNet" class="headerlink" title="4.4.2 AlexNet"></a>4.4.2 AlexNet</h3><blockquote>
<p>2012年在ImageNet竞赛上大放异彩的AlexNet。结构复杂因为当时GPU性能不好用了两个，现在可以用一个GPU替代。AlexNet网络层数更深，第一次引入激活层ReLu,全连接层引入Dropout层防止过拟合。</p>
</blockquote>
<h3 id="4-4-3-VGGNet"><a href="#4-4-3-VGGNet" class="headerlink" title="4.4.3 VGGNet"></a>4.4.3 VGGNet</h3><blockquote>
<p>2014年ImageNet竞赛亚军，总结用了更小的滤波器，更深的结构，AlexNet只有8层，而VGGNet有16层～19层，这就是运用多个小滤波器减少参数。</p>
</blockquote>
<h3 id="4-4-4-GoogLeNet"><a href="#4-4-4-GoogLeNet" class="headerlink" title="4.4.4 GoogLeNet"></a>4.4.4 GoogLeNet</h3><blockquote>
<p>2014年提出，如今进化到v4版本，最核心部分采取了比VGGNet更深的结构，一共有22层，但它的参数比AlexNet少了12倍，同时有很高的计算效率，因为采用了很有效的Inception模块，没有全连接层，是2014年比赛冠军。<br>Inception模块设计了一个局部的网络拓扑结构，然后将这些模块对别一起形成抽象层网络结构，具体来说是几个并行的滤波器对输入进行卷积和池化，这些滤波器有不同的感受野，最后按深度拼接一起形成输出层。<br>问题是参数较多，对这个版本有了Inception模块新设计。</p>
</blockquote>
<h3 id="4-4-5-ResNet"><a href="#4-4-5-ResNet" class="headerlink" title="4.4.5 ResNet"></a>4.4.5 ResNet</h3><ul>
<li>误差:即观测值与真实值的偏离;</li>
<li>残差:观测值与拟合值的偏离.<blockquote>
<p>2015年ImageNet竞赛冠军，由微软研究院提出，通过残差模块能欧成功地训练高达152层深的神经网络<br>残差学习单元相当于将学习莫表改变了，不再是学习一个完整的输出h(x)，而是学习输出和输入的差别H(x)-x，即残差。</p>
</blockquote>
</li>
</ul>
<p><strong><em>以上的模型都定义在torchvision里面，也有预训练好的参数，这些预训练好的网络为后面介绍的前一学习和微调做了很好的铺垫。</em></strong></p>
<h2 id="4-5-再实现MNIST手写数字分类"><a href="#4-5-再实现MNIST手写数字分类" class="headerlink" title="4.5 再实现MNIST手写数字分类"></a>4.5 再实现MNIST手写数字分类</h2><p>成功跑出了MNIST分类模型，挺激动的，就是电脑cpu太慢了，训练半天。</p>
<h2 id="4-6-图像增强的方法"><a href="#4-6-图像增强的方法" class="headerlink" title="4.6 图像增强的方法"></a>4.6 图像增强的方法</h2><p>前面专注于CNN的层结构，现在从另外的角度——图像增强的方面入手，提高模型的准确率和泛化能力。</p>
<ul>
<li>影响图像的因素<ul>
<li>拍摄的光照强度</li>
<li>物体的姿势</li>
<li>是否有遮蔽物</li>
</ul>
</li>
</ul>
<p><strong><em>针对这些问题，Torchvision.transforms包括所有图像增强的方法</em></strong></p>
<ul>
<li>torchvision.transforms图像增强的方法.<ol>
<li>Scale，对图片的尺度进行缩小和放大；</li>
<li>CenterCrop，对图片正中心进行给定大小的裁剪；</li>
<li>RandomCrop，对图片进行给定大小的随机裁剪；</li>
<li>RandomHorizaontalFlip，对图片进行概率为0.5的随机水平反转；</li>
<li>RandomSizedCrop，首先对图片进行随机尺寸的裁剪，然后对裁剪的图片进行一个随机比例的缩放，最后将图片边尘给定的大小，在InceptionNet中比较流行</li>
<li>pad对图片进行边界零填充<br>还有其他方法，可以用OpenCV或者PIL等第三方图形库实现。<h2 id="4-7-实现cifar10分类"><a href="#4-7-实现cifar10分类" class="headerlink" title="4.7 实现cifar10分类"></a>4.7 实现cifar10分类</h2></li>
</ol>
</li>
</ul>
<p><strong><em>下个星期再重点学习，先复习考试。</em></strong></p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/10/13/Pytorch文档阅读笔记/" rel="next" title="Pytorch文档阅读笔记">
                  <i class="fa fa-chevron-left"></i> Pytorch文档阅读笔记
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/11/03/OpenCV3计算机视觉阅读笔记/" rel="prev" title="OpenCV3计算机视觉阅读笔记">
                  OpenCV3计算机视觉阅读笔记 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN重读笔记"><span class="nav-number">1.</span> <span class="nav-text">CNN重读笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#章节框架"><span class="nav-number">1.1.</span> <span class="nav-text">章节框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-主要任务及起源"><span class="nav-number">1.2.</span> <span class="nav-text">4.1 主要任务及起源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-卷积神经网络的原理和结构"><span class="nav-number">1.3.</span> <span class="nav-text">4.2 卷积神经网络的原理和结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-0-概述"><span class="nav-number">1.3.1.</span> <span class="nav-text">4.2.0 概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-卷积层"><span class="nav-number">1.3.2.</span> <span class="nav-text">4.2.1 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-池化层"><span class="nav-number">1.3.3.</span> <span class="nav-text">4.2.2 池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-全连接层"><span class="nav-number">1.3.4.</span> <span class="nav-text">4.2.3 全连接层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-4-卷积神经网络的基本形式"><span class="nav-number">1.3.5.</span> <span class="nav-text">4.2.4 卷积神经网络的基本形式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-PyTorch卷积模块"><span class="nav-number">1.4.</span> <span class="nav-text">4.3 PyTorch卷积模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-卷积层"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.3.1 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-2-池化层"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.3.2 池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-3-提取层结构"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3.3 提取层结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-4-如何提取参数及自定义初始化"><span class="nav-number">1.4.4.</span> <span class="nav-text">4.3.4 如何提取参数及自定义初始化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-卷积神经网络案例分析"><span class="nav-number">1.5.</span> <span class="nav-text">4.4 卷积神经网络案例分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-1-LeNeT"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.4.1 LeNeT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-2-AlexNet"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.4.2 AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-3-VGGNet"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.4.3 VGGNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-4-GoogLeNet"><span class="nav-number">1.5.4.</span> <span class="nav-text">4.4.4 GoogLeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-5-ResNet"><span class="nav-number">1.5.5.</span> <span class="nav-text">4.4.5 ResNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-再实现MNIST手写数字分类"><span class="nav-number">1.6.</span> <span class="nav-text">4.5 再实现MNIST手写数字分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-图像增强的方法"><span class="nav-number">1.7.</span> <span class="nav-text">4.6 图像增强的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-实现cifar10分类"><span class="nav-number">1.8.</span> <span class="nav-text">4.7 实现cifar10分类</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1570163730249&di=dcd36b04d1066a90ddb1f132ae3a6bcc&imgtype=0&src=http%3A%2F%2Fhbimg.b0.upaiyun.com%2Ffe60497fd762440686b6d5702c2c9f19df71fb9911009-LVWEJj_fw658"
      alt="望星的太阳花">
  <p class="site-author-name" itemprop="name">望星的太阳花</p>
  <div class="site-description" itemprop="description">You are my JavaSript in my HTML.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">望星的太阳花</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.1</div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  

  

</body>
</html>
