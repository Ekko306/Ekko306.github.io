<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="动手学深度学习第2章 预备知识2.2 数据操作主要学习用MXNet包，好像和Numpy有相似的地方，但好像又有区别，之后总结。很基础 12345678910111213141516171819202122from mxnet import ndx=nd.arrange(12)x.shapex.size  #与shape不同 size得到实例中元素个数x.reshape((3, 4)) #x.res">
<meta property="og:type" content="article">
<meta property="og:title" content="dive into deep learning">
<meta property="og:url" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="动手学深度学习第2章 预备知识2.2 数据操作主要学习用MXNet包，好像和Numpy有相似的地方，但好像又有区别，之后总结。很基础 12345678910111213141516171819202122from mxnet import ndx=nd.arrange(12)x.shapex.size  #与shape不同 size得到实例中元素个数x.reshape((3, 4)) #x.res">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/cnn.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/conv.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/opt.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/gd1.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/gd2.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/gd3.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/nd.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/mom1.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/mom2.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/mom3.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/ac.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/wait.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/cv.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/anchor.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/dog.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/multi.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/ssd.png">
<meta property="og:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/fcn.png">
<meta property="og:updated_time" content="2020-02-11T04:05:58.498Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="dive into deep learning">
<meta name="twitter:description" content="动手学深度学习第2章 预备知识2.2 数据操作主要学习用MXNet包，好像和Numpy有相似的地方，但好像又有区别，之后总结。很基础 12345678910111213141516171819202122from mxnet import ndx=nd.arrange(12)x.shapex.size  #与shape不同 size得到实例中元素个数x.reshape((3, 4)) #x.res">
<meta name="twitter:image" content="http://yoursite.com/2020/01/22/dive-into-deep-learning/cnn.png">
  <link rel="canonical" href="http://yoursite.com/2020/01/22/dive-into-deep-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>dive into deep learning | Hexo</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/22/dive-into-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="望星的太阳花">
      <meta itemprop="description" content="You are my JavaSript in my HTML.">
      <meta itemprop="image" content="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1570163730249&di=dcd36b04d1066a90ddb1f132ae3a6bcc&imgtype=0&src=http%3A%2F%2Fhbimg.b0.upaiyun.com%2Ffe60497fd762440686b6d5702c2c9f19df71fb9911009-LVWEJj_fw658">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">dive into deep learning

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-01-22 15:02:08" itemprop="dateCreated datePublished" datetime="2020-01-22T15:02:08+08:00">2020-01-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-11 12:05:58" itemprop="dateModified" datetime="2020-02-11T12:05:58+08:00">2020-02-11</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep-Learning</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="动手学深度学习"><a href="#动手学深度学习" class="headerlink" title="动手学深度学习"></a>动手学深度学习</h1><h2 id="第2章-预备知识"><a href="#第2章-预备知识" class="headerlink" title="第2章 预备知识"></a>第2章 预备知识</h2><h3 id="2-2-数据操作"><a href="#2-2-数据操作" class="headerlink" title="2.2 数据操作"></a>2.2 数据操作</h3><p>主要学习用MXNet包，好像和Numpy有相似的地方，但好像又有区别，之后总结。<br>很基础</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line">x=nd.arrange(<span class="number">12</span>)</span><br><span class="line">x.shape</span><br><span class="line">x.size  <span class="comment">#与shape不同 size得到实例中元素个数</span></span><br><span class="line">x.reshape((<span class="number">3</span>, <span class="number">4</span>)) <span class="comment">#x.reshape((-1, 4)自动</span></span><br><span class="line">nd.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">nd.ones(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">#通过python的列表置顶需要创建的NDArray中每个元素的值</span></span><br><span class="line">Y = nd.array([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">nd.random.normal(<span class="number">0</span>, <span class="number">1</span>, shape=(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment">#正态分布</span></span><br><span class="line">X + Y</span><br><span class="line">X * Y</span><br><span class="line">X / Y</span><br><span class="line">Y.exp()</span><br><span class="line">nd.dot(X, Y.T)  <span class="comment">#Y.T矩阵转置</span></span><br><span class="line"><span class="comment">#将多个矩阵联结 </span></span><br><span class="line">nd.concat(X, Y, dim=<span class="number">0</span>) <span class="comment">#维度0 在行上连结</span></span><br><span class="line">nd.concat(X, Y, dim=<span class="number">1</span>) <span class="comment">#维度1 在列上连结</span></span><br><span class="line">x==y <span class="comment">#判断元素中相同的部分</span></span><br><span class="line">X.sum()</span><br><span class="line">X.norm().asscalar() <span class="comment">#将结果转换成Python中的标量。.norm()计算矩阵范数</span></span><br><span class="line">Y.exp()=nd.exp(Y)</span><br></pre></td></tr></table></figure>

<p><strong>广播机制</strong><br>前面是形状相同的NDArray做按元素运算。当对两个形状不同的NDArray按元素运算时，可能出发广播机制：线适当复制元素使这两个NDArray形状相同后再按元素运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = nd.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">B = nd.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">A + B</span><br></pre></td></tr></table></figure>

<p><strong>索引</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">x[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">X[<span class="number">1</span>:<span class="number">2</span>, :] = <span class="number">12</span> <span class="comment">#截取一部分重新赋值</span></span><br><span class="line"><span class="comment">#[[ 0.  1.  2.  3.]</span></span><br><span class="line"><span class="comment">#[12. 12. 12. 12.]</span></span><br><span class="line"><span class="comment">#[ 8.  9. 10. 11.]]</span></span><br></pre></td></tr></table></figure>

<p><strong>运算的内存开销</strong><br>这个很酷，对效率有关<br>前面的例子里我们对每个操作新开内存来存储运算结果。举个例子，像<code>Y=X+Y</code>这样的运算也会新开内存，然后将Y指向新内存。可以用python自带id函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">before = id(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line">before, id(Y)</span><br><span class="line"><span class="comment">#(112511066600, 112511066400)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#指定特定内存，可以用前面的索引</span></span><br><span class="line">Z = Y.zeros_like() </span><br><span class="line">before = id(Z)</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"></span><br><span class="line"><span class="comment">#实际上上面我们为了计算X+Y还是开了临时内存来存储计算结果，再复制到z对应的内存。为了避免这个临时内存开销，我们可以使用运算符全名函数汇总的out参数</span></span><br><span class="line">nd.elemwise_add(X, Y, out=Z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果X的值在之后的程序中不会复用，我们也可以用X[:] = X + Y或者 X+=Y来减少运算的内存开销</span></span><br><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before <span class="comment">#True</span></span><br></pre></td></tr></table></figure>

<p><strong>NDArray和NumPy相互变换</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">P = np.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">D = nd.array(P)  <span class="comment">#numpy转换成NDArray</span></span><br><span class="line"></span><br><span class="line">D.asnumpy()  <span class="comment">#NDArray转换成numpy</span></span><br></pre></td></tr></table></figure>

<p><strong>so easy！ 妈妈再也不用担心我的学习！</strong></p>
<h3 id="2-3-自动求梯度"><a href="#2-3-自动求梯度" class="headerlink" title="2.3 自动求梯度"></a>2.3 自动求梯度</h3><p>深度学习中我们经常求梯度。本节介绍如何使用MXNet提供的autograd模块自动求梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd <span class="comment">#自动求梯度的包  </span></span><br><span class="line">x = nd.arange(<span class="number">4</span>).reshape((<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">x.attach_grad() <span class="comment">#绑定求x值</span></span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    y = <span class="number">2</span> * nd.dot(x.T, x)</span><br><span class="line">y.backward()</span><br><span class="line"><span class="keyword">assert</span> (x.grad - <span class="number">4</span> * x).norm().asscalar() == <span class="number">0</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<p>用了autograd将预测模式转换为训练模式（预测模式不需要记录梯度？）某些情况下，同一个模型在训练模式和预测模式下的行为并不相同。我们在后面的章节详细介绍区别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(autograd.is_training()) <span class="comment"># False</span></span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    print(autograd.is_training()) <span class="comment">#True</span></span><br></pre></td></tr></table></figure>

<p>还有鬼东西，对python控制流求梯度<br>即使函数的计算图包含了Python的控制流（如条件和循环控制），我们也可能对变量求梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(a)</span>:</span></span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm().asscalar() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.sum().asscalar() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = nd.random.normal(shape=<span class="number">1</span>)</span><br><span class="line">a.attach_grad()</span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    c = f(a)</span><br><span class="line">c.backward()</span><br><span class="line"></span><br><span class="line">a.grad == c / a</span><br></pre></td></tr></table></figure>

<p>easy!就是说明了啥python流的情况也适用</p>
<h2 id="第3章-深度学习基础"><a href="#第3章-深度学习基础" class="headerlink" title="第3章 深度学习基础"></a>第3章 深度学习基础</h2><h3 id="3-1-线性回归-（全概念）"><a href="#3-1-线性回归-（全概念）" class="headerlink" title="3.1 线性回归 （全概念）"></a>3.1 线性回归 （全概念）</h3><p>区分回归与分类</p>
<h4 id="3-1-1-线性回归的基本要素"><a href="#3-1-1-线性回归的基本要素" class="headerlink" title="3.1.1 线性回归的基本要素"></a>3.1.1 线性回归的基本要素</h4><ul>
<li>1.模型</li>
<li>2.模型训练<ul>
<li>1.训练数据<br>训练集(training set)<br>样本(sample)<br>标签(label)<br>特征(feature)</li>
<li>2.损失函数</li>
<li>3.优化算法</li>
</ul>
</li>
<li>3.模型预测<h4 id="3-1-2-线性回归的表示方法"><a href="#3-1-2-线性回归的表示方法" class="headerlink" title="3.1.2 线性回归的表示方法"></a>3.1.2 线性回归的表示方法</h4></li>
</ul>
<p><strong>1.神经网络图</strong><br>深度学习中，我们可以使用神经网络图直观地表现模型结构。<br>线性回归是一个单层的神经网络。<br>输入层<br>输出层：全连接层或稠密层<br><strong>2.矢量计算表达式</strong><br>说了半天就是用矢量效率高，运用广播机制加上b。<br>矢量重写损失函数</p>
<h3 id="3-2-线性回归的从零开始实现"><a href="#3-2-线性回归的从零开始实现" class="headerlink" title="3.2 线性回归的从零开始实现"></a>3.2 线性回归的从零开始实现</h3><p>尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，会导致我们很难深入理解深度学习是如何工作的。因此，本节将介绍如何只利用NDArray和autograd来实现一个线性回归的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>

<h4 id="3-2-1-生成数据集"><a href="#3-2-1-生成数据集" class="headerlink" title="3.2.1 生成数据集"></a>3.2.1 生成数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = nd.random.normal(scale=<span class="number">1</span>, shape=(num_examples, num_inputs))</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += nd.random.normal(scale=<span class="number">0.01</span>, shape=labels.shape) <span class="comment">#加上随机噪声</span></span><br></pre></td></tr></table></figure>

<h4 id="3-2-2-读取数据集"><a href="#3-2-2-读取数据集" class="headerlink" title="3.2.2 读取数据集"></a>3.2.2 读取数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = nd.array(indices[i: min(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features.take(j), labels.take(j)  <span class="comment"># take函数根据索引返回对应元素</span></span><br></pre></td></tr></table></figure>

<h4 id="3-2-3-初始化模型参数"><a href="#3-2-3-初始化模型参数" class="headerlink" title="3.2.3 初始化模型参数"></a>3.2.3 初始化模型参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">b = nd.zeros(shape=(<span class="number">1</span>,))</span><br><span class="line">w.attach_grad() <span class="comment">#绑定要自动求导的变量</span></span><br><span class="line">b.attach_grad() <span class="comment">#绑定要自动求导的变量</span></span><br></pre></td></tr></table></figure>

<h4 id="3-2-4-定义模型"><a href="#3-2-4-定义模型" class="headerlink" title="3.2.4 定义模型"></a>3.2.4 定义模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</span><br></pre></td></tr></table></figure>

<h4 id="3-2-5-定义损失函数"><a href="#3-2-5-定义损失函数" class="headerlink" title="3.2.5 定义损失函数"></a>3.2.5 定义损失函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>

<h4 id="3-2-6-定义优化算法"><a href="#3-2-6-定义优化算法" class="headerlink" title="3.2.6 定义优化算法"></a>3.2.6 定义优化算法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad / batch_size   <span class="comment">#看见没有 这里param.grad就是利用梯度做优化</span></span><br></pre></td></tr></table></figure>

<h4 id="3-2-7-训练模型"><a href="#3-2-7-训练模型" class="headerlink" title="3.2.7 训练模型"></a>3.2.7 训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X</span></span><br><span class="line">    <span class="comment"># 和y分别是小批量样本的特征和标签</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>

<p><strong><em>感觉贼酷！</em></strong></p>
<h3 id="3-3-线性回归的简洁实现"><a href="#3-3-线性回归的简洁实现" class="headerlink" title="3.3 线性回归的简洁实现"></a>3.3 线性回归的简洁实现</h3><p>本节中，我们将介绍如何使用MXNet提供的Gluon接口更方便地实现线性回归的训练</p>
<h4 id="3-3-1-生成数据集"><a href="#3-3-1-生成数据集" class="headerlink" title="3.3.1 生成数据集"></a>3.3.1 生成数据集</h4><p>和前面一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = nd.random.normal(scale=<span class="number">1</span>, shape=(num_examples, num_inputs))</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += nd.random.normal(scale=<span class="number">0.01</span>, shape=labels.shape)</span><br></pre></td></tr></table></figure>

<h4 id="3-3-2-读取数据集"><a href="#3-3-2-读取数据集" class="headerlink" title="3.3.2 读取数据集"></a>3.3.2 读取数据集</h4><p>Gluon提供了data包来读取数据。由于data常作为变量名，将倒入的data模块用添加了Gluon首字母的假名gdata代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">dataset = gdata.ArrayDataset(features, labels)</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line">data_iter = gdata.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-3-3-定义模型"><a href="#3-3-3-定义模型" class="headerlink" title="3.3.3 定义模型"></a>3.3.3 定义模型</h4><p>可以方便地用nn的包来实现，也有集成的sequential方法，现在模型是简单的全连接层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"></span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>值得一提，在Gluon中我们无须指定每一层的输入形状，例如线性回归的输入个数，当模型得到数据时，例如后面执行net(X)时，模型将自动推断每一层的输入个数。为开发提供便利。</p>
<h4 id="3-3-4-初始化模型参数"><a href="#3-3-4-初始化模型参数" class="headerlink" title="3.3.4 初始化模型参数"></a>3.3.4 初始化模型参数</h4><p>使用net前初始化模型参数，如线性回归模型中的权重和偏差。我们在MXNet导入init模块。该模块提供了模型参数初始化的各种方法。通过init.Normal(sigma=0.01)指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure>

<p>so cool～！</p>
<h4 id="3-3-5-定义损失函数"><a href="#3-3-5-定义损失函数" class="headerlink" title="3.3.5 定义损失函数"></a>3.3.5 定义损失函数</h4><p>在Gluon中，loss模块定义了各种损失函数。我们用假名gloss代替loss模块，并使用它提供的平方损失为模型的损失参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line">loss = gloss.L2Loss()  <span class="comment">#平方损失又称为L2范数损失</span></span><br></pre></td></tr></table></figure>

<h4 id="3-3-6-定义优化算法"><a href="#3-3-6-定义优化算法" class="headerlink" title="3.3.6 定义优化算法"></a>3.3.6 定义优化算法</h4><p>导入Gluon后，我们创建一个Trainer实例，并指定学习率为0.03的小批量随机梯度下降sgd为优化算法（不用自己写）。该算法将用来迭代net实例所有通过add函数嵌套的层包含的全部参数。这些参数可以通过collect_params函数来获取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.03</span>&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="3-3-7-训练模型"><a href="#3-3-7-训练模型" class="headerlink" title="3.3.7 训练模型"></a>3.3.7 训练模型</h4><p>在使用Gluon训练模型时，我们通过调用Trainer实例来的step函数来迭代模型参数。3.2节我们提到，由于变量l是长度为batch_size的一维NDArray，执行l.backward()等价于执行l.sum().backward()。按照小批量随机梯度下降法的定义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>

<p><strong><em>要明白随机梯度下降法是对损失函数进行，要随机让损失函数变小的同时学习改变模型参数</em></strong><br>最后查看学习到的参数，双击666！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dense = net[<span class="number">0</span>]   <span class="comment">#注意指定第一层的权重</span></span><br><span class="line">true_w, dense.weight.data()</span><br><span class="line">true_b, dense.bias.data()</span><br></pre></td></tr></table></figure>

<h3 id="3-4-softmax回归"><a href="#3-4-softmax回归" class="headerlink" title="3.4 softmax回归"></a>3.4 softmax回归</h3><p>前几节的线性回归模型适用于输出连续值的情景。在另一类情景中，模型输出可以是想一个图像类别的离散分类问题。我们可以使用诸如softmax回归在内的分类模型，和线性回归不同sofrmax回归的<strong>输出单元</strong>从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经网络中的分类模型。</p>
<h4 id="3-4-1-分类问题"><a href="#3-4-1-分类问题" class="headerlink" title="3.4.1 分类问题"></a>3.4.1 分类问题</h4><p>之前也晓得，分类问题可以用回归方法解决但是会影响到回归质量，一般使用更加适合离散值输出的模型来解决分类问题（softmax函数）。</p>
<h4 id="3-4-2-softmax回归模型"><a href="#3-4-2-softmax回归模型" class="headerlink" title="3.4.2 softmax回归模型"></a>3.4.2 softmax回归模型</h4><p>o1 = x1w11 + x2w21 + x3w31 + x4w41 + b1<br>o2 = x1w12 + x2w22 + x3w32 + x4w42 + b2<br>o3 = x1w13 + x2w23 + x3w33 + x4w43 + b3<br>softmax回归是一个单层神经网络<br><strong>softmax运算</strong><br>将值的大小变成概率 so cool</p>
<h4 id="3-4-3-单样本样本分类的矢量计算表达式"><a href="#3-4-3-单样本样本分类的矢量计算表达式" class="headerlink" title="3.4.3 单样本样本分类的矢量计算表达式"></a>3.4.3 单样本样本分类的矢量计算表达式</h4><p>为了提高计算效率，我们将单样本分类通过矢量计算来表达。</p>
<h4 id="3-4-4-小批量分类的矢量计算表达式"><a href="#3-4-4-小批量分类的矢量计算表达式" class="headerlink" title="3.4.4 小批量分类的矢量计算表达式"></a>3.4.4 小批量分类的矢量计算表达式</h4><p>通常我们对小批量数据做矢量计算。广义上来讲，给定一个小批量样本，其批量样本大小为n,输入个数为d，输出个数为q。就是每个向量的大小变大了<br>Onxq=XnxdWdxq + blxq<br>Yhatnxq = softmax(Onxq)o</p>
<h4 id="3-4-5-交叉熵损失函数"><a href="#3-4-5-交叉熵损失函数" class="headerlink" title="3.4.5 交叉熵损失函数"></a>3.4.5 交叉熵损失函数</h4><p>更科学的损失函数，最小化交叉熵损失函数等价于最大化数据集所有标签类别的联合预测概率。</p>
<h4 id="3-4-6-模型预测及评价"><a href="#3-4-6-模型预测及评价" class="headerlink" title="3.4.6 模型预测及评价"></a>3.4.6 模型预测及评价</h4><p>准确率 啥啥率 有好多种不同</p>
<h3 id="3-5-图像分类数据集（Fashion-MNIST）"><a href="#3-5-图像分类数据集（Fashion-MNIST）" class="headerlink" title="3.5 图像分类数据集（Fashion-MNIST）"></a>3.5 图像分类数据集（Fashion-MNIST）</h3><p>一个比MNIST更复杂的图像分类数据集</p>
<h4 id="3-5-1-获取数据集"><a href="#3-5-1-获取数据集" class="headerlink" title="3.5.1 获取数据集"></a>3.5.1 获取数据集</h4><p>easy 还有许多读取啥的函数方法</p>
<h4 id="3-5-2-获取小批量"><a href="#3-5-2-获取小批量" class="headerlink" title="3.5.2 获取小批量"></a>3.5.2 获取小批量</h4><ul>
<li>可以自己定义来用yield，这里用DataLoader实例。每次读取一个样本数为batch_size的小批量数据。这里的pillage大小batch_sze是一个超参数</li>
<li>DataLoader可以用num_workers性能加速</li>
<li>ToTensor实例将图像数据从uint8格式变换成32浮点数并除以255使得所有像素的数值均在0到1之间。ToTensor实例还将图像通道从最后一维移到最前一维方便之后介绍的卷积神经网络。通过数据集的transform_first函数，我们将ToTensor的变换应用在每个数据样本（图像和标签）的第一个元素，即图像之上。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">transformer = gdata.vision.transforms.ToTensor()</span><br><span class="line"><span class="keyword">if</span> sys.platform.startswith(<span class="string">'win'</span>):</span><br><span class="line">    num_workers = <span class="number">0</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_workers = <span class="number">4</span>  <span class="comment"># 会比较快</span></span><br><span class="line"></span><br><span class="line">train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),</span><br><span class="line">                              batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                              num_workers=num_workers)</span><br><span class="line">test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),</span><br><span class="line">                             batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                             num_workers=num_workers)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="3-6-softmax回归的从零开始实现"><a href="#3-6-softmax回归的从零开始实现" class="headerlink" title="3.6 softmax回归的从零开始实现"></a>3.6 softmax回归的从零开始实现</h3><p>3.6.1 读取数据集<br>3.6.2 初始化模型参数<br>3.6.3 实现softmax运算<br>先解决多维度NDArray按维度操作的问题，之后定义soft运算（不用包纯手工）</p>
<p>3.6.4 定义模型<br>我们的模型就是纯的softmax函数</p>
<p>3.6.5 定义损失函数<br>交叉熵损失函数，先学会pick函数</p>
<p>3.6.6 计算分类准确率<br>先明白准确率的原理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(axis=<span class="number">1</span>) == y.astype(<span class="string">'float32'</span>)).mean().asscalar()</span><br></pre></td></tr></table></figure>

<p>再预测真个net在数据集合上的准确率</p>
<p>3.6.7 训练模型<br>和之前线性回归差不多 多了准确率的计算 熟练掌握就好</p>
<p>3.6.8 预测<br>学会怎么这么优美地把预测 真实 图片摆放在一起 中间三行是展示封装函数逻辑的 注释也能运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:   <span class="comment">#注意这里迭代 测试集 只有255个</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.asnumpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>).asnumpy())</span><br><span class="line">titles = [true + <span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]  <span class="comment">#有些逻辑 自己写不一定能写出来</span></span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure>

<h3 id="3-7-softmax回归的简洁实现"><a href="#3-7-softmax回归的简洁实现" class="headerlink" title="3.7 softmax回归的简洁实现"></a>3.7 softmax回归的简洁实现</h3><p>贼简洁 明白原理直接调用包就ok</p>
<h3 id="3-8-多层感知机"><a href="#3-8-多层感知机" class="headerlink" title="3.8 多层感知机"></a>3.8 多层感知机</h3><p>已经学会了线性回归和softmax回归在内的单层神经网络😁。但是深度学习更多关注多层模型。在本节中，我们将以多层感知机（multilayer perceptron, MLP）为例，介绍多层神经网络的特点。</p>
<h4 id="3-8-1-隐藏层"><a href="#3-8-1-隐藏层" class="headerlink" title="3.8.1 隐藏层"></a>3.8.1 隐藏层</h4><p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。</p>
<h4 id="3-8-2-激活函数"><a href="#3-8-2-激活函数" class="headerlink" title="3.8.2 激活函数"></a>3.8.2 激活函数</h4><p>防止仿射变换，引入非线性变换（加入多少隐藏层都是仿射变换）<br>1.ReLU函数<br>简单的线性变换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x.relu() <span class="comment">#内置函数</span></span><br></pre></td></tr></table></figure>

<p>2.sigmoid函数</p>
<ul>
<li>将元素的值变换到0和1之间，sigmoid函数在早期的神经网络中较为普遍，但它目前逐渐被简单的ReLU函数取代。控制信息在神经网络中的流动。</li>
<li>sigmoid的导数：当输入为0时，sigmoid函数的导数达到最大值0.25，当输入偏离0时，sigmoid函数的导数越接近0。</li>
</ul>
<p>3.tanh函数</p>
<ul>
<li>tanh函数可以将元素的值变换到-1到1之间。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</li>
<li>导数：当输入为0时，达到最大值1，当输入越偏离0时，tanh函数的导数越接近0。<h4 id="3-8-3-多层感知机"><a href="#3-8-3-多层感知机" class="headerlink" title="3.8.3 多层感知机"></a>3.8.3 多层感知机</h4>多层感知机就是含有<strong>至少一个隐藏层</strong>的全连接层组成的神经网络，且<strong>每个隐藏层的输出通过激活函数</strong>进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号<br>$$<br>\begin{aligned}<br>\boldsymbol{H} &amp;= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\<br>\boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,<br>\end{aligned}<br>$$<br>其中$\phi$表示激活函数。</li>
<li>分类问题中：我们可以对输出O做softmax运算，并使用softmax回归中的交叉熵损失函数。（也就是以前直接输出，现在在是经过一个隐藏层接着激活之后给输出）</li>
<li>回归问题中：我们将输出层的输出个数设为1，并将输出o直接提供给线性回归中使用的平方损失函数。</li>
</ul>
<p>666 虽然有点迷迷糊糊的</p>
<h3 id="3-9-多层感知机的从零开始实现"><a href="#3-9-多层感知机的从零开始实现" class="headerlink" title="3.9 多层感知机的从零开始实现"></a>3.9 多层感知机的从零开始实现</h3><p>3.9.1 读取数据集<br>3.9.2 定义模型参数<br>多了一层隐藏层的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, num_hiddens))</span><br><span class="line">b1 = nd.zeros(num_hiddens)</span><br><span class="line">W2 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_hiddens, num_outputs))</span><br><span class="line">b2 = nd.zeros(num_outputs)</span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br></pre></td></tr></table></figure>

<p>3.9.3 定义激活函数<br>不使用包，自己定义</p>
<p>3.9.4 定义模型这里有不同的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H = relu(nd.dot(X, W1) + b1)   <span class="comment"># 这里多了一层！！隐藏层！！</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(H, W2) + b2</span><br></pre></td></tr></table></figure>

<p><strong><em>我去没有定义优化算法 应该封装在最后训练函数里了</em></strong><br>3.9.5 定义损失函数<br>为了稳定性使用gluon的包</p>
<p>3.9.6 训练模型<br>easy</p>
<h3 id="3-10-多层感知机的简洁实现"><a href="#3-10-多层感知机的简洁实现" class="headerlink" title="3.10 多层感知机的简洁实现"></a>3.10 多层感知机的简洁实现</h3><p>so 简洁！！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据与训练模型</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>,</span><br><span class="line">              <span class="literal">None</span>, trainer)</span><br></pre></td></tr></table></figure>

<h3 id="3-11-模型选择、欠拟合和过拟合"><a href="#3-11-模型选择、欠拟合和过拟合" class="headerlink" title="3.11 模型选择、欠拟合和过拟合"></a>3.11 模型选择、欠拟合和过拟合</h3><blockquote>
<p>提出问题：为啥模型在训练数据集上更准确时，它在测试数据集上却不一定准确</p>
</blockquote>
<h4 id="3-11-1-训练误差和泛化误差"><a href="#3-11-1-训练误差和泛化误差" class="headerlink" title="3.11.1 训练误差和泛化误差"></a>3.11.1 训练误差和泛化误差</h4><ul>
<li>训练误差：模型在训练数据集上表现的误差</li>
<li>泛化误差：模型在任意一个测试数据样本上表现出的误差的期望，？？并常常通过测试数据集上的误差来近似？？</li>
</ul>
<p>计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数<br>机器学习模型应关注降低泛化误差</p>
<h4 id="3-11-2-模型选择"><a href="#3-11-2-模型选择" class="headerlink" title="3.11.2 模型选择"></a>3.11.2 模型选择</h4><p>机器学习中，通常需要评估若干候选模型的表现并从中选择模型。<br>可供选择的候选模型可以是有不同超参数的同类模型。<br>对多层感知机：可以选择隐藏层的个数，以及每个隐藏层中隐藏单元个数和激活函数。<br>通常要在模型选择上下一番功夫。经常使用验证数据集。<br><strong>1.验证数据集</strong><br>测试集：只能在超参数和模型参数选定后使用一次（只能用来测试），不能用来调参和选择模型<br>训练集：无法从训练误差顾及泛化误差，不能依赖训练集选择模型。<br>鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。<br>称为验证数据集，简称验证集。例如我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。<br>实际应用中，由于数据不容易获取，测试数据极少使用一次就丢弃。因此验证数据集和测试数据集界限比较模糊。<br><strong>2.k折交叉验证</strong><br>由于验证数据集不参与模型训练，所以当训练数据不够用时，预留大量验证数据集有点奢侈。所以用k折交叉验证。</p>
<blockquote>
<p>即将训练数据集分成k个不重合的子数据集，然后我们做k次模型训练和验证，每一次用一个子数据集验证，其他k-1个子数据集来做训练模型。每次用到的子数据集不同。最后用k次训练误差和验证误差取平均/</p>
</blockquote>
<h4 id="3-11-3-欠拟合和过拟合"><a href="#3-11-3-欠拟合和过拟合" class="headerlink" title="3.11.3 欠拟合和过拟合"></a>3.11.3 欠拟合和过拟合</h4><p>要同时避免欠拟合和过拟合，虽然因素很多，重点讨论模型复杂度和训练数据集大小。<br><strong>1.模型复杂度</strong><br>多项式模型复杂度更高，更容易减小训练误差，但容易造成过拟合，太简单欠拟合。<br><strong>2.训练数据集大小</strong><br>样本数据集过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此希望数据集更大</p>
<h4 id="3-11-4-多项式函数拟合实验"><a href="#3-11-4-多项式函数拟合实验" class="headerlink" title="3.11.4 多项式函数拟合实验"></a>3.11.4 多项式函数拟合实验</h4><p>我去这个贼6 自己看不懂！<br><strong>1.生成数据集</strong><br><strong>2.定义、训练和测试模型</strong><br><strong><em>我去牛皮！！</em></strong><br>在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数，当模型得到数据时，例如后面执行net(x)时，模型将自动推断出每一层的输入个数，我们我们将在第4章介绍这种机制</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">1</span>))    <span class="comment">#妈的只需要这样 1 是输出个数的意思</span></span><br><span class="line">net.initialize()</span><br></pre></td></tr></table></figure>

<p><strong>3.三阶多项式函数拟合（正常）</strong><br><strong>4.线性函数拟合（欠拟合）</strong><br><strong>5.训练样本不足（过拟合）</strong></p>
<h3 id="3-12-权重衰减"><a href="#3-12-权重衰减" class="headerlink" title="3.12 权重衰减"></a>3.12 权重衰减</h3><h4 id="3-12-1-方法"><a href="#3-12-1-方法" class="headerlink" title="3.12.1 方法"></a>3.12.1 方法</h4><p>引入权重衰减（等价于L2范数正则化），防止过拟合<br>损失函数改变：增加$$\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,$$<br>迭代方式改变：$$<br>\begin{aligned}<br>w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\<br>w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).<br>\end{aligned}<br>$$</p>
<h4 id="3-12-2-高维线性回归实验"><a href="#3-12-2-高维线性回归实验" class="headerlink" title="3.12.2 高维线性回归实验"></a>3.12.2 高维线性回归实验</h4><p>通过高维线性回归为例引入一个过拟合问题，并使用权重衰减解决</p>
<h4 id="3-12-3-从零开始实现"><a href="#3-12-3-从零开始实现" class="headerlink" title="3.12.3 从零开始实现"></a>3.12.3 从零开始实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">                <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">                l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br></pre></td></tr></table></figure>

<p>只是在损失函数添加惩罚<br>没有在迭代增加</p>
<h4 id="3-12-4-简洁实现"><a href="#3-12-4-简洁实现" class="headerlink" title="3.12.4 简洁实现"></a>3.12.4 简洁实现</h4><p>可以分别对w和b都进行权重衰减（默认对两个都会耍贱）<br>但可以自己设置只对w进行衰减，权重衰减可以通过Gluon的wd超参数来指定。</p>
<h3 id="3-13-丢弃法"><a href="#3-13-丢弃法" class="headerlink" title="3.13 丢弃法"></a>3.13 丢弃法</h3><p>3.13.1 方法<br>除了权重拟合，还可以用丢弃法解决过拟合的问题<br>隐藏单元的计算表达式，对隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。有p的概率被清零，有1-p的概率会被除以1-p做拉伸。丢弃概率是丢弃法的超参数<br>由于训练中隐藏层神经元的丢弃是随机的，即h1~h5都有可能被清零，输出层的计算无法过度依赖h1～h5中的任一个，从而在训练模型时起到正则化的作用。<br>3.13.2 从零开始实现<br>先学dropout函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_prob)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X.zeros_like()</span><br><span class="line">    mask = nd.random.uniform(<span class="number">0</span>, <span class="number">1</span>, X.shape) &lt; keep_prob   <span class="comment">#uniform 不知道干啥的</span></span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br></pre></td></tr></table></figure>

<p>1、定义模型参数<br>2、定义模型<br>3、训练和测试模型<br>3.13.3 简洁实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(drop_prob1),  <span class="comment"># 在第一个全连接层后添加丢弃层</span></span><br><span class="line">        nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(drop_prob2),  <span class="comment"># 在第二个全连接层后添加丢弃层</span></span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br></pre></td></tr></table></figure>

<h3 id="3-14-正向传播、反向传播和计算图"><a href="#3-14-正向传播、反向传播和计算图" class="headerlink" title="3.14 正向传播、反向传播和计算图"></a>3.14 正向传播、反向传播和计算图</h3><p>之前我们是使用小批量随机梯度下降的优化算法来训练模型。默认使用了反向传播，本节使用数学来描述正向传播和反向传播。具体来说，我们将以带L2范数正则化的含单隐藏层的多层感知机为样例模型解释正向传播和反向传播。<br>巴拉巴拉一堆数学看不懂 但原理应该了解一些 就是从后向前求取梯度，吴恩达的课好像也讲过<br>还有正向传播依赖反向传播，反向传播依赖正向传播。</p>
<h3 id="3-15-数值稳定性和模型初始化"><a href="#3-15-数值稳定性和模型初始化" class="headerlink" title="3.15 数值稳定性和模型初始化"></a>3.15 数值稳定性和模型初始化</h3><p>3.15.1 衰减和爆炸<br>当神经网络层数较多，模型的数值稳定性容易变差。容易很多层会很小或者很大<br>3.15.2 随机初始化模型参数<br>1.MXNet的默认随机初始化<br>2.Xavier随机初始化</p>
<h3 id="3-16-实战Kaggle比赛：房价预测"><a href="#3-16-实战Kaggle比赛：房价预测" class="headerlink" title="3.16 实战Kaggle比赛：房价预测"></a>3.16 实战Kaggle比赛：房价预测</h3><p>终于有实际代码了，理论好复杂。<br>本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。<br>很多 有点复杂 自己看书 再理解会比较好</p>
<h2 id="第4章-深度学习计算"><a href="#第4章-深度学习计算" class="headerlink" title="第4章 深度学习计算"></a>第4章 深度学习计算</h2><blockquote>
<p>第3章介绍了包括多层感知机在内的简单深度学习模型的原理。本章我们将简要概括深度学习的各个重要组成部分，如模型构造、参数的访问和初始化等，自定义层，读取、存储和使用GPU。</p>
</blockquote>
<h3 id="4-1-模型构造"><a href="#4-1-模型构造" class="headerlink" title="4.1 模型构造"></a>4.1 模型构造</h3><p>之前是Sequential类构造模型。这里我们介绍一种基于Block类的模型构造方法：它让模型构造更加灵活</p>
<h4 id="4-1-1-继承Block类来构造模型"><a href="#4-1-1-继承Block类来构造模型" class="headerlink" title="4.1.1 继承Block类来构造模型"></a>4.1.1 继承Block类来构造模型</h4><p>Block类是nn模块里提供的一个模型构造类。这里MLP类重载了Block类的<strong>init</strong>函数和forward函数，分别用于创建模型参数和定义向前计算<br><strong>新的模型构造方法</strong> 只是构造一个模型，更多像一个方法<br>注意：这里并没有将Block类命名为Layer（层）或者Model（模型）之类的名字，因为Block类是一个可以自由组建的不见。它的子类既可以是一个层（如Gluon提供的Dense类），又可以是一个模型（如这里的MLP类），或者是模型。下面通过例子展示它的灵活性😓</p>
<h4 id="4-1-2-Sequential类继承自Block类"><a href="#4-1-2-Sequential类继承自Block类" class="headerlink" title="4.1.2 Sequential类继承自Block类"></a>4.1.2 Sequential类继承自Block类</h4><p>事实上Sequential类继承自Block类，它提供add函数来逐一添加串联的Block子类实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。<br>666 完全可以用自己方法写一套Sequential()出来</p>
<h4 id="4-1-3-构造复杂的模型"><a href="#4-1-3-构造复杂的模型" class="headerlink" title="4.1.3 构造复杂的模型"></a>4.1.3 构造复杂的模型</h4><p>虽然Sequential可以让模型构造更简单，不需要定义forward函数，但直接用Block类可以极大地拓展模型构造的灵活性。<br>构建稍微复杂点的网络FancyMLP，通过get_constant函数创建训练中不被迭代的参数。</p>
<p>因为FancyMLP和Squential类都是Block类的子类，我们可以嵌套调用它们</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.add(NestMLP(), nn.Dense(<span class="number">20</span>), FancyMLP())</span><br></pre></td></tr></table></figure>

<h3 id="4-2-模型参数的访问、初始化和共享"><a href="#4-2-模型参数的访问、初始化和共享" class="headerlink" title="4.2 模型参数的访问、初始化和共享"></a>4.2 模型参数的访问、初始化和共享</h3><p>前面我们通过init模块来初始化模型的全部参数。我们也介绍了访问模型的简单方法。本节深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。<br>先定义一个与4.1节中相同的含单隐藏层的多层感知机。依然使用默认方式初始化参数，并做一次向前计算。与之前不同，包含MXNet的init模块，包含多种模型初始化方法。</p>
<h4 id="4-2-1-访问模型参数"><a href="#4-2-1-访问模型参数" class="headerlink" title="4.2.1 访问模型参数"></a>4.2.1 访问模型参数</h4><p>用[]索引访问所有隐藏参数</p>
<ul>
<li><p>索引方式：easy  小学生都会</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight</span><br><span class="line">net[<span class="number">0</span>].weight.data()</span><br><span class="line">net[<span class="number">0</span>].weight.grad()</span><br><span class="line">net[<span class="number">1</span>].weight.data()</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以用collect_params函数来获取net变量的所有嵌套</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.collect_params()</span><br><span class="line">net.collect_params(<span class="string">'.*weight'</span>) <span class="comment">#给参数筛选</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="4-2-2-初始化模型参数"><a href="#4-2-2-初始化模型参数" class="headerlink" title="4.2.2 初始化模型参数"></a>4.2.2 初始化模型参数</h4><p>之前有模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数全为零。MXNet中提供了多种预设的初始化方法。<br>easy 不会用自己看 贼6！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.initialize(init=init.Normal(sigma=<span class="number">0.01</span>), force_reinit=<span class="literal">True</span>) <span class="comment">#均值为0，标准差为0.0正态分布</span></span><br><span class="line">net.initialize(init=init.Constant(<span class="number">1</span>), force_reinit=<span class="literal">True</span>) <span class="comment"># 权为1的初始化</span></span><br><span class="line">net[<span class="number">0</span>].weight.initialize(init=init.Xavier(), force_reinit=<span class="literal">True</span>) <span class="comment"># Xavier()方法</span></span><br></pre></td></tr></table></figure>

<h4 id="4-2-3-自定义初始化方法"><a href="#4-2-3-自定义初始化方法" class="headerlink" title="4.2.3 自定义初始化方法"></a>4.2.3 自定义初始化方法</h4><p>有时候需要的初始化方法init模块没有提供。这时，我们可以实现一个Initializer类的子类，像使用其他初始化方法那样使用它。通常，我们只需要实现_init_weight这个函数，并将其传入NDArray修改成初始化的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyInit</span><span class="params">(init.Initializer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weight</span><span class="params">(self, name, data)</span>:</span></span><br><span class="line">        print(<span class="string">'Init'</span>, name, data.shape)</span><br><span class="line">        data[:] = nd.random.uniform(low=<span class="number">-10</span>, high=<span class="number">10</span>, shape=data.shape)</span><br><span class="line">        data *= data.abs() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.initialize(MyInit(), force_reinit=<span class="literal">True</span>)</span><br><span class="line">net[<span class="number">0</span>].weight.data()[<span class="number">0</span>]   <span class="comment">#自定义</span></span><br><span class="line">net[<span class="number">0</span>].weight.initialize(init=init.Xavier(), force_reinit=<span class="literal">True</span>)   <span class="comment">#官方 init=。。。。</span></span><br></pre></td></tr></table></figure>

<p>还可以用Parameter类的set_data函数来直接改写模型参数。例如在下例中我们将隐藏层参数在现有的基础上加1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.set_data(net[<span class="number">0</span>].weight.data() + <span class="number">1</span>)</span><br><span class="line">net[<span class="number">0</span>].weight.data()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="4-2-4-共享模型参数"><a href="#4-2-4-共享模型参数" class="headerlink" title="4.2.4 共享模型参数"></a>4.2.4 共享模型参数</h4><p>共享参数：</p>
<ul>
<li><p>forward函数里多次调用同个层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FancyMLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">      super(FancyMLP, self).__init__(**kwargs)</span><br><span class="line">      <span class="comment"># 使用get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）</span></span><br><span class="line">      self.rand_weight = self.params.get_constant(</span><br><span class="line">          <span class="string">'rand_weight'</span>, nd.random.uniform(shape=(<span class="number">20</span>, <span class="number">20</span>)))</span><br><span class="line">      self.dense = nn.Dense(<span class="number">20</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">      x = self.dense(x)  <span class="comment">#!!</span></span><br><span class="line">      <span class="comment"># 使用创建的常数参数，以及NDArray的relu函数和dot函数</span></span><br><span class="line">      x = nd.relu(nd.dot(x, self.rand_weight.data()) + <span class="number">1</span>)  </span><br><span class="line">      <span class="comment"># 复用全连接层。等价于两个全连接层共享参数</span></span><br><span class="line">      x = self.dense(x)   <span class="comment">#!!</span></span><br><span class="line">      <span class="comment"># 控制流，这里我们需要调用asscalar函数来返回标量进行比较</span></span><br><span class="line">      <span class="keyword">while</span> x.norm().asscalar() &gt; <span class="number">1</span>:</span><br><span class="line">          x /= <span class="number">2</span></span><br><span class="line">      <span class="keyword">if</span> x.norm().asscalar() &lt; <span class="number">0.8</span>:</span><br><span class="line">          x *= <span class="number">10</span></span><br><span class="line">      <span class="keyword">return</span> x.sum()</span><br></pre></td></tr></table></figure>
</li>
<li><p>另外一种方法，在构造层指定特定的参数。如果不同层使用同一份参数，那么它们在前向计算和反向传播时都会共享相同的参数。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">shared = nn.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">net.add(nn.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        shared, <span class="comment"># shared在第二层和第三层</span></span><br><span class="line">        nn.Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>, params=shared.params),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line">X = nd.random.uniform(shape=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">net[<span class="number">1</span>].weight.data()[<span class="number">0</span>] == net[<span class="number">2</span>].weight.data()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>  在构造第三隐藏层时，我们通过params来指定它使用第二隐藏层的参数。因为模型参数包含了梯度，所以在反向传播计算时，第二隐藏层和第三隐藏层的梯度都会被累加在shared.params.grad()里。</p>
</li>
</ul>
<h3 id="4-3-模型参数的延后初始化"><a href="#4-3-模型参数的延后初始化" class="headerlink" title="4.3 模型参数的延后初始化"></a>4.3 模型参数的延后初始化</h3><p>就是模型参数初始化有猫腻<br>之前net在调用初始化函数initialize之后、在做前向计算net(X)之前时，权重参数的形状汇总出现了0。虽然直觉上initialize完成了所有参数初始化过程，然而这在Gluon中却时不一定的。草 没事找事</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyInit</span><span class="params">(init.Initializer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weight</span><span class="params">(self, name, data)</span>:</span></span><br><span class="line">        print(<span class="string">'Init'</span>, name, data.shape)</span><br><span class="line">        <span class="comment"># 实际的初始化逻辑在此省略了</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net.initialize(init=MyInit())   <span class="comment">#首次初始化没有 只有前向计算一次 才能推断出来 再进行初始化 并且这个初始化只会执行一次 之后不会 再初始化</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>延后初始化：系统将真正的参数初始阿虎啊延后到得到足够信息时才执行的行为。<br>这个延后初始化就是方便你 不需要指定输入的大小，系统会自动判断输入的大小 只需要确定输出即可</p>
</blockquote>
<p>避免延后初始化：</p>
<ol>
<li><p>我们对已初始化的模型重新初始化。因为参数形状不会发生变化，所以系统能够立即进行重新初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.initialize(init=MyInit(), force_reinit=<span class="literal">True</span>) <span class="comment">#前面已经初始化过了</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>我们在创建层的时候制定了它输入的个数，使系统不需要额外的信息来推测参数形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>, in_units=<span class="number">20</span>,    activation=<span class="string">'relu'</span>))   <span class="comment">#说明了 in_units</span></span><br><span class="line">net.add(nn.Dense(<span class="number">10</span>, in_units=<span class="number">256</span>))</span><br><span class="line"></span><br><span class="line">net.initialize(init=MyInit())</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="4-4-自定义层"><a href="#4-4-自定义层" class="headerlink" title="4.4 自定义层"></a>4.4 自定义层</h3><p>深度学习的魅力之一在于神经网络中各式各样的层，例如全连接层和第5章和第6章中将要介绍的卷积层、池化层与循环层。虽然Gluon提供了大量使用的层，但有时候我们依然希望自定义层。本节将介绍如何使用NDArray来自定义一个Gluon的层，从而可以被重复调用。</p>
<h4 id="4-4-1-不含模型参数的自定义层"><a href="#4-4-1-不含模型参数的自定义层" class="headerlink" title="4.4.1 不含模型参数的自定义层"></a>4.4.1 不含模型参数的自定义层</h4><p>实现一个CenteredLayer类通过继承Block类自定义一个将输入减掉均值后输出的层，并将层的计算定义在forward函数里。这个层不含模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(CenteredLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br></pre></td></tr></table></figure>

<h4 id="4-4-2-含模型参数的自定义层"><a href="#4-4-2-含模型参数的自定义层" class="headerlink" title="4.4.2 含模型参数的自定义层"></a>4.4.2 含模型参数的自定义层</h4><p>利用Parameter类和ParameterDict类。在自定义含模型参数的层时，我们可以利用Block类自带的ParameterDict类型的成员变量params。<strong><em>它是一个由字符串类型的参数名字映射到Parameter类型的模型参数的字典。</em></strong>，通过get函数从ParameterDict创建Parameter实例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="comment"># units为该层的输出个数，in_units为该层的输入个数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, in_units, **kwargs)</span>:</span></span><br><span class="line">        super(MyDense, self).__init__(**kwargs)</span><br><span class="line">        self.weight = self.params.get(<span class="string">'weight'</span>, shape=(in_units, units))</span><br><span class="line">        self.bias = self.params.get(<span class="string">'bias'</span>, shape=(units,))  <span class="comment">#self.params.get</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        linear = nd.dot(x, self.weight.data()) + self.bias.data()</span><br><span class="line">        <span class="keyword">return</span> nd.relu(linear)</span><br></pre></td></tr></table></figure>

<h3 id="4-5-读取和存储"><a href="#4-5-读取和存储" class="headerlink" title="4.5 读取和存储"></a>4.5 读取和存储</h3><p>目前为止，我们介绍了如何处理数据以及如何构建、训练和测试深度学习模型。我们有时需要把训练好的模型部署到很多不同的设备上。在这种情况下，我们可以把内存中训练好的模型参数存储到硬盘后供后续读取使用。<br>6666 迁移 实用</p>
<h4 id="4-5-1-读写NDArry"><a href="#4-5-1-读写NDArry" class="headerlink" title="4.5.1 读写NDArry"></a>4.5.1 读写NDArry</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nd.save(<span class="string">'x'</span>, x)   <span class="comment">#真的多出来了😂</span></span><br><span class="line">x2 = nd.load(<span class="string">'x'</span>)</span><br><span class="line"></span><br><span class="line">nd.save(<span class="string">'xy'</span>, [x, y])</span><br><span class="line">x2, y2 = nd.load(<span class="string">'xy'</span>) <span class="comment">#存储一列NDArray并读回内存。</span></span><br><span class="line"></span><br><span class="line">nd.save(<span class="string">'mydict'</span>, mydict)</span><br><span class="line">mydict2 = nd.load(<span class="string">'mydict'</span>) <span class="comment">#存储并读取一个从字符串映射到NDArray的字典。</span></span><br></pre></td></tr></table></figure>

<p>easy</p>
<h4 id="4-5-2-读写Gluon模型的参数"><a href="#4-5-2-读写Gluon模型的参数" class="headerlink" title="4.5.2 读写Gluon模型的参数"></a>4.5.2 读写Gluon模型的参数</h4><p>还可以读写Gluon模型的参数。Gluon的Block类提供了save_parameters函数和load_parameters函数来读写模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'mlp.params'</span></span><br><span class="line">net.save_parameters(filename) <span class="comment">#666 用的时候不会再查</span></span><br></pre></td></tr></table></figure>

<h3 id="4-6-GPU计算"><a href="#4-6-GPU计算" class="headerlink" title="4.6 GPU计算"></a>4.6 GPU计算</h3><p>到目前为止，我们都在用CPU九三，要高效必须使用GPU，本节中，会介绍如何使用单块的NVIDIA GPU来计算。<br>首先，确保已经安装了至少一块NVIDIA GPU。<br>然后，下载CUDA并按照提示设置好相应路径（附录C）<br>😂没有GPU<br>安葬好了用<code>!nvidia-smi</code>查看显卡信息<br>略 之后需要再看😓</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = nd.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], ctx=mx.gpu())   <span class="comment">#没有gpu😂 指定到gpu</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#copyto函数让变量在设备之间传递，下面将内存上的NDArray变量x复制到gpu(0)上</span></span><br><span class="line"><span class="comment">#copyto函数总是为目标变量开新的内存或显存</span></span><br><span class="line">y = x.copyto(mx.gpu())</span><br><span class="line">y.copyto(mx.gpu()) <span class="keyword">is</span> y   <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果源变量和目标变量的context一致，as_in_context函数使目标变量和源变量共享源变量的内内存或显存。</span></span><br><span class="line">z = x.as_in_context(mx.gpu())</span><br><span class="line">y.as_in_context(mx.gpu()) <span class="keyword">is</span> y <span class="comment"># True</span></span><br></pre></td></tr></table></figure>

<p>MXNet要求计算的所有输入数据都在内存或统一显卡的显存上。这样设计的原因是CPU和不同的GPU之间的数据交互通常比较耗时。</p>
<h2 id="第5章-卷积神经网络"><a href="#第5章-卷积神经网络" class="headerlink" title="第5章 卷积神经网络"></a>第5章 卷积神经网络</h2><p><img src="/2020/01/22/dive-into-deep-learning/cnn.png" alt="卷积神经网络"><br>哦豁！ 我学过😂 你已经赢过这场战斗啦， 再来一遍找点乐子吧！<br>我去 真的多😓<br>这几年取得突破性成就的基石，</p>
<ol>
<li>先介绍卷积神经网络中卷积层和池化层的工作原理</li>
<li>解释填充、步幅、输入通道和输出通道的含义。</li>
<li>探究数个具有代表性的深度卷积神经网络的设计思路<ul>
<li>AlexNET</li>
<li>NiN</li>
<li>GoogLeNet</li>
<li>ResNet</li>
<li>DenseNet</li>
</ul>
</li>
<li>虽然深度模型看上去只是具有很多层的神经网络，然而获得有效的深度模型并不容易。有幸的是，本章阐述的<strong>批量归一化</strong>和<strong>残差网络</strong>为训练和设计深度模型提供了两类重要思路。<h3 id="5-1-二维卷积层"><a href="#5-1-二维卷积层" class="headerlink" title="5.1 二维卷积层"></a>5.1 二维卷积层</h3>卷积神经网络是含有卷积层的神经网络。本章中介绍的卷积神经网络均使用最常见的二维卷积层，具有高和宽两个维度常用来处理图像数据。<br>本节中，我们将介绍简单形式的二维卷积层的工作原理。<h4 id="5-1-1-二维互相关运算"><a href="#5-1-1-二维互相关运算" class="headerlink" title="5.1.1 二维互相关运算"></a>5.1.1 二维互相关运算</h4>虽然卷积层得名于卷积运算，但我们通常在卷积层中使用更加直观的互相关运算。</li>
</ol>
<ul>
<li>卷积运算</li>
<li>互相关运算：在二维卷积层中，一个二维输入数组和一个二维核数组（卷积核或过滤器）通过互相关运算输出一个二维数组。<br>卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽。<br><img src="/2020/01/22/dive-into-deep-learning/conv.png" alt="conv"></li>
</ul>
<p>自己看图贼好理解。<br>代码实现在corr2d函数里。它接受输入数组X与核数组K，并输出数组Y。</p>
<h4 id="5-1-2-二维卷积层"><a href="#5-1-2-二维卷积层" class="headerlink" title="5.1.2 二维卷积层"></a>5.1.2 二维卷积层</h4><p>是前面互相关运算深化：<br>二维卷积层将输入和卷积做互相关运算，并加上一个标量偏差来得到输出。并在训练模型的时候，先对卷积核做随机初始化。然后不断迭代卷积核（权重）和偏差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size, **kwargs)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__(**kwargs)</span><br><span class="line">        self.weight = self.params.get(<span class="string">'weight'</span>, shape=kernel_size)</span><br><span class="line">        self.bias = self.params.get(<span class="string">'bias'</span>, shape=(<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight.data()) + self.bias.data()</span><br></pre></td></tr></table></figure>

<h4 id="5-1-3-图像中物体边缘检测"><a href="#5-1-3-图像中物体边缘检测" class="headerlink" title="5.1.3 图像中物体边缘检测"></a>5.1.3 图像中物体边缘检测</h4><p>说明了卷积层可通过重复使用卷积核有效地表征局部空间 easy</p>
<h4 id="5-1-4-通过数据学习核数组"><a href="#5-1-4-通过数据学习核数组" class="headerlink" title="5.1.4 通过数据学习核数组"></a>5.1.4 通过数据学习核数组</h4><p>前面的卷积核是我们自己想的，也可以来迭代训练我们的卷积核参数：<br>首先构造一个卷积层，将其卷积核初始化成随机数组。接下来在每一次迭代中，我们使用平方误差来比较Y和卷积层的输出，然后计算梯度来更新权重。简单起见，这里的卷积层忽略偏差。</p>
<h4 id="5-1-5-互相关运算和卷积运算"><a href="#5-1-5-互相关运算和卷积运算" class="headerlink" title="5.1.5 互相关运算和卷积运算"></a>5.1.5 互相关运算和卷积运算</h4><p>互相关运算和卷积运算类似。为了得到卷积运算的输出，我么只需要将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出并不相同。<br>但是卷积层可以用互相关运算替代卷积运算，因为深度学习中核数组的参数都是学习出来的，两种运算不影响。为了与大多数文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。</p>
<h4 id="5-1-6-特征图和感受野"><a href="#5-1-6-特征图和感受野" class="headerlink" title="5.1.6 特征图和感受野"></a>5.1.6 特征图和感受野</h4><ul>
<li>特征图：就是二维卷积层输出的二维数组（可以看作输入在空间维度（宽和高）上某一级的表征）</li>
<li>感受野：影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）<br>可以使用更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。666！</li>
</ul>
<p>我们常使用“元素”一次来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可以被称为“单元”。当含义明确时，本书不对这两个术语做严格区分。</p>
<h3 id="5-2-填充和步幅"><a href="#5-2-填充和步幅" class="headerlink" title="5.2 填充和步幅"></a>5.2 填充和步幅</h3><p>在前面的例子里面我们可以看到，我们使用宽高为3的输入与宽高为2的卷积核得到宽高为2的输出。一般输入nhxnw,卷积核窗口形状是khxkw，那么输出形状将会是$(n_h-k_h+1) \times (n_w-k_w+1).$。所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。他们可以给定形状的输入和卷积核改变输出形状。</p>
<h4 id="5-2-1-填充"><a href="#5-2-1-填充" class="headerlink" title="5.2.1 填充"></a>5.2.1 填充</h4><blockquote>
<p><strong>填充</strong>是指在输入高和宽的两侧填充元素（通常是0元素）。</p>
</blockquote>
<p>例子在输入宽和高的两侧分别添加值为0的元素，使输入高和宽从3变成了5，并导致输出高和宽由2增加到4.图中的阴影部分为第一个输出元素机器计算所使用的输入和核数组元素。<br>一般来说，如果高的两侧一共填充ph行，在宽的两侧一共填充pw列，那么输出形状将会是：(nh-kh+ph+1)x(nw-kw+pw+1)也就是说，输出的高和宽会分别增加ph和pw。<br>很多情况下，我们会设置ph=kh-1和pw=kw-1来使输入和输出具有相同的高和宽。这样才方便在构造网络时推测每个层的输出形状。<br>卷积神经网络经常使用奇数高和宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。<br>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有不相同的高和宽。</p>
<h4 id="5-2-2-步幅"><a href="#5-2-2-步幅" class="headerlink" title="5.2.2 步幅"></a>5.2.2 步幅</h4><blockquote>
<p>我们将每次滑动的行数和列数称为步幅。</p>
</blockquote>
<p>有些根据啥步幅来计算的公式加上输入来计算输出自己看吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面我们令高和宽上的步幅均为2，从而使输入的高和宽减半。</span></span><br><span class="line">conv2d = nn.Conv2D(<span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, strides=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>

<p>复杂一点，魔鬼的步伐：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2D(<span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), strides=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>

<p>填充数：(ph, pw)。步幅数：(sh, sw)<br>默认情况下，填充为0，步幅为1。</p>
<p>**<br>填充可以增加输出的高和宽。常用来使输出与输入具有相同的高和宽。<br>步幅可以减少输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n。<br>**</p>
<h3 id="5-3-多输入通道和多输出通道"><a href="#5-3-多输入通道和多输出通道" class="headerlink" title="5.3 多输入通道和多输出通道"></a>5.3 多输入通道和多输出通道</h3><p>之前研究都是二维数组，但真实又RGB三个通道。本节介绍含多个输入通道和多个输出通道的卷积核。</p>
<h4 id="5-3-1-多输入通道"><a href="#5-3-1-多输入通道" class="headerlink" title="5.3.1 多输入通道"></a>5.3.1 多输入通道</h4><p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。<br>例子时单输出通道，可以通过add_n函数，对每个通道做互相关运算，然后通过add_n函数来进行累加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="comment"># 首先沿着X和K的第0维（通道维）遍历。然后使用*将结果列表变成add_n函数的位置参数</span></span><br><span class="line">    <span class="comment"># （positional argument）来进行相加</span></span><br><span class="line">    <span class="keyword">return</span> nd.add_n(*[d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> zip(X, K)])</span><br><span class="line"></span><br><span class="line">X = nd.array([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]],</span><br><span class="line">              [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]])</span><br><span class="line">K = nd.array([[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]], [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br></pre></td></tr></table></figure>

<h4 id="5-3-2-多输出通道"><a href="#5-3-2-多输出通道" class="headerlink" title="5.3.2 多输出通道"></a>5.3.2 多输出通道</h4><p>修改卷积核让形状为：c0xcixkhxkw，在做互相关运算时，每个输出通道上的结构由卷积核在该输出通道上的核数组与整个输入数组计算而来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">K = nd.stack(K, K + <span class="number">1</span>, K + <span class="number">2</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="comment"># 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起</span></span><br><span class="line">    <span class="keyword">return</span> nd.stack(*[corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K])</span><br></pre></td></tr></table></figure>

<h4 id="5-3-3-1-x-1-卷积层"><a href="#5-3-3-1-x-1-卷积层" class="headerlink" title="5.3.3 1 x 1 卷积层"></a>5.3.3 1 x 1 卷积层</h4><p>最后讨论卷积窗口形状为1x1的多通道卷积层。因为使用了最小窗口，1x1卷积失去了卷积层可以识别宽和高维度上相邻元素狗层的模式的功能。实际上1x1卷积核的主要计算发生在通道维上。<br>看不懂，不说人话，大概：<br>假设我们将通道维当特征维，将高和宽维度上的元素当成数据样本，那么1x1卷积层的作用与全链接层等价。<br><strong><em>作用：在之后的模型里我们将看到1x1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度</em></strong></p>
<h3 id="5-4-池化层"><a href="#5-4-池化层" class="headerlink" title="5.4 池化层"></a>5.4 池化层</h3><p>之前，我们在边缘检测的例子里，我们构造卷积核从而精确地找到了像素变化的位置。<br>但实际问题中，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能在出现在卷积层输出Y中的不同位置，进而对后面的模式识别造成了不便。<br>本节中我们介绍池化层，它的提出是为了缓解卷积层对位置的过度敏感性。</p>
<h4 id="5-4-1-二维最大池化层平均池化层"><a href="#5-4-1-二维最大池化层平均池化层" class="headerlink" title="5.4.1 二维最大池化层平均池化层"></a>5.4.1 二维最大池化层平均池化层</h4><p>真6666，在卷积层后面接一个池化层，当卷积层像素有浮动变化也能检测出来，太6了。<br>设卷积层输入是X、池化层（最大池化层）输出为Y。无论是X[i, j]和X[i, j+1]值不同还是X[i, j+1]和X[i, j+2]不同，池化层输出均有Y[i, j]=1。也就是说，使用2x2最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。<br>实现与卷积层差不多，只是最后的输出不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span><span class="params">(X, pool_size, mode=<span class="string">'max'</span>)</span>:</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = nd.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">'max'</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].max()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">'avg'</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()       </span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<h4 id="5-4-2-填充和步幅"><a href="#5-4-2-填充和步幅" class="headerlink" title="5.4.2 填充和步幅"></a>5.4.2 填充和步幅</h4><p>工作原理同卷积层一样，改变填充和步幅来改变输出形状。<br>可以用nn模块里的二维最大池化MaxPool2D来演示<br>so easy</p>
<h4 id="5-4-3-多通道"><a href="#5-4-3-多通道" class="headerlink" title="5.4.3 多通道"></a>5.4.3 多通道</h4><p>与卷积层不同，池化层对每个输入通道分别池化，而不是向卷积层那样按照通道相加。意味着池化层的输出通道数与输入通道数相等。<br>easy</p>
<h3 id="5-5-卷积神经网络（LeNet）"><a href="#5-5-卷积神经网络（LeNet）" class="headerlink" title="5.5 卷积神经网络（LeNet）"></a>5.5 卷积神经网络（LeNet）</h3><p>之前我们构造一个单隐藏层的多层感知机来对Fashion-MNIST数据集中的图像进行分类。每张宽高均是28像素。我们将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中。<br>局限性：</p>
<ul>
<li>图像在同一列邻近的的像素在这个向量中可能相距较远，他们构成的模式可能难以被模型识别。（有些特征不能被识别）</li>
<li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li>
</ul>
<p>卷积层尝试解决这个问题：</p>
<ul>
<li>一方面，卷积层尝保留输入形状，使图像的像素在高和宽两个方向的相关性均可能被有效识别</li>
<li>另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免尺寸过大。<br>卷积神经网络就是含卷积层的网络。<h4 id="5-5-1-LeNet模型"><a href="#5-5-1-LeNet模型" class="headerlink" title="5.5.1 LeNet模型"></a>5.5.1 LeNet模型</h4><blockquote>
<p>LeNet分为卷积层块和全连接层块两个部分。</p>
</blockquote>
</li>
</ul>
<p>卷积层块和全连接层块都有特点，so cool<br>书上贼详细 还有代码 不复制粘贴太多<br>可以看到，在卷积层中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10</p>
<h4 id="5-5-2-训练模型"><a href="#5-5-2-训练模型" class="headerlink" title="5.5.2 训练模型"></a>5.5.2 训练模型</h4><h3 id="5-6-深度卷积神经网络（AlexNet）"><a href="#5-6-深度卷积神经网络（AlexNet）" class="headerlink" title="5.6 深度卷积神经网络（AlexNet）"></a>5.6 深度卷积神经网络（AlexNet）</h3><p>先介绍一些历史 翻译的好差看不懂， 好像是当初数据集和硬件不够，深度学习很困难，机器学习开始热门，机器学习使用传统的方法用已有的特征提取函数生成图像的特征，再利用机器学习模型对图像的特征分类（通过勤劳与只会设计并生成的手工特征），但是其实本质上那些特征也是可以通过深度学习出来的。用较干净的数据集和较有效的特征去学习更好。</p>
<h4 id="5-6-1-学习特征表示"><a href="#5-6-1-学习特征表示" class="headerlink" title="5.6.1 学习特征表示"></a>5.6.1 学习特征表示</h4><ol>
<li>缺失要素一： 数据</li>
<li>缺失要素二： 硬件<h4 id="5-6-2-AlexNet"><a href="#5-6-2-AlexNet" class="headerlink" title="5.6.2 AlexNet"></a>5.6.2 AlexNet</h4>AlexNet与LeNet的设计理念非常类似，但又有显著区别：</li>
<li>AlexNet与较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li>
<li>AlexNet将sigmoid激活函数改成了更简单的ReLU激活函数（好处书上有）。</li>
<li>AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。</li>
<li>AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</li>
</ol>
<p>模型书上介绍很详细 很酷</p>
<h4 id="5-6-3-读取数据集"><a href="#5-6-3-读取数据集" class="headerlink" title="5.6.3 读取数据集"></a>5.6.3 读取数据集</h4><p>ImageNet数据集训练时间较长，仍然使用Fashion-MINST数据集来演示AlexNet。要把数据集Resize成224.</p>
<h4 id="5-6-4-训练模型"><a href="#5-6-4-训练模型" class="headerlink" title="5.6.4 训练模型"></a>5.6.4 训练模型</h4><p>学习率减小，其他差不多</p>
<h3 id="5-7-使用重复元素的网络（VGG）"><a href="#5-7-使用重复元素的网络（VGG）" class="headerlink" title="5.7 使用重复元素的网络（VGG）"></a>5.7 使用重复元素的网络（VGG）</h3><p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则来指导后来的研究者如何设计新的网络。我们在本章的后续几节介绍不同的深度网络设计思路<br>本节介绍了VGG，它提出了可以通过重复使用简单的基础块来构建深度模型的思路</p>
<h4 id="5-7-1-VGG块"><a href="#5-7-1-VGG块" class="headerlink" title="5.7.1 VGG块"></a>5.7.1 VGG块</h4><p>连续使用数个相同的填充为1，窗口形状为3x3的卷积层后接上一个步幅为2、窗口形状为2x2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。</p>
<h4 id="5-7-2-VGG网络"><a href="#5-7-2-VGG网络" class="headerlink" title="5.7.2 VGG网络"></a>5.7.2 VGG网络</h4><p>和AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。定义conv_arch来指定多个卷积块的大小，池化层每次将输入的高和宽减半，同时每次把输出通道数翻倍。因为每个卷积层的窗口大小一样，所以多数卷积层都有相同的模型参数尺寸和计算复杂度。<br>不同就是卷积层都是重复的。</p>
<h4 id="5-7-3-训练模型"><a href="#5-7-3-训练模型" class="headerlink" title="5.7.3 训练模型"></a>5.7.3 训练模型</h4><h3 id="5-8-网络中的网络（NiN）"><a href="#5-8-网络中的网络（NiN）" class="headerlink" title="5.8 网络中的网络（NiN）"></a>5.8 网络中的网络（NiN）</h3><p>之前VGG和AlexNet在设计上对LeNet的改进主要是在于如何对这两个（卷积层块和全连接层块）进行加宽（增加通道数）和加深。本节介绍NiN。它提出另外一个思路，即串联多个由卷积层和“全连接”层构成小网络来构建一个深层网络。</p>
<h4 id="5-8-1-NiN块"><a href="#5-8-1-NiN块" class="headerlink" title="5.8.1 NiN块"></a>5.8.1 NiN块</h4><p>卷积层的输入和输出一般是四维数组（样本，通道，高，宽），而全连接层的输入和输出通常是二维数组（样本，特征）。如果想再全连接层后再接上卷积层，则需要将全连接层输出变为四维。之前说的1x1卷积层可以替代全连接层解决这个问题。<br>NiN块是NiN中的基础块，它由一个卷积层加上两个充当全连接层的1x1卷积层串联而成，其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</p>
<h4 id="5-8-2-NiN模型"><a href="#5-8-2-NiN模型" class="headerlink" title="5.8.2 NiN模型"></a>5.8.2 NiN模型</h4><p>NiN和AlexNet有很多相似的地方。NiN使用卷积窗口形状分别为11x11, 5x5和3x3的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为3x3的最大池化层。<br>有与AlexNet显著不同：<br>NiN去掉了最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层（窗口形状等于输入空间维形状的平均池化层）对每个通道中所有元素求平均并直接用于分类。NiN的这个设计的好处是可以显著减小模型参数尺寸而缓解过拟合。但是会增加训练时间。</p>
<h4 id="5-8-3-训练模型"><a href="#5-8-3-训练模型" class="headerlink" title="5.8.3 训练模型"></a>5.8.3 训练模型</h4><p>相似 学习率更大</p>
<h3 id="5-9-含并行连结的网络（GoogLeNet）"><a href="#5-9-含并行连结的网络（GoogLeNet）" class="headerlink" title="5.9 含并行连结的网络（GoogLeNet）"></a>5.9 含并行连结的网络（GoogLeNet）</h3><p>GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上足了很大改进。</p>
<h4 id="5-9-1-Inception块"><a href="#5-9-1-Inception块" class="headerlink" title="5.9.1 Inception块"></a>5.9.1 Inception块</h4><p>基础卷积块叫做Inception块，结构更复杂。<br>有4条并行的线路。前3条1x1，3x3，5x5的卷积层来抽取不同空间尺寸下的信息。第4条最大池化层。<br>Inception块中自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</p>
<h4 id="5-9-2-GoogLeNet模型"><a href="#5-9-2-GoogLeNet模型" class="headerlink" title="5.9.2 GoogLeNet模型"></a>5.9.2 GoogLeNet模型</h4><p>我去模型有5个模块贼复杂，自己看懂肯定没问题，会深入研究就不一定了。自己看。</p>
<h4 id="5-9-3-训练模型"><a href="#5-9-3-训练模型" class="headerlink" title="5.9.3 训练模型"></a>5.9.3 训练模型</h4><h3 id="5-10-批量归一化"><a href="#5-10-批量归一化" class="headerlink" title="5.10 批量归一化"></a>5.10 批量归一化</h3><p>之前我们是对数据标准化预处理。对浅层就够了，随着模型训练的惊醒，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已经标准化，训练汇总模型参数的更行依然很容易造成靠近输出层输出剧烈变化。计算数值不稳定很难训练出有效的深度模型。<br>批量归一化的提出正式为了应对深度模型训练的挑战。批量归一化利用<strong>小批量</strong>上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在在各层的中间输出数值更稳定。<strong>有两种方法批量归一化和残差网络。</strong></p>
<h4 id="5-10-1-批量归一化层"><a href="#5-10-1-批量归一化层" class="headerlink" title="5.10.1 批量归一化层"></a>5.10.1 批量归一化层</h4><p>对全连接层和卷积层做批量归一化会有不同。</p>
<ol>
<li>对全连接层做批量归一化<br> 对全连接层做批量归一化，一般将批量归一化置于仿射变换和激活函数之间。需要对小批量求均值和方差。可以学习拉伸参数和偏移参数，但是如果批量归一化无益学出来的模型可以不使用批量归一化。</li>
<li>对卷积层做批量归一化<br> 对卷积层来说，批量归一化发生在卷积运算之后、应用激活函数之前。如果卷积层有多个通道，就对每个通道做独立的批量归一化拥有独立的拉伸和偏移参数，均为标量。单通道就需要对所有mxpxq元素用一个均值和方差，即该通道中mxpxq个元素的均值方差。</li>
<li>预测时的批量归一化<br> 区分训练和预测。训练的时候，我们将批量大小设置大一些，从而使批量内样本的均值和方差的计算都较为准确。预测的时候，我们希望模型对任意输入有确定的输出，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。<br> 解决方法：<ul>
<li>通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。和丢弃法一样，批量归一化在训练模式和预测模式下的计算结果不一样。<h4 id="5-10-2-从零开始实现"><a href="#5-10-2-从零开始实现" class="headerlink" title="5.10.2 从零开始实现"></a>5.10.2 从零开始实现</h4>通过NDArray来实现批量归一化层<br>定义batch_norm函数：计算均值方差做归一化操作<br>定义BatchNorm层：保存参与求梯度和迭代的拉伸参数和偏移参数，同时也维护移动平均得到的均值和方差，方便在预测的时候被使用。指定的num_features参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的num_dims参数对全连接层和卷积层来说分别为2和4。<h4 id="5-10-3-使用批量归一化层的的LeNet"><a href="#5-10-3-使用批量归一化层的的LeNet" class="headerlink" title="5.10.3 使用批量归一化层的的LeNet"></a>5.10.3 使用批量归一化层的的LeNet</h4>在所有的卷积层或全连接层、激活层之前加入批量归一化层。<h4 id="5-10-4-简洁实现"><a href="#5-10-4-简洁实现" class="headerlink" title="5.10.4 简洁实现"></a>5.10.4 简洁实现</h4>简洁实现不需要指定num_features和num_dims。这些参数都将通过延后初始化而自动获取。</li>
</ul>
</li>
</ol>
<h3 id="5-11-残差网络（ResNet）"><a href="#5-11-残差网络（ResNet）" class="headerlink" title="5.11 残差网络（ResNet）"></a>5.11 残差网络（ResNet）</h3><p>对神经网络模型添加新的层，充分训练后理论上有可能更容易降低训练误差，然而在时间中，添加过多的层后训练误差往往不降反升。即使用批量归一化带来数值的稳定仍然不行。</p>
<h4 id="5-11-1-残差块"><a href="#5-11-1-残差块" class="headerlink" title="5.11.1 残差块"></a>5.11.1 残差块</h4><p>啥残差映射f(x)-x。右图虚线框中的部分则需要拟合出有关恒等映射的残差映射f(x)-x。残差映射在实际中往往更容易优化。当然也可以学习成理想理想映射，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。这个就是残差块，在残差块中，输入可通过跨层的数据线路更快地向前传播。<br><strong><em>设输入为x。假设图中最上方激活函数输入的理想映射为f(x)。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分需要拟合出有关恒等映射的残差映射f(x)-x</em></strong><br>沿用了VGG全3x3卷积层的设计。残差块里首先有2个相同输出通道数的3x3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样设计要求2个卷积层的输出于输入形状一样，从而可以相加。如果想改变通`道数，就需要引入一个额外的1x1卷积层来将输入变换成需要的形状后再做相加运算。</p>
<h4 id="5-11-2-ResNet模块"><a href="#5-11-2-ResNet模块" class="headerlink" title="5.11.2 ResNet模块"></a>5.11.2 ResNet模块</h4><p>ResNet的前两层跟之前介绍的GooLeNet一样，在输出通道数为64，步幅为2的7x7卷积层后接步幅为2的3x3的最大池化层。不同指出在于ResNet每个卷积层后等价批量归一化层。<br>后面几层ResNet用了4个由残差块组成的模块，每个模块使用若干同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于以前用了步幅为2的最大池化层，无须减小高和宽。之后每个模块在第一个残差块将上一个模块的通道数翻倍，并将高和宽减半。<br>最后加上全局平均池化层后接上全连接层输出。<br>这里每个Res模块有4个卷积层（不计1x1层）加上头尾共有18层，可以很方便的进行配置。<br>我去，贼复杂，但是效果好。</p>
<h4 id="5-11-3-训练模型"><a href="#5-11-3-训练模型" class="headerlink" title="5.11.3 训练模型"></a>5.11.3 训练模型</h4><h3 id="5-12-稠密连接网络（DenseNet）"><a href="#5-12-稠密连接网络（DenseNet）" class="headerlink" title="5.12 稠密连接网络（DenseNet）"></a>5.12 稠密连接网络（DenseNet）</h3><p>ResNet中的跨层连接设计引申出数个后续工作。本节介绍一个稠密连接网络，主要区别如图：<br>在跨层上有主要区别：使用相加和使用连结。<br>于ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A相加，而是在通道维上连结。这样模块A的输出可以直接传入模块B后面的层。在这个设计里，模块A直接跟模块B后面的所有层连接在了一起。<br>DenseNet的主要构建模块是稠密块和过度层。</p>
<ul>
<li>稠密块：定义了输入和输出是如何连结的。</li>
<li>过渡层：用来控制通道数，使之不过大。</li>
</ul>
<h4 id="5-12-1-稠密块"><a href="#5-12-1-稠密块" class="headerlink" title="5.12.1 稠密块"></a>5.12.1 稠密块</h4><p>使用了ResNet改良版的“批量归一化、激活和卷积”结构。在“conv_block”函数中实现这个结构。稠密块由多个conv_block组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。<br><strong><em>卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率</em></strong></p>
<h4 id="5-12-2-过渡层"><a href="#5-12-2-过渡层" class="headerlink" title="5.12.2 过渡层"></a>5.12.2 过渡层</h4><p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡用来控制模型复杂度。它通过1x1卷积层来减小通道数，并使用步幅为2的平均池化层来减半高和宽，从而进一步降低模型复杂度。</p>
<h4 id="5-12-3-DenseNet模型"><a href="#5-12-3-DenseNet模型" class="headerlink" title="5.12.3 DenseNet模型"></a>5.12.3 DenseNet模型</h4><p>贼复杂 巴拉巴拉</p>
<ul>
<li>首先使用同ResNet一样的单卷积层和最大池化层。</li>
<li>类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。我们可以设置每个稠密块有多少个卷积层。稠密块的卷积层通道数等于增长率。（ResNet使用4个残差块）</li>
<li>我们使用过度层来减半高和宽（ResNet使用步幅为2的残差块在每个模块之间减小高和宽）</li>
<li>ResNet一样，最后使用全局池化层和全连接层来输出。<h4 id="5-12-4-训练模型"><a href="#5-12-4-训练模型" class="headerlink" title="5.12.4 训练模型"></a>5.12.4 训练模型</h4>终于刷完卷积神经网络了 我去 有些复杂 之后也能看。</li>
</ul>
<h2 id="第7章-优化算法"><a href="#第7章-优化算法" class="headerlink" title="第7章 优化算法"></a>第7章 优化算法</h2><p><img src="/2020/01/22/dive-into-deep-learning/opt.png" alt="优化算法"><br>训练模型，使用优化算法不断迭代模型参数来降低模型损失函数的值。迭代终止，模型的训练随之终止，此时的模型参数就是模型通过训练所学习到的参数。<br>优化算法十分重要：</p>
<ul>
<li>一方面，训练一个复杂的深度学习模型可能需要数小时，数日，甚至数周的时间，而优化算法的表现直接影响模型的训练效率；</li>
<li>另一方面，理解各种优化算法的原理以及其中超参数的意义将有助于我们更有针对性的调参，从而使深度学习模型表现更好。<h3 id="7-1-优化与深度学习"><a href="#7-1-优化与深度学习" class="headerlink" title="7.1 优化与深度学习"></a>7.1 优化与深度学习</h3>深度学习中，我们通常会预先定义一个损失函数，有了损失函数以后，我们可以使用优化算法试图将其最小化。在优化中，这样的<strong>损失函数</strong>通常被称作优化问题的目标函数。依据惯例，优化算法通常只考虑最小化目标函数。其实任何最大化问题都可以很容易地转化为最小化问题。<h4 id="7-1-1-优化与深度学习的关系"><a href="#7-1-1-优化与深度学习的关系" class="headerlink" title="7.1.1 优化与深度学习的关系"></a>7.1.1 优化与深度学习的关系</h4>实际上优化和深度学习的目标有区别。</li>
<li>优化是为了降低训练误差</li>
<li>深度学习是为了降低泛化误差。<br>为了降低泛化为茶，除了使用优化算法降低训练误差以外，还需要注意应对过拟合。<br>本章中，我们值关注优化算法在最小目标函数上的表现，而不关注模型的泛化误差。<h4 id="7-1-2-优化在深度学习中的挑战"><a href="#7-1-2-优化在深度学习中的挑战" class="headerlink" title="7.1.2 优化在深度学习中的挑战"></a>7.1.2 优化在深度学习中的挑战</h4>之前说优化问题有解析解和数值解，深度学习中绝大多数目标函数都很复杂。因此，很多优化问题不存在解析解，而需要基于数值方法的优化算法找到近似解，即数值解。讨论的优化算法都是这类基于数值方法的算法。为了求得最小化目标函数的数值解，我们将通过优化算法有限次迭代模型参数来尽可能地降低损失函数的值。<br>优化算法有两个挑战：局部最小值和鞍点。</li>
</ul>
<ol>
<li><p>局部最小值<br>对于目标函数$f(x)$，如果$f(x)$在$x$上的值比在$x$邻近的其他点的值更小，那么$f(x)$可能是一个局部最小值（local minimum）。如果$f(x)$在$x$上的值是目标函数在整个定义域上的最小值，那么$f(x)$是全局最小值（global minimum）。<br>深度学习模型的目标函数可能有若干局部最优解。当一个优化问题的数值解在局部最有解附近时，周围梯度近似为0，最终迭代求得的数值解可能令目标函数局部最小化而非全局最小化。</p>
</li>
<li><p>鞍点<br>梯度接近或变成0可能时由于当前解在局部最优解附近造成的。事实上，另一种可能性时当前解在鞍点附近。<br>f(x)=x3和f(x,y)=x2-y2我们可以找出函数的鞍点位置。<br>在图f(x,y)=x2-y2的鞍点位置，目标函数在x轴方向上时局部最小值，但在y轴方向上时局部最大值。</p>
<blockquote>
<p>假设一个函数的输入为k维向量，输出为标量，那么它的海森矩阵有k个特征值。该函数在梯度为0的位置上可能时局部最小值、局部最大值或者鞍点。</p>
</blockquote>
</li>
</ol>
<ul>
<li>当函数的海森矩阵在梯度为0的位置上的特征值全为正时，该函数得到局部最小值。</li>
<li>当函数的海森矩阵在梯度为0的位置上的特征值全为负时，该函数得到局部最大值。</li>
<li>当函数的海森矩阵在梯度为0的位置上的特征值有正有负时，该函数得到鞍点。</li>
</ul>
<p>由随机矩阵理论，由于深度学习模型参数参数通常都是高维的（k很大），<strong>目标函数的鞍点通常比局部最小值更常见。</strong></p>
<p>在深度学习中，虽然找到目标函数的全局最优解很难，但这<strong>并非必要</strong>。我们将在本章接下来的几节中逐一介绍常用优化算法，在很多实际问题中都能训练出十分有效的深度学习模型。</p>
<h3 id="7-2-梯度下降和随机梯度下降"><a href="#7-2-梯度下降和随机梯度下降" class="headerlink" title="7.2 梯度下降和随机梯度下降"></a>7.2 梯度下降和随机梯度下降</h3><p>随机梯度很少使用，但是理解梯度的意义以及沿着梯度反方向更新自变量可能降低目标函数值的原因是学习后续优化算法的基础，随后引出随机梯度下降。</p>
<h4 id="7-2-1-一维梯度下降"><a href="#7-2-1-一维梯度下降" class="headerlink" title="7.2.1 一维梯度下降"></a>7.2.1 一维梯度下降</h4><p>以简单的以为梯度下降，解释梯度下降算法可能降低目标函数值的原因。（数学理论easy 数学贼差😂）<br><img src="/2020/01/22/dive-into-deep-learning/gd1.png" alt="卷积神经网络"><br>观察f(x)=x2的一维梯度下降</p>
<h4 id="7-2-2-学习率"><a href="#7-2-2-学习率" class="headerlink" title="7.2.2 学习率"></a>7.2.2 学习率</h4><p>用实际例子观察学习率的影响：<br><img src="/2020/01/22/dive-into-deep-learning/gd2.png" alt="卷积神经网络"><br><img src="/2020/01/22/dive-into-deep-learning/gd3.png" alt="卷积神经网络"></p>
<ul>
<li><p>学习率过小导致x更新缓慢从而需要更多的迭代才能得到较好的解。</p>
</li>
<li><p>学习率过大���|nf’(x)|可能会过大从而使前面提到的一阶泰勒展开式不再成立：这时我们无法保证迭代x会降低f（x）的值。</p>
<h4 id="7-2-3-多维梯度下降"><a href="#7-2-3-多维梯度下降" class="headerlink" title="7.2.3 多维梯度下降"></a>7.2.3 多维梯度下降</h4><p>前面是一维梯度下降，考虑更广大的情况：目标函数的输入为向量，输入是一个d维向量。目标函数f(x)有关x的梯度是一个由d个偏导数组成的向量。<br>定义学习率，自变量，自变量状态看代码很容易自己理解。<br><img src="/2020/01/22/dive-into-deep-learning/nd.png" alt="nd"></p>
<h4 id="7-2-4-随机梯度下降"><a href="#7-2-4-随机梯度下降" class="headerlink" title="7.2.4 随机梯度下降"></a>7.2.4 随机梯度下降</h4><p>深度学习，目标函数通常是训练数据集中有关各个样本的损失函数的平均。设fi(x)是有关索引为i的训练数据的损失函数，n是训练数据样本数，x是模型的参数向量。<br>如果使用梯度下降，每次自变量迭代的计算开销为O(n),它随着n线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。<br>哦哦晓得了，之前是把所有的样本求采样来做迭代，sgd就是随机均匀采样一个样本索引，并计算梯度来迭代x。每次迭代的计算开销从梯度下降的O(n)降到了常数O(1)。随机梯度是对梯度的无偏估计，这意味着，平均来说，随机梯度是对梯度的很好一个良好的估计。</p>
<h3 id="7-3-小批量随机梯度下降"><a href="#7-3-小批量随机梯度下降" class="headerlink" title="7.3 小批量随机梯度下降"></a>7.3 小批量随机梯度下降</h3></li>
<li><p>batch-size: 之前说的batch，你看下面，小批量是用来分次迭代的（最后会把所有样本迭代完），每次会记录这次批量的损失，然后进行梯度下降优化模型，就相当于一次次利用小的batch来优化，epoch就是重复实现多次（保留损失函数和模型参数（[w, b]），内存不消就不会消失）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X</span></span><br><span class="line">    <span class="comment"># 和y分别是小批量样本的特征和标签</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="keyword">with</span> autograd.record(): <span class="comment">#打开记录梯度</span></span><br><span class="line">            l = loss(net(X, w, b), y)  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line">    train_l = loss(net(features, w, b), labels)   <span class="comment">#记录损失</span></span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().asnumpy()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad / batch_size   <span class="comment">#看见没有 这里param.grad就是利用梯度做优化</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>批量梯度下降：每一次迭代中，梯度下降使用整个训练数据集来计算梯度</p>
</li>
<li><p>随机梯度下降：在每次迭代中只随机采样一个样本来计算梯度。</p>
</li>
<li><p>小批量随机梯度下降：我们可以在每轮迭代中随机均匀采样多个样本来组成一个小批量</p>
</li>
</ul>
<p>描述小批量随机梯度下降：<br>？？时间步？？<br>小批量随机梯度下降随机均匀采样一个由训练数据样本索引组成的小批量Bi，我们可以通过重复采样（允许同一个小批量出现重复的样本）或者不重复采样得到一个小批量中的各个样本。计算时间步t的小批量Bi上目标函数位于xt-1处的梯度。｜B｜代表批量大小，小批量中的样本个数，是一个超参数。同随机梯度一样也是无偏估计。给定学习率nt（取正数），有小批量随机梯度下降对自变量的迭代。<br>基于随机采样得到的梯度的方差在迭代过程中无法减小（随机采样的都一样），因此会不断衰减学习率，这样学习率和随机梯度乘积的方差会不断减小。而梯度下降在迭代过程中一直使用目标函数的真实梯度，无须自我衰减学习率。<br>批量大小也对效率有影响，计算开销为O(B)。当批量大小为1时为随机梯度下降，当批量大小等于训练样本时，为梯度下降。当批量小，并行内存效率低，迭代时间长。当批量大，每个小批量梯度里面可能含有更多的冗余信息。为了得到更好的解，批量较大时比批量较小时需要计算的样本数目可能更多，例如增大迭代周期数。</p>
<h4 id="7-3-1-读取数据集"><a href="#7-3-1-读取数据集" class="headerlink" title="7.3.1 读取数据集"></a>7.3.1 读取数据集</h4><p>简单，就是读取新的NASA的数据集，使用前1500个样本和5个特征，并用标准化对数据进行预处理。</p>
<h4 id="7-3-2-从零开始实现"><a href="#7-3-2-从零开始实现" class="headerlink" title="7.3.2 从零开始实现"></a>7.3.2 从零开始实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, states, hyperparams)</span>:</span>     <span class="comment">#hyperparams超参数的意思  状态输入states可能以后用 训练模式和预测模式sgd不一样</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">        p[:] -= hyperparams[<span class="string">'lr'</span>] * p.grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(trainer_fn, states, hyperparams, features, labels,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">10</span>, num_epochs=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line">    w = nd.random.normal(scale=<span class="number">0.01</span>, shape=(features.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    b = nd.zeros(<span class="number">1</span>)</span><br><span class="line">    w.attach_grad()</span><br><span class="line">    b.attach_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_loss</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> loss(net(features, w, b), labels).mean().asscalar()</span><br><span class="line"></span><br><span class="line">    ls = [eval_loss()]</span><br><span class="line">    data_iter = gdata.DataLoader(</span><br><span class="line">        gdata.ArrayDataset(features, labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch_i, (X, y) <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                l = loss(net(X, w, b), y).mean()  <span class="comment"># 使用平均损失 这里是.mean平均，然后sgd不需要除以batch_size，之前是叠加</span></span><br><span class="line">            l.backward()</span><br><span class="line">            trainer_fn([w, b], states, hyperparams)  <span class="comment"># 迭代模型参数states没有用</span></span><br><span class="line">            <span class="keyword">if</span> (batch_i + <span class="number">1</span>) * batch_size % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                ls.append(eval_loss())  <span class="comment"># 每100个样本记录下当前训练误差</span></span><br><span class="line">    <span class="comment"># 打印结果和作图</span></span><br><span class="line">    print(<span class="string">'loss: %f, %f sec per epoch'</span> % (ls[<span class="number">-1</span>], time.time() - start))</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    d2l.plt.plot(np.linspace(<span class="number">0</span>, num_epochs, len(ls)), ls)</span><br><span class="line">    d2l.plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">    d2l.plt.ylabel(<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure>

<p>再测试三种不同大小batch_size(小批量随机梯度下降的小批量)下的训练效果。</p>
<h4 id="7-3-3-简洁实现"><a href="#7-3-3-简洁实现" class="headerlink" title="7.3.3 简洁实现"></a>7.3.3 简洁实现</h4><h3 id="7-4-动量法"><a href="#7-4-动量法" class="headerlink" title="7.4 动量法"></a>7.4 动量法</h3><p>之前我们的梯度下降，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫做最陡下降。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量，但是如果仅仅只根据当前位置更新会带来一些问题（问题如下）：</p>
<h4 id="7-4-1-梯度下降的问题"><a href="#7-4-1-梯度下降的问题" class="headerlink" title="7.4.1 梯度下降的问题"></a>7.4.1 梯度下降的问题</h4><p>给了实际例子二维输入二维输出，目标函数f(x)=0.1x12+2x22，用学习率0.1较小可以避免自变量在数值方向上乐鼓目标函数最优解，然而会造成自变量在水平方向上朝最优解移动变慢。当学习率0.6较大，此时自变量在竖直方向不断越过最优解并逐渐变发散。问题！！！！</p>
<h4 id="7-4-2-动量法"><a href="#7-4-2-动量法" class="headerlink" title="7.4.2 动量法"></a>7.4.2 动量法</h4><p>用动量法可以解决这个问题，改变迭代迭代的方式：<br>原来：<br>$$\boldsymbol{x}<em>t \leftarrow \boldsymbol{x}</em>{t-1} - \eta_t \boldsymbol{g}<em>t.$$<br>现在：<br>$$<br>\begin{aligned}<br>\boldsymbol{v}_t &amp;\leftarrow \gamma \boldsymbol{v}</em>{t-1} + \eta_t \boldsymbol{g}<em>t, \<br>\boldsymbol{x}_t &amp;\leftarrow \boldsymbol{x}</em>{t-1} - \boldsymbol{v}_t,<br>\end{aligned}<br>$$<br>当gamma等于0.5就收获比较好的效果，解决这个问题。</p>
<ol>
<li>指数加权移动平均<br>要从数学上理解动量法，就要先理解指数加权移动平均。<br>哎，数学菜的一批，结论大概是这个样子：<br>在实际中，我们常常将yt看作是对最近1/(1-gamma)个时间步的xt值的加权平均。而且，离当前时间步t越近的xt值获得的权重越大（越接近1）</li>
<li>由指数加权平均理解动量法<br>对动量法的速度变量做变形。由指数加权移动平均的形式可以得，速度变量vt实际上是对序列做了指数加权移动平均。</li>
</ol>
<p><strong>换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的1/(1-y)个时间步的更新量做了指数加权移动平均后再除以1-y。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决于当前梯度，还取决于过去的各个梯度在各个方向上是否一致。之前的问题都是所有梯度在水平方向上为正（向右），竖直发那个相爱能够上时正时负，这样就可以使用较大的学习率，从而使自变量向最有解更快移动。</strong></p>
<h4 id="7-4-3-从零开始实现"><a href="#7-4-3-从零开始实现" class="headerlink" title="7.4.3 从零开始实现"></a>7.4.3 从零开始实现</h4><p>实现就是更改以下优化算法的实现，加入动量：<br>动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量states表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span><span class="params">()</span>:</span></span><br><span class="line">    v_w = nd.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    v_b = nd.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, states, hyperparams)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        v[:] = hyperparams[<span class="string">'momentum'</span>] * v + hyperparams[<span class="string">'lr'</span>] * p.grad</span><br><span class="line">        p[:] -= v</span><br></pre></td></tr></table></figure>

<p>就是根据数学分析实现，easy。应用注意：</p>
<ul>
<li>先将超参数momentum设为0.5，这时可以看成是特殊的小批量随机梯度：其小批量随机梯度为最近2个时间步的2倍小批量梯度的加权平均。<br><img src="/2020/01/22/dive-into-deep-learning/mom1.png" alt="动量法"></li>
<li>将超参数momentum增大到0.9，这时依然可以看成是特殊的小批量随机梯度下降，其小批量随机梯度为最近10个时间步的10倍小批量梯度的加权平均。先保持学习率不变<br><img src="/2020/01/22/dive-into-deep-learning/mom2.png" alt="动量法"></li>
<li>可以看到目标函数后期迭代过程不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度打了5倍，我们可以尝试将学习旅减小到原来的1/5，此时目标函数值在下降了一段时间后变化更加平滑。<br><img src="/2020/01/22/dive-into-deep-learning/mom3.png" alt="动量法"><h4 id="7-4-4-简洁实现"><a href="#7-4-4-简洁实现" class="headerlink" title="7.4.4 简洁实现"></a>7.4.4 简洁实现</h4>简洁实现又一个momentum的参数，直接赋值就可以了，so简洁！<h3 id="7-5-AdaGrad算法"><a href="#7-5-AdaGrad算法" class="headerlink" title="7.5 AdaGrad算法"></a>7.5 AdaGrad算法</h3>之前介绍的优化算法，目标函数自变量在每一个元素相同步间都是用同一个学习率来自我迭代。<br>$$<br>x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad<br>x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.<br>$$<br>但是前面看到，当x1和x2的梯度值有较大差别时，需要选择足够小的学习率在梯度值较大的维度上不发散。但这样会导致自变量在梯度较小的维度上迭代过慢，动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。<br>本节介绍AdaGrad算法，它根据自变量在每个维度的梯度值的大小李艾调整各个维度上的学习率，从而避免统一的学习率难以适应所有的维度。<h4 id="7-5-1-算法"><a href="#7-5-1-算法" class="headerlink" title="7.5.1 算法"></a>7.5.1 算法</h4>在时间步0，AdaGrad将s0中每个元素初始化为0。在时间步t，首先将小批量随机梯度gt按元素平方后累加到变量st<br>$$\boldsymbol{s}<em>t \leftarrow \boldsymbol{s}</em>{t-1} + \boldsymbol{g}<em>t \odot \boldsymbol{g}_t,$$<br>其中$\odot$是按元素相乘，接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整。<br>$$\boldsymbol{x}_t \leftarrow \boldsymbol{x}</em>{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,$$</li>
</ul>
<p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<h4 id="7-5-2-特点"><a href="#7-5-2-特点" class="headerlink" title="7.5.2 特点"></a>7.5.2 特点</h4><ol>
<li>小批量随机梯度按元素平方的累加变量st出现在学习率的分母项中。 -&gt;  如果目标函数有关自变量中某个元素的偏导数一直较小，那么该元素的学习率下降较慢，</li>
<li>由于st一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变） -&gt;   当学习率在迭代早期下降较快且当前解依然不佳时，AdaGrad算法在得带后期由于学习率较小，很难找到有用解。<h4 id="7-5-3-从零开始实现"><a href="#7-5-3-从零开始实现" class="headerlink" title="7.5.3 从零开始实现"></a>7.5.3 从零开始实现</h4>easy，AdaGrad算法需要对每个自变量维护它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。<br>训练用更大的学习率来训练模型。<h4 id="7-5-4-简洁实现"><a href="#7-5-4-简洁实现" class="headerlink" title="7.5.4 简洁实现"></a>7.5.4 简洁实现</h4>Trainer实例中有’adagrad’关键字<h3 id="7-6-RMSProp算法"><a href="#7-6-RMSProp算法" class="headerlink" title="7.6 RMSProp算法"></a>7.6 RMSProp算法</h3>之前提到，AdaGrad算法，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这个问题RMSProp算法对AdaGrad算法做了一点小小的修改。<h4 id="7-6-1-算法"><a href="#7-6-1-算法" class="headerlink" title="7.6.1 算法"></a>7.6.1 算法</h4>之前说过指数加权平均。不同于AdaGrad算法里将变量st是截至时间步t所有小批量随机梯度gt按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数0&lt;=y&lt;1，RMSProp在时间步&gt;0计算：<br>$$\boldsymbol{s}<em>t \leftarrow \gamma \boldsymbol{s}</em>{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t. $$<br>x的迭代一样。<br>因为RMSProp算法的状态变量是对平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的指数加权移动平均，所以可以看作最近1/(1-y)个时间步的小批量随机梯度平方项的加权平均。如此以来，自变量每个元素的学习率在迭代过程中就不一直降低（或不变）。（看不懂）<h4 id="7-6-2-从零开始实现"><a href="#7-6-2-从零开始实现" class="headerlink" title="7.6.2 从零开始实现"></a>7.6.2 从零开始实现</h4>easy<h4 id="7-6-3-简洁实现"><a href="#7-6-3-简洁实现" class="headerlink" title="7,6,3 简洁实现"></a>7,6,3 简洁实现</h4>参数’rmsprop’，超参数多了gammal: 0.9。<h3 id="7-7-AdaDelta算法"><a href="#7-7-AdaDelta算法" class="headerlink" title="7.7 AdaDelta算法"></a>7.7 AdaDelta算法</h3>除了RMSProp算法之外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进。特别的是，AdaDelta算法没有学习率这一超参数。<h4 id="7-7-1-算法"><a href="#7-7-1-算法" class="headerlink" title="7.7.1 算法"></a>7.7.1 算法</h4>第一步和RMSProp算法一样，使用了小批量随机梯度gt按元素平方的指数加权移动平均变量st。在时间步0.，它的所有元素被初始化为0。一毛一样，不同的数，AdaDelta算法还维护一个额外的状态变量$\Delta\boldsymbol{x}<em>t$。<br>可以看到如果不考虑稳定常数，AdaDelta算法与RMSProp算法的不同之处在于使用了$\sqrt{\Delta\boldsymbol{x}</em>{t-1}}$来替代超参数n,<em>没有学习率！！！这个参数</em>。<h4 id="7-7-2-从零开始实现"><a href="#7-7-2-从零开始实现" class="headerlink" title="7.7.2 从零开始实现"></a>7.7.2 从零开始实现</h4>AdaDelta算法需要对每个自变量维护两个状态变量，即st和$\Delta\boldsymbol{x}_t$。按公式实现算法。<h4 id="7-7-3-简洁实现"><a href="#7-7-3-简洁实现" class="headerlink" title="7.7.3 简洁实现"></a>7.7.3 简洁实现</h4>名称为’adadelta’的Trainer实例，超参数由’rho’指定。<h3 id="7-8-Adam算法"><a href="#7-8-Adam算法" class="headerlink" title="7.8 Adam算法"></a>7.8 Adam算法</h3><h4 id="7-8-1-算法"><a href="#7-8-1-算法" class="headerlink" title="7.8.1 算法"></a>7.8.1 算法</h4>（应该最先进的吧）<br>Adam算法使用了动量变量vt和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量st（两个都做指数加权移动平均）。<br>由于我们将v0和s0中的元素中的元素都初始化为零，但是需要注意的是，当t较小时，过去各时间步小批量随机梯度权值只和会较小。利用偏差修正。<br>接下来Adam算法使用偏差修正之后的变量将模型参数中每个元素的学习率通过按元素重新调整。<br>最后使用gt‘迭代自变量。<h4 id="7-8-2-从零开始实现"><a href="#7-8-2-从零开始实现" class="headerlink" title="7.8.2 从零开始实现"></a>7.8.2 从零开始实现</h4></li>
</ol>
<h2 id="第8章-计算性能"><a href="#第8章-计算性能" class="headerlink" title="第8章 计算性能"></a>第8章 计算性能</h2><h3 id="8-1-命令式和符号式混合编程"><a href="#8-1-命令式和符号式混合编程" class="headerlink" title="8.1 命令式和符号式混合编程"></a>8.1 命令式和符号式混合编程</h3><p>命令式编程很方便，但它的运行可能很慢。<br>与命令式编程不同，符号式编程通常在计算机流程完全定义好后才被执行。多个深度学习框架，如Theano和TensorFlow，都使用了符号式编程。通常，符号式编程的程序需要下面3个步骤。</p>
<ol>
<li>定义计算流程</li>
<li>把计算流程编译成可执行的程序</li>
<li>给定输入，调用编译好的程序执行。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">def add(a, b):</span></span><br><span class="line"><span class="string">    return a + b</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fancy_func_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'''</span></span><br><span class="line"><span class="string">def fancy_func(a, b, c, d):</span></span><br><span class="line"><span class="string">    e = add(a, b)</span></span><br><span class="line"><span class="string">    f = add(c, d)</span></span><br><span class="line"><span class="string">    g = add(e, f)</span></span><br><span class="line"><span class="string">    return g</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evoke_str</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> add_str() + fancy_func_str() + <span class="string">'''</span></span><br><span class="line"><span class="string">print(fancy_func(1, 2, 3, 4))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">prog = evoke_str()</span><br><span class="line">print(prog)</span><br><span class="line">y = compile(prog, <span class="string">''</span>, <span class="string">'exec'</span>)</span><br><span class="line">exec(y)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>以上定义的3个函数都仅以<strong>字符串</strong>的形式返回计算流程。最后，我们通过compile函数编译完整地获取整个程序，因此有更多空间优化计算，例如，编译的时候可以将程序改写成print((1+2)+(3+4))甚至改成print(10)，这样不仅减少了函数调用，还节省了内存。<br>对比两种编程方式，我们看到：</p>
<ol>
<li>命令式编程更方便。当我们在Python里使用命令式编程时，大部分代码很直观。同时，命令式编程更容易调试。这是因为我们可以很方便地获取并打印所有的中间变量值，或者使用Python的调试工具。</li>
<li>符号式编程更高效并更容易移植。一方面，在编译的时候系统很容易做更多优化；另一方面，符号式编程可以将程序编程一个与Python无关的格式，从而可以使程序在非Python环境下运行，以避开Python解释器的性能问题。<h4 id="8-1-1-混合式编程取两者之长"><a href="#8-1-1-混合式编程取两者之长" class="headerlink" title="8.1.1 混合式编程取两者之长"></a>8.1.1 混合式编程取两者之长</h4>Theano和受其启发的后来者TensorFlow使用了符号式编程，Chainer和它的追随者PyTorch使用了命令式编程。<br>Gluon考虑使用混合编程，同时得到两者好处。用用应该使用纯命令式进行开发和调试；当需要产品级别的计算性能和部署时，用户可以将大部分命令时程序转换成符号式程序来运行。Gluon通过提供混合式编程方式做到这一点。<br>在混合式编程中，我们可以通过使用HybridBlock类或者HybridSequential类构建模型。默认他们和Block类和Sequential类一样照命令式编程的方式执行，但是当调用hybridize函数后，Gluon会转换成依据符号式编程的方式执行。<h4 id="8-1-2-使用HybridSequential类构造模型"><a href="#8-1-2-使用HybridSequential类构造模型" class="headerlink" title="8.1.2 使用HybridSequential类构造模型"></a>8.1.2 使用HybridSequential类构造模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd, sym</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span><span class="params">()</span>:</span></span><br><span class="line">    net = nn.HybridSequential()  <span class="comment"># 这里创建HybridSequential实例</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            nn.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            nn.Dense(<span class="number">2</span>))</span><br><span class="line">    net.initialize()</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">x = nd.random.normal(shape=(<span class="number">1</span>, <span class="number">512</span>))</span><br><span class="line">net = get_net()</span><br><span class="line">net(x)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>就是替代了以前的Sequential实例，<br>我们可以通过调用hybridize函数来编译和优化HbridSequential实例中串联的层的计算。<br><strong>注意只有继承HybridBlock类的层才会被优化计算。例如HybridSequential类和Gluon提供的Dense类都是HybrudBlock类的子类，他们会被优化计算。如果一个层只是继承自Block类而不是HybridBlock类，那么它将不会被优化</strong></p>
<ol>
<li>计算性能<br>调用net.hybridize()之后时间减小</li>
<li>获取符号式程序<br><code>net.export(&#39;my_mlp&#39;)</code>会生成.json和.params文件，分别为符号式程序和模型参数。可以被Python或mxnet支持的其他前端语言获取，如C++,R,Scala,Perl和其他语言。这样就可以很方便地使用其他前端语言或者其他设备上部署训练好的模型。同时，由于部署时使用的是符号式编程，计算性能往往比命令式程序的性能更好。</li>
</ol>
<p><strong>在MXNet类中，符号式程序指的式基于Symbol类型的程序。</strong><br>我们知道，当给net提供NDArray类型的输入x后，net(x)会根据x直接计算模型输出并返回结果。对于调用过hybridize函数后的模型，我们还可以给它输入一个Symbol类型的变量，net(x)会返回Symbol类型的结果</p>
<h4 id="8-1-3-使用HybridBlock类构造模型"><a href="#8-1-3-使用HybridBlock类构造模型" class="headerlink" title="8.1.3 使用HybridBlock类构造模型"></a>8.1.3 使用HybridBlock类构造模型</h4><p>和Sequential类与Block类之间的关系一样，HybridSequential类继承自HybridBlock类。与Block实现forward函数不太一样，对于HybridBlock实例，我们需要实现hybrid_forward函数。<br>调用hybridize函数后的模型计算性能和可移植性。但是调用hybridize函数后的模型会影响灵活性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HybridNet</span><span class="params">(nn.HybridBlock)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(HybridNet, self).__init__(**kwargs)</span><br><span class="line">        self.hidden = nn.Dense(<span class="number">10</span>)</span><br><span class="line">        self.output = nn.Dense(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span><span class="params">(self, F, x)</span>:</span></span><br><span class="line">        print(<span class="string">'F: '</span>, F)</span><br><span class="line">        print(<span class="string">'x: '</span>, x)</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        print(<span class="string">'hidden: '</span>, x)</span><br><span class="line">        <span class="keyword">return</span> self.output(x)</span><br></pre></td></tr></table></figure>

<p>在hybrid_forward函数中这里有个F，我们知道，MXNet既有基于命令式编程的NDArray类，又有基于符号式编程的symbol类。由于这个两个类的函数基本一致，MXNet会根据输入来决定F使用NDArray还是Symbol。<br>之后做实验发现，对于NDArray执行一次和进行一次前向计算结果不变，但是调用net.hybridize()后，标志编程symbol并且，进行一次前向计算无打印结果，原因是：上一次调用hybridize函数后运行net(x)的时候，符号式程序已经得到。之后再运行net(x)的时候MXNet将不再访问Python代码，而是直接在C++后端执行符号式程序。这也是调用了hybridize函数后模型计算性能会提升的一个原因。<br>但会受损失灵活性：</p>
<ul>
<li>像这个例子，如果我们希望使用3条打印语句调试代码，执行符号式编程无法打印。</li>
<li>对于asnumpy这样Symbol所不支持的函数，以及像a +=b和a[:] = a + b（需改写为a= a + b）这样的原地（in-place）操作，我们无法在hybrid_forward函数中使用并在调用hybridize函数后进行前向计算。<h3 id="8-2-异步计算"><a href="#8-2-异步计算" class="headerlink" title="8.2 异步计算"></a>8.2 异步计算</h3>MXNet使用异步计算来提升计算性能。理解它的工作原理既有助于开发更高效的程序，又有助于内存资源有限的情况下主动降低计算性能从而减小内存开销。<h4 id="8-2-1-MXNet中的异步计算"><a href="#8-2-1-MXNet中的异步计算" class="headerlink" title="8.2.1 MXNet中的异步计算"></a>8.2.1 MXNet中的异步计算</h4>广义上来讲，MXNet包括：</li>
<li>用户直接用来交互的前端</li>
<li>系统用来执行计算的后端<br>用户写好的前端MXNet程序会传给后端执行计算。后端有自己的线程在队列中不断收集任务并执行它们。<br>MXNet通过前端线程和后端线程的交互实现异步计算。异步计算是指，前端线程无须等待当前指令从后端线程返回结果就继续执行后面的命令。<br>很多简单例子<br><img src="/2020/01/22/dive-into-deep-learning/ac.png" alt="ac"><br>除非我们需要打印或保存计算结果，否则我们无须关心目前结果在内存中是否已经计算好了。只要数据是保存在NDArray里并使用MXNet提供的运算符，MXNet将默认使用异步计算来获取高性能。<h4 id="8-2-2-用同步函数让前端等待计算结果"><a href="#8-2-2-用同步函数让前端等待计算结果" class="headerlink" title="8.2.2 用同步函数让前端等待计算结果"></a>8.2.2 用同步函数让前端等待计算结果</h4>有许多方法让前端线程等待后端计算结果完成：</li>
<li>print函数</li>
<li>使用wait_to_read函数让前端等待某个的NDArray的计算结果完成，再执行后面的语句</li>
<li>用waitall函数零前端前面所有计算结果完成，这是性能测试中常用的方法<br>一看就会<br><img src="/2020/01/22/dive-into-deep-learning/wait.png" alt="wait"><br>此外，任何将NDArray转换成其他不支持异步计算的数据结构的操作都会让前端等待计算结果。例如，当我们调用asnumpy函数和asscalar函数时。</li>
</ul>
<p><strong>上面介绍的wait_to_read函数、waitall函数、asnumpy函数、asscalar函数和print函数会触发让前端等待后端计算结果的行为。这类函数通常称为同步函数。</strong></p>
<h4 id="8-2-3-使用异步计算提升计算性能"><a href="#8-2-3-使用异步计算提升计算性能" class="headerlink" title="8.2.3 使用异步计算提升计算性能"></a>8.2.3 使用异步计算提升计算性能</h4><p>在下面例子中，我们用for循环不断对变量y赋值。当在for循环内使用同步函数wait_to_read时，每次赋值不使用异步计算；当在for循环外使用同步函数waitall时，则使用异步计算。<br>使用异步计算能提升一定的计算性能。<br>我们将这3个阶段的耗时分别设为 t1,t2,t3 。如果不使用异步计算，执行1000次计算的总耗时大约为 1000(t1+t2+t3) ；如果使用异步计算，由于每次循环中前端都无须等待后端返回计算结果，执行1000次计算的总耗时可以降为 t1+1000t2+t3 （假设 1000t2&gt;999t1，可能我的mac不满足 ）。</p>
<h4 id="8-2-4-异步计算对内存的影响"><a href="#8-2-4-异步计算对内存的影响" class="headerlink" title="8.2.4 异步计算对内存的影响"></a>8.2.4 异步计算对内存的影响</h4><p>66666:<br>之前我们训练模型是在每个小批量上训练之后，评测以下模型，如模型的损失或准确率，通常会使用同步函数，如asscalar函数或者asnumpy函数。如果去掉这些同步函数，前端会将大量的小批量计算任务在极短的时间丢给后端，从而可能导致占用更多内存。当我们在每个小批量上使用同步函数时，前端每次在迭代时仅会讲一个小批量的任务丢给后端执行计算，并通常会减小内存占用。<br>内存资源有限，建议大家在训练模型时，对每个小批量使用同步函数。预测也在每个小批量用同步函数。<br>例子中发现，使用同步函数，时间会变长，但内存占用变少。</p>
<h3 id="8-3-自动并行计算"><a href="#8-3-自动并行计算" class="headerlink" title="8.3 自动并行计算"></a>8.3 自动并行计算</h3><h4 id="8-3-1-CPU和GPU的并行计算"><a href="#8-3-1-CPU和GPU的并行计算" class="headerlink" title="8.3.1 CPU和GPU的并行计算"></a>8.3.1 CPU和GPU的并行计算</h4><h4 id="8-3-2-计算和通信的并行计算"><a href="#8-3-2-计算和通信的并行计算" class="headerlink" title="8.3.2 计算和通信的并行计算"></a>8.3.2 计算和通信的并行计算</h4><h3 id="8-4-多GPU计算"><a href="#8-4-多GPU计算" class="headerlink" title="8.4 多GPU计算"></a>8.4 多GPU计算</h3><h4 id="8-4-1-数据并行"><a href="#8-4-1-数据并行" class="headerlink" title="8.4.1 数据并行"></a>8.4.1 数据并行</h4><h4 id="8-4-2-定义模型"><a href="#8-4-2-定义模型" class="headerlink" title="8.4.2 定义模型"></a>8.4.2 定义模型</h4><h4 id="8-4-3-多GPU之间同步数据"><a href="#8-4-3-多GPU之间同步数据" class="headerlink" title="8.4.3 多GPU之间同步数据"></a>8.4.3 多GPU之间同步数据</h4><h4 id="8-4-4-单个小批量上的多GPU训练"><a href="#8-4-4-单个小批量上的多GPU训练" class="headerlink" title="8.4.4 单个小批量上的多GPU训练"></a>8.4.4 单个小批量上的多GPU训练</h4><h4 id="8-4-5-定义训练函数"><a href="#8-4-5-定义训练函数" class="headerlink" title="8.4.5 定义训练函数"></a>8.4.5 定义训练函数</h4><h4 id="8-4-6-多GPU训练实验"><a href="#8-4-6-多GPU训练实验" class="headerlink" title="8.4.6 多GPU训练实验"></a>8.4.6 多GPU训练实验</h4><h3 id="8-5-多GPU计算的简洁实现"><a href="#8-5-多GPU计算的简洁实现" class="headerlink" title="8.5 多GPU计算的简洁实现"></a>8.5 多GPU计算的简洁实现</h3><h4 id="8-5-1-多GPU上初始化模型参数"><a href="#8-5-1-多GPU上初始化模型参数" class="headerlink" title="8.5.1 多GPU上初始化模型参数"></a>8.5.1 多GPU上初始化模型参数</h4><h4 id="8-5-2-多GPU训练模型"><a href="#8-5-2-多GPU训练模型" class="headerlink" title="8.5.2 多GPU训练模型"></a>8.5.2 多GPU训练模型</h4><h2 id="第9章-计算机视觉"><a href="#第9章-计算机视觉" class="headerlink" title="第9章 计算机视觉"></a>第9章 计算机视觉</h2><p><img src="/2020/01/22/dive-into-deep-learning/cv.png" alt="wait"><br>前面已经实践了简单图像分类任务。</p>
<ol>
<li>先介绍两种有助于提升模型的泛化能力的方法。即图像增广和微调，并将它们用于图像分类。（由于深度神经网络能够对图像逐级有效地进行表征，这一特性被广泛应用在目标检测，语义分割和样式迁移这些主流计算机视觉任务中，并取得了成功）</li>
<li>围绕这一思想，首先，我们将描述目标检测的工作流程与各类方法。之后，我们将探究如何使用全卷积网络对图像做语义分割。</li>
<li>接下来，我们再解释如何使用样式迁移技术生成像书本明面一样的图像</li>
<li>最后，我们在两个计算机视觉的重要数据集上实践本章和前几章的知识。</li>
</ol>
<h3 id="9-1-图像增广"><a href="#9-1-图像增广" class="headerlink" title="9.1 图像增广"></a>9.1 图像增广</h3><p>数据越多,深度神经网络越成功。</p>
<ul>
<li>图像增广技术通过对训练图像做一些列随机改变，来产生相似但不同的样本，扩大了数据集的规模。</li>
<li>另外，图像增广，随机改变样本可降低模型对某些元素的以来，从而提高模型的泛化能力。（例如，可以对图像进行不同方式的裁剪，使感兴趣的物体在不同位置，减小对位置的依赖性，也可以调整亮度、色彩等因素来降低对色彩的敏感度。）<h4 id="9-1-1-常用的图像增广方法"><a href="#9-1-1-常用的图像增广方法" class="headerlink" title="9.1.1 常用的图像增广方法"></a>9.1.1 常用的图像增广方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span><span class="params">(imgs, num_rows, num_cols, scale=<span class="number">2</span>)</span>:</span> <span class="comment"># 定义绘图函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(img, aug, num_rows=<span class="number">2</span>, num_cols=<span class="number">4</span>, scale=<span class="number">1.5</span>)</span>:</span>  <span class="comment"># 大部分图像增广的方法具有一定的随机性。为了方便观察图像增广的效果，用一个辅助函数apply。对输入图像img多次运行图像增广方法aug并展示所有的结果。</span></span><br><span class="line">    Y = [aug(img) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_rows * num_cols)]</span><br><span class="line">    show_images(Y, num_rows, num_cols, scale)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><p>翻转和裁剪</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机翻转左右 一半的概率</span></span><br><span class="line">apply(img, gdata.vision.transforms.RandomFlipLeftRight())   </span><br><span class="line"><span class="comment">#随机翻转上下</span></span><br><span class="line">apply(img, gdata.vision.transforms.RandomFlipTopBottom())    </span><br><span class="line"></span><br><span class="line"><span class="comment">#随机裁剪每次随机裁剪出一块面积为原面积 10%∼100% 的区域，且该区域的宽和高之比随机取自 0.5∼2 ，然后再将该区域的宽和高分别缩放到200像素</span></span><br><span class="line">shape_aug = gdata.vision.transforms.RandomResizedCrop(</span><br><span class="line">    (<span class="number">200</span>, <span class="number">200</span>), scale=(<span class="number">0.1</span>, <span class="number">1</span>), ratio=(<span class="number">0.5</span>, <span class="number">2</span>))    </span><br><span class="line">apply(img, shape_aug)</span><br></pre></td></tr></table></figure>
</li>
<li><p>变化颜色</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将图像的亮度随机变化为原图亮度的 50% （即 1−0.5 ） ∼150% （即 1+0.5 ）。</span></span><br><span class="line">apply(img, gdata.vision.transforms.RandomBrightness(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机变化图像的色调。</span></span><br><span class="line">apply(img, gdata.vision.transforms.RandomHue(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建RandomColorJitter实例并同时设置如何随机变化图像的亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。</span></span><br><span class="line">color_aug = gdata.vision.transforms.RandomColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation=<span class="number">0.5</span>, hue=<span class="number">0.5</span>)</span><br><span class="line">apply(img, color_aug)</span><br></pre></td></tr></table></figure>
</li>
<li><p>叠加多个图像增广方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将多个图像增广方法叠加使用。我们可以通过Compose实例将上面定义的多个图像增广方法叠加起来，再应用到每张图像之上。</span></span><br><span class="line">augs = gdata.vision.transforms.Compose([</span><br><span class="line">    gdata.vision.transforms.RandomFlipLeftRight(), color_aug, shape_aug])</span><br><span class="line">apply(img, augs)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="9-1-2-使用图像增广训练模型"><a href="#9-1-2-使用图像增广训练模型" class="headerlink" title="9.1.2 使用图像增广训练模型"></a>9.1.2 使用图像增广训练模型</h4><p>妈的训练集下载半天。<br>我们使用CIFAR-10数据集，而不是以前的Fashion-MNIST。因为Fashion-MNIST数据集中物体的位置和尺寸已经经过归一化处理，而CIFAR-10数据集汇总物体的颜色和大小区别更加显著。<br>为了在预测得到确定的结果，我们只在训练的时候应用含图像操作的图像增广，简单的随机左右翻转。此外使用Totensor实例将小批量图像转成MXNet需要的格式，即形状为<br>（批量大小，通道数，高，宽）、值域在0到1之间且类型为32为浮点数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flip_aug = gdata.vision.transforms.Compose([</span><br><span class="line">    gdata.vision.transforms.RandomFlipLeftRight(), <span class="comment">#随机翻转 训练使用</span></span><br><span class="line">    gdata.vision.transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">no_aug = gdata.vision.transforms.Compose([ <span class="comment">#预测使用</span></span><br><span class="line">    gdata.vision.transforms.ToTensor()])</span><br></pre></td></tr></table></figure>

<p>还有evlauate_accuracy函数评价模型的分类准确率，更加通用。</p>
<h3 id="9-2-微调"><a href="#9-2-微调" class="headerlink" title="9.2 微调"></a>9.2 微调</h3><p>好像是<strong><em>迁移学习</em></strong>，我去。<br>前面的Fashion-MNIST训练数据集上训练模型只有6万张图像，最广泛的大规模图像数据集ImageNet有超过1000万的图像和1000类的物体，平常我们用到的数据集在两者之间。<br>如果我们训练不同的椅子，自己拍照片可能数据集不够，最后精确度不够，同时适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。<br>为了应对问题，降低成本。方法是应用迁移学习，将从源数据集学到的只是迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多与椅子无关，但在该数据集上训练的模型可以抽象较通用的图像特征。从而能帮助识别边缘、纹理、形状和物体组成等等。这些特征对识别椅子同样有效。<br>应用技术——微调，微调由4步组成：</p>
<ol>
<li>在源数据集上预绚练一个神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型，复制了源模型除了输出层以外的所有模型设计及其参数。</li>
<li>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，其余层的参数都是源模型的参数微调得到。</li>
</ol>
<p>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</p>
<h4 id="热狗识别"><a href="#热狗识别" class="headerlink" title="热狗识别"></a>热狗识别</h4><ol>
<li><p>获取数据集<br>下载好了，贼大<br>在训练时，我们先从图像中裁剪处随机大小和随机高宽比的随机区域，然后将该区域作为高和宽均为224的输入。测试时，我们将图像的高和宽均缩放为256，然后从中裁剪处高宽均为224像素的中心区域作为输入，此外对RGB是三个颜色通道的数值做标准化：每个数值减去该通道所有数值的平均值，再除以该通道数值的标准差作为输出（数据预处理）。</p>
</li>
<li><p>定义和初始化模型<br>使用ImageNet数据集上预训练的ResNet-18作为源模型，指定<code>pretrained=True</code>来自动下载并家在预训练的模型参数，要联网下载。<br>预训练的源模型实例包含两个成员变量，即features和output，我去太酷了！！。前者包含模型除输出层以外的所有层，后者为模型的输出层。这样划分主要为了方便微调出输出层以外的所有层的模型参数。对输出层，是一个全连接层，它将ResNet最终的全局平均池化层输出变换成ImageNet数据集上1000类的输出<br>我们新建一个神经网络作为目标模型，它的定义与与训练的源模型，一样但最后class=2.在下面代码中，目标模型实例参数被初始化为源模型相应层的模型参数。由于features中的模型参数是ImageNet数据集上预训练得到的，已经足够好，因此一般只使用较小的学习率，output一般用较大的学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">finetune_net = model_zoo.vision.resnet18_v2(classes=<span class="number">2</span>)</span><br><span class="line">finetune_net.features = pretrained_net.features</span><br><span class="line">finetune_net.output.initialize(init.Xavier())</span><br><span class="line"><span class="comment"># output中的模型参数将在迭代中使用10倍大的学习率</span></span><br><span class="line">finetune_net.output.collect_params().setattr(<span class="string">'lr_mult'</span>, <span class="number">10</span>)  <span class="comment">#learning rate * 10</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>微调模型</p>
</li>
</ol>
<h3 id="9-3-目标检测和边界框"><a href="#9-3-目标检测和边界框" class="headerlink" title="9.3 目标检测和边界框"></a>9.3 目标检测和边界框</h3><p>前面章节，我们可以训练一个图片，来完成分类任务，但有时候我们不仅想知道类别，还想知道它在图像中的具体位置，这类任务是目标检测或物体检测。<br><strong>目标检测</strong><br>目标位置概念<br><strong>边界框</strong><br>在目标检测里，我们通常使用边界框来描述目标位置。边界框是一个矩形框，可以用矩形的左上角的x和y和矩形的右下角的x和y坐标确定。图中的坐标原点在图像的左上角，原点往右和往下分别为x轴的正方向和y轴的正方向。</p>
<h3 id="9-4-锚框"><a href="#9-4-锚框" class="headerlink" title="9.4 锚框"></a>9.4 锚框</h3><p>目标检测概念了解了，对于具体实现。<br>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含感兴趣感目标，并调整区域边缘从而更准确地预测目标函数的真实边界框。不同的模型使用的区域采样方法不同，这里介绍一种：<br>以每个像素为中心生成多个大小和宽高比不同的边界框。这些边界框被称为锚框。</p>
<h4 id="9-4-1-生成多个锚框"><a href="#9-4-1-生成多个锚框" class="headerlink" title="9.4.1 生成多个锚框"></a>9.4.1 生成多个锚框</h4><p>假设输入图像高为h，宽为w。我们分别以图像的每个像素为中心生成不同形状的anchor。参数为s和r，加上输入图像宽w高h。</p>
<ul>
<li>大小为s(0&lt;s&lt;=1)</li>
<li>宽高比r(r&gt;0)</li>
</ul>
<p>所得锚框的宽和高分别为ws根号r和hs/根号r。<br>然后我们分别定义好一组大小s1,…,sn和一组宽高比r1,…,rm。如果以每个像素为中心使用所有的大小与宽高比的组合，一共有whnm个锚框，虽然可能覆盖真实边界框，但是计算复杂度太高。因此，通常我们只对包含s1或r1大小与宽高比的组合感兴趣。也就是说，以相同像素为中心的锚框的数量为n+m-1。对于整个输入图像，我们将一共生成wh(n+m-1)个锚框。<br><strong>具体实现：</strong></p>
<ol>
<li>生成锚框变量的方法已实现在MultiBoxPrior函数中。指定输入、一组大小和一组宽高比，该函数将返回输入的所有锚框。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img = image.imread(<span class="string">'../img/catdog.jpg'</span>).asnumpy()</span><br><span class="line">h, w = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">print(h, w)</span><br><span class="line">X = nd.random.uniform(shape=(<span class="number">1</span>, <span class="number">3</span>, h, w))  <span class="comment"># 构造输入数据</span></span><br><span class="line">Y = contrib.nd.MultiBoxPrior(X, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">Y.shape</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>返回形状为（批量大小，锚框个数，4）</p>
<ol start="2">
<li>利用reshape将锚框变量y的想哦门户之见爱漫画弄啊美味哦（图像高，图像宽，以相同像素为中心的锚框个数，4）后，就可以通过指定像素位置来获取所有该项像素为中心的锚框了。<br>变量boxes中x和y轴的坐标值分别已除以图像的宽和高。在绘图时，我们需要回复锚框的原始坐标值（bbox_scale）。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">boxes = Y.reshape((h, w, <span class="number">5</span>, <span class="number">4</span>))</span><br><span class="line">boxes[<span class="number">250</span>, <span class="number">250</span>, <span class="number">0</span>, :]</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong><em>就是不晓得规则是什么 为什么？ 为啥定了5 就是这五个 不是有各3各参数吗？ 应该9个？</em></strong></p>
<h4 id="9-4-2-交并比"><a href="#9-4-2-交并比" class="headerlink" title="9.4.2 交并比"></a>9.4.2 交并比</h4><p>为了优化必须量化“较好”，一种直观的方法是衡量锚框和真实边界之间的相似度。jaccard系数可以衡量两个集合的相似度。给定集合A和B，他们的jaccard系数即二者交集大小除以二者并集大小。<br>可以用边界框替代整个像素面，本节的剩余部分，我们将使用交并比来衡量锚框与真实边界框以及锚框与锚框之间的相似度。</p>
<h4 id="9-4-3-标注训练集的锚框"><a href="#9-4-3-标注训练集的锚框" class="headerlink" title="9.4.3 标注训练集的锚框"></a>9.4.3 标注训练集的锚框</h4><p>labelImage woc真复杂 还好聪明的我看懂了<br>为了训练目标检测模型，我们需要为每个边框标注两类标签：</p>
<ul>
<li>锚框所含目标的类别，简称类别</li>
<li>真实边界框相对锚框的偏移量，简称偏移量<br>目标检测时，先生成多个锚框，然后为多个锚框预测类别和偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。<br>如何为锚框分配与其相似的真实边界框呢？（狗的框怎么分配就是狗的框而不是猫的框）<br>可以根据锚框索引和真实边界框索引巴拉巴拉一大堆，看这个图结合书能理解：<br><img src="/2020/01/22/dive-into-deep-learning/anchor.png" alt="anchor"><br>分配好真实边框就可以标注锚框的类别和偏移量。</li>
<li>偏移量：<br>$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},<br>\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},<br>\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},<br>\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),$$<br>贼复杂，其中常数的默认值为$\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1, \sigma_w=\sigma_h=0.2$。</li>
<li>类别：如果一个锚框没有被分配真实边界框，我们只需要将该锚框的类别设置为背景。<ul>
<li>类别为背景的锚框通常被称为负类锚框</li>
<li>其余被称为正类锚框</li>
</ul>
</li>
</ul>
<p><img src="/2020/01/22/dive-into-deep-learning/dog.png" alt="dog"><br>我们可以通过contrib.nb模块中的MultiBoxTarget函数来为锚框标注类别和偏移量。<br><img src="/2020/01/22/dive-into-deep-learning/multi.png" alt="dog"><br>我们根据锚框与真实边界框在图像中的位置来分析这些标注的类型（方法多的要死，自己看书能懂）。<br>返回值的三项含义：</p>
<ul>
<li>返回的第一项是每个锚框标注的4个偏移量，其中负类锚框的偏移量标注为0</li>
<li>返回的第二项为掩码变量，形状为（批量大小，锚框个数的4倍）。掩码变量中的元素与每个锚框的4个偏移量一一对应，由于我们不关心对背景的检测，有关负类的偏移量不应影响目标函数。<br>掩码变量的作用：<br>通过元素惩罚，掩码变量中的0可以计算目标函数之前过滤掉负类的偏移量</li>
<li>返回的结果有3项，均为NDArray。第三项表示为锚框标注的实例<h4 id="9-4-4-输出预测边界框"><a href="#9-4-4-输出预测边界框" class="headerlink" title="9.4.4 输出预测边界框"></a>9.4.4 输出预测边界框</h4>在模型预测阶段，生成多个锚框，但锚框数目较多时，同一个目标上可能输出较多相似的预测边框之，为了让结果更加简洁，可以移除相似的预测边界框，常用的方法叫做非极大值抑制。</li>
</ul>
<p><strong>非极大抑制（NMS） 工作原理：</strong><br>最大的预测概率p：该预测概率所对应的类别即B的预测类别。我们将p称为预测边界框的置信度。在同一图像上，我们将预测类别非背景的预测边界框按置信度从高到底排序，得到列表L。<br>然后先选最高的置信度的然后求别的和他的交并比，交并比大于一个超参数阈值就删掉，然后第二，第三，so easy 说的吓人。<br>可以使用contrib.nd模块的MultiBoxDetection函数来执行非极大值一直并设阈值为0.5。为NDArray输入都增加了样本维，第一个值是类别（-1表示背景后者NMS中被删除），第二表示置信度，后面四个表示样本边框左上角右下角坐标。<br>实践中，我们可以先把置信度较低的删除减小计算量。还可以对NMDS的输出筛选，例如只保留其中置信度较高的结果作为最终输出。</p>
<h3 id="9-5-多尺度目标检测"><a href="#9-5-多尺度目标检测" class="headerlink" title="9.5 多尺度目标检测"></a>9.5 多尺度目标检测</h3><p>在9.4节中，我们在实验中以输入图像的每个像素的中心生成多个锚框。这些锚框是对输入图像不同区域的采样。然而，如果以图像每个像素为中心，很容易计算量过大。<br>减少锚框个数并不难，一种简单的方法是在输入图像中均匀采样一小部分采样，并以采样的像素为中心生成锚框。<br>不同尺度下，可以生成不同数量和不同大小的锚框。锚框目标较小则出现位置增多。<br>5.1节提到卷积神经网络的特征图：</p>
<ul>
<li>特征图：就是二维卷积层输出的二维数组（可以看作输入在空间维度（宽和高）上某一级的表征）<br>可以通过定义特征图的形状来确定任一图像上均匀采样的锚框中心。（你的特征图大就均匀采样挤的采样位置少）</li>
<li>我们在特征图fmap上以每个单元（像素）为中心生成锚框anchors。（均匀采样）</li>
<li>因为锚框anchors中x轴和y轴的坐标值分别已除以特征图fmap的宽和高，这些值域在0和1之间的值表达了锚框在特征图中的相对位置。</li>
<li>因为锚框anchors的中心遍布特征图fmap上的所有单元，anchors的中心在任一图像的空间相对位置一定是均匀分布的。（均匀采样就挤来挤去）</li>
<li>具体来说，当特征图的宽和高分别设为fmap_w和fmap_h时，该函数将在任一图像上均匀采样fmap_h行fmap_w列个像素，分别以他们为中心生成大小为s（假设列表长度为1）的不同宽高比的锚框。</li>
</ul>
<p>我们已经在多个尺度上生成了不同大小的锚框，相应地，我们需要在不同尺度下检测不同大小的目标。<br>（卷积神经网络的方法）<br>某个尺度下，假设我们依据ci张形状为hxw的特征图生成hxw组不同中心的锚框，且魅族的锚框个数为a。接下来依据真实边界框的类别和位置，每个锚框将被标注类别和偏移量。在当前的尺度下，目标检测模型需要根据输入图像预测hxw组不同中心的锚框的类别和偏移量（训练）。<br>草<br>本质上，我们用输入图像在某个感受野区域内的信息来预测输入图像上与该区域位置相近的锚框的类别和偏移量。<br>妈的看不懂。<br>当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们将分别用来检测不同大小的目标。例如，我们可以通过设计网络，令较近输出层的特征图中每个单元拥有更广阔的感受野，从而检测输入图像中更大尺寸的目标。</p>
<h3 id="9-6-目标检测数据集（皮卡丘）"><a href="#9-6-目标检测数据集（皮卡丘）" class="headerlink" title="9.6 目标检测数据集（皮卡丘）"></a>9.6 目标检测数据集（皮卡丘）</h3><p>目标检测领域并没有类似MNIST或Fashion-MNIST那样的小数据集。合成一个皮卡丘的数据集，使用了MXNet提供的im2rec工具将图像转换成二进制的RecordIO格式。该格式可以降低数据集在磁盘上的存储开销，又能提高读取效率。自己看GluonCV的包，做广告我去。</p>
<h4 id="9-6-1-获取数据集"><a href="#9-6-1-获取数据集" class="headerlink" title="9.6.1 获取数据集"></a>9.6.1 获取数据集</h4><p>自己下载</p>
<h4 id="9-6-2-读取数据集"><a href="#9-6-2-读取数据集" class="headerlink" title="9.6.2 读取数据集"></a>9.6.2 读取数据集</h4><p>通过创建ImageDetIter实例来读取目标检测数据集。</p>
<ul>
<li>对训练数据集：<ul>
<li>随机读取。由于数据集的格式为RecordIO，我们需要提供图像索引文件train.idx以随机读取小批量。</li>
<li>此外，对于训练集中的每张图像，我们采用随机裁剪，并要求裁剪出的图像至少覆盖每个目标95%，由于裁剪随机，最多尝试200次，不满足条件不裁剪。</li>
</ul>
</li>
<li>对测试数据集：<ul>
<li>不随机裁剪测试数据集</li>
<li>无须按随机顺序读取测试数据集<br>读取一个小批量打印图像和标签的形状。</li>
</ul>
</li>
<li>图像形状：（批量大小，通道数，高，宽）</li>
<li>标签；（批量大小，m，5）</li>
</ul>
<p>m等于数据集中单个图像最多含有的边界框个数。小批量计算虽然高效，但它要求每张图像含有相同数量的边界框一边放在同一个批量中。<br>由于每张图像含有的边界框个数可能不同，我们为边界框个数小于m的图像填充非法边界框，直到每张图像均含有m个边界框。这样，我们就可以每次读取小批量的图像。<br>图像中每个边框的标签由长度为5的数组表示。数组中第一个为边界框所含目标的类别，当值为-1时，该边界框为填充用的非法边界框</p>
<h4 id="9-6-3-图示数据"><a href="#9-6-3-图示数据" class="headerlink" title="9.6.3 图示数据"></a>9.6.3 图示数据</h4><p>画出是个，角度大小和位置每张图不一样</p>
<h3 id="9-7-单发多框检测（SSD）"><a href="#9-7-单发多框检测（SSD）" class="headerlink" title="9.7 单发多框检测（SSD）"></a>9.7 单发多框检测（SSD）</h3><p>前面吐血般学了边界框、锚框、多尺度目标检测和数据集（有些还没看懂），现在基于这些背景只是来狗仔一个目标检测模型——SSD单发多框检测。</p>
<h4 id="9-7-1-定义模型"><a href="#9-7-1-定义模型" class="headerlink" title="9.7.1 定义模型"></a>9.7.1 定义模型</h4><p>概念大概如下，由基础网络快和若干个多尺度特征块串联组成。</p>
<ul>
<li>基础网络块：用来从原始图像中抽取特征，一般采用深度卷积神经网络。<br>（深度卷积神经网络中）SSD中选用了在分类层之间截断的VGG，现在常用ResNet替代，我们可以设计基础网络，使它的宽和高较大，基于特征图生成的锚框数量多检测尺寸较小目标。</li>
<li>多尺度特征块：将上一层高宽缩小，使特征图中每个单元在输入图像上的感受野变宽，用来检测尺寸大目标。<br>因此SSD是一个多尺度的目标检测模型。<br><img src="/2020/01/22/dive-into-deep-learning/ssd.png" alt="dog"></li>
</ul>
<ol>
<li>类别预测层<br>设目标类别个数为q。每个锚框类别个数为q+1（背景0）。某个尺度下，设特征图的高和宽分别为h和w，如果以其中每个单元为中心生成a个锚框，那么我们需要对hwa个锚框分类，全连接层很容易参数过多，使用5.8节NiN的方法，使用卷积层的通道来输出类别预测的方法，降低模型复杂度。</li>
</ol>
<p><strong>具体实现：</strong><br>类别预测层使用一个保持输入高和宽的卷积层。这样，输出和输入在特征图宽和高的空间坐标一一对应。<br>考虑输出和输入同一空间坐标(x, y)：输出特征上也输出(x, y)，输出通道数为a(q+1)（a个锚框），其中索引为i(q+q)+j的通道代表了索引为i的锚框有关类别索引j的预测。</p>
<ol start="2">
<li>边界框预测层<br>与类别预测层差不多，把q+1个类别换成4个。</li>
<li>连结多尺度的预测<br>SSD根据多个尺度下的特征图生成锚框并预测类别和偏移量。由于每个尺度上特征图的形状或以同一单元为中心生成的锚框个数都可能不同，因此不同尺度的预测输出形状可能不同。<br>这些形状不同不连贯。<br>我们对同一批量数据构造两个不同尺度下的特征图Y1和Y2，其中Y2相对于Y1来数高和宽分别减半，锚框个数不同，除了批量大小一样，其他维度都不一样。我们需要将它们变形层统一的格式并将多尺度的预测连结，从而让后续计算更简单。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_pred</span><span class="params">(pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pred.transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)).flatten()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_preds</span><span class="params">(preds)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nd.concat(*[flatten_pred(p) <span class="keyword">for</span> p <span class="keyword">in</span> preds], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>通道维包含中心相同的锚框的预测结果。首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的（批量大小，高x宽x通道数）的格式，方便维度1上的连结。</p>
<ol start="4">
<li>高和宽减半块<br>串联了</li>
</ol>
<ul>
<li>填充为1的3x3卷积层 （不改变特征图的形状）</li>
<li>填充为1和步幅为2的2x2最大池化层 （直接将特征图的高和宽减半，增大感受野更宽阔）</li>
</ul>
<ol start="5">
<li>基础网络块<br>基础网络块用来从<strong>原始图像中抽取特征</strong>。为了计算简洁，我们在这里构造一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍</li>
<li>完整的模型<br>SSD模型一共包含5个模块，每个模块输出的特征图既同来生成锚框，又用来预测这些锚框的类别和偏移量。第一模块为基础网络块，第二模块至地四模块为高和宽减半块，第五模块使用全局最大池化层将高和宽降到1。第二模块至第五模块均为多尺度特征块。<br>定义前向计算：<br>不仅返回卷积计算输出的特征图Y，还返回根据Y生成的当前尺度的锚框，以及基于Y预测的锚框类别和偏移量。<br>我们提到，图中较靠近顶部的多尺度特征块用来检测尺寸较大的目标，因此要确定不同尺度下锚框大小的较小值，再来确定不同尺度下锚框的较大值。<br>最后定义完整的模型TinySSD<h4 id="9-7-2-训练模型"><a href="#9-7-2-训练模型" class="headerlink" title="9.7.2 训练模型"></a>9.7.2 训练模型</h4></li>
<li>读取数据集和初始化<br>前面的皮卡丘数据集，并初始化模型参数并定义优化算法。</li>
<li>定义损失函数和评价函数<br>两个损失：</li>
</ol>
<ul>
<li>有关类别的损失，重用之前图像分类问题里一直使用的交叉熵损失函数</li>
<li>有关证类锚框偏移量的损失。预测偏移量是一个回归问题，但不使用3.1节的平方损失，而使用L1范数损失，即预测值与真实值之间差的绝对值，掩码变量bbox_masks令负类锚框和填充锚框不参与损失计算。</li>
</ul>
<ol start="3">
<li>训练模型<br>跟以往相同，计算预测类别和偏移量再与真实计算损失，再迭代。<h4 id="9-7-3-预测目标"><a href="#9-7-3-预测目标" class="headerlink" title="9.7.3 预测目标"></a>9.7.3 预测目标</h4>也需要自己定义函数：</li>
<li>读取测试图像，将其变换尺寸，然后转成卷积层需要的四维格式。</li>
<li>通过MultiBoxDetection函数根据锚框及其预测偏移量得到预测边界框，并通过非极大值抑制移除相似的预测边界框</li>
<li>最后将置信度不低于0.3的边界框选为最终输出并用以展示。</li>
</ol>
<h3 id="9-8-区域卷积神经网络（R-CNN）系列"><a href="#9-8-区域卷积神经网络（R-CNN）系列" class="headerlink" title="9.8 区域卷积神经网络（R-CNN）系列"></a>9.8 区域卷积神经网络（R-CNN）系列</h3><p>区域卷积神经网络是将深度模型应用与目标检测的开创性工作之一。<br>介绍R-CNN和它的一系列改进方法：</p>
<ul>
<li>R-CNN</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
<li>Mask R-CNN<h4 id="9-8-1-R-CNN"><a href="#9-8-1-R-CNN" class="headerlink" title="9.8.1 R-CNN"></a>9.8.1 R-CNN</h4>R-CNN首先对图像选取若干提议区域并罩住他们的类别和边界框（如偏移量）。然后用卷积神经网络对每个提议区域做前向计算抽取特征。之后对每个提议区域的特征预测类别和边界框。步骤如下：</li>
</ul>
<ol>
<li>对输入图像使用选择性搜索来选取多个高质量的提议区域。这些提议区域通常是多个尺度下选取的，并具有不同的形状和大小并被标注类别和真实边界框。</li>
<li>选取一个与训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向计算输出抽取的提议区域特征。（获取特征再转接到下面去）</li>
<li>将每个提议区域的特征连同其标注标注的类别作为一个样本，训练多个支持向量机对目标分类。其中每个支持向量机用来判断样本是否属于某一个类别。</li>
<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。<br>思路很好通过预训练的卷积神经网络有效提取了图像特征，但主要特点是速度慢。可能提议区域非常多。<h4 id="9-8-2-Fast-R-CNN"><a href="#9-8-2-Fast-R-CNN" class="headerlink" title="9.8.2 Fast R-CNN"></a>9.8.2 Fast R-CNN</h4>R-CNN的主要问题是每个提议区独立抽取特征，由于这些区域通常有大量重叠，独立的特征抽取导致大量的重复计算。Fast R-CNN和R-CNN的一个主要改进在于只对整个图像做卷积神经网络的前向计算。（去掉了提议区域，但有毛病后面还是有提议区域还是慢😂）<br>都是努力学习特征最后用于分类预测。<br>计算步骤如下：</li>
<li>用来提取特征的卷积神经网络的输入是整个图像，而不是各个提议区域，而且，这个网络通常会参与训练，即更新模型参数。设输入一张体香，卷积神经网络的输出形状结尾1xcxh1xw1</li>
<li>假设选择性搜索生成n个提议区域。形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域。这些兴趣区域需要抽出形状形同的特征（假设高和宽均分别分别指定为h2和w2）便于连结后输出。引入个贵复杂要死RoI池化层，最后将卷积神经网络的输出和提议区域输入，输出连结后的各个提议区域标出的兴趣区域所抽取的特征，形状为nxcxh2xw2。</li>
<li>通过全连接层将输出形状变为nxd，其中超参数d取决于模型设计。</li>
<li>预测类别时，将全连接层的输出形状再变换为nxq并使用softmax回归。预测边界框时，将全连接层的输出的形状变换为nx4。</li>
</ol>
<p>RoI池化层和之前介绍的池化层不同。<br>原来池化层中，我们通过设置池化窗口、填充和步幅来控制输出形状。而RoI（兴趣区域池化层）对每个区域的输出形状是可以直接指定的（就是不需要改变那些参数来控制输出），例如，指定每个区域输出的高和宽分别为h2和w2，假设某一RoI窗口的高和宽分别为h和w，该窗口被划分为形状为h2xw2的子窗口网格，且每个子窗口的大小大约为(h/h2)x(w/w2)。任一子窗口的高和宽要取整，其中的最大元素作为该子窗口的输出。<br>因此，兴趣区域池化层可以从形状各异的兴趣区域中均抽取出形状相同的特征。<br>看例子，一看就会。</p>
<h4 id="9-8-3-Faster-R-CNN"><a href="#9-8-3-Faster-R-CNN" class="headerlink" title="9.8.3 Faster R-CNN"></a>9.8.3 Faster R-CNN</h4><p>Fast R-CNN需要选择性搜索中生成较多的提议区域，以获取较精确的目标检测结果。Faster R-CNN提出将选择性搜索替换成<strong>区域建议网络</strong>，从而减少提议区域的生成数量，并保证目标检测的精度。<br><strong>区域提议网络</strong>的计算步骤如下：</p>
<ol>
<li>使用填充为1的3x3卷积层变换卷积神经网络的输出，并将输出通道数记为c。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为c的新特征。</li>
<li>以特征图每个图像为中心，生成多个不同大小的狂高比的锚框并标注它们。</li>
<li>用锚框中心单元长度为c的特征分别预测该锚框的二元类别（含目标还是背景）和边界框。</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即兴趣区域池化层所需要的提议区域。</li>
</ol>
<p>特别的是，区域提议网络作为Faster R-CNN的一部分，是和整个模型一起训练得到的，也是模型参数。也就是Faster R-CNN的目标函数既包括目标检测中的类别和边界框检测，又包括区域提议网络中锚框的热源类别和边界框预测。最后能学习到生成高质量的提议区域，保证在减少提议区域数量也能保证精度。</p>
<h4 id="9-8-4-Mask-R-CNN"><a href="#9-8-4-Mask-R-CNN" class="headerlink" title="9.8.4 Mask R-CNN"></a>9.8.4 Mask R-CNN</h4><p>如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利用这些详尽的标注信息进一步提升目标检测的精度。<br>Mask R-CNN将兴趣区域池化层替换成了兴趣区域对齐层，即通过双线性插值来保留特征图上的空间信息，从而更适用于像素级预测。兴趣区域对齐层的输出<strong>包含了所有兴趣区域的形状相同的特征图</strong>。它们既用来预测兴趣区域的类别和边界框，又通过额外的全卷积网络预测目标的<strong>像素级位置（全卷积网络介绍）</strong>。</p>
<h3 id="9-9-语义分割和数据集"><a href="#9-9-语义分割和数据集" class="headerlink" title="9.9 语义分割和数据集"></a>9.9 语义分割和数据集</h3><p>任务不一样。前面一直用方形边界框来标注和预测图像中的目标。本节探讨语义分割，它关注如何将图像分割成属于不同语义类别的区域。值得一体的是，这些是像素级的。边框更加细致。</p>
<h4 id="9-9-1-图像分割和实例分割"><a href="#9-9-1-图像分割和实例分割" class="headerlink" title="9.9.1 图像分割和实例分割"></a>9.9.1 图像分割和实例分割</h4><p>两个与语义分割相似的重要问题：</p>
<ul>
<li>图像分割：将图像分割成若干组成区域，通常利用图像中像素之间的相关性，但是在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。可能把一个狗分成两部分😂。</li>
<li>实例分割又叫同时检测并分割。它研究如何识别图像中各个目标实例的像素级区域。与语义分割有所不同，实例分割不仅需要区分语义，还要区分不同的目标实例（更难一些）。比如图像中有两只狗，实例分割还需要区分像素属于那一只狗，谁是谁。<h4 id="9-9-2-Pascal-VOC2012语义分割数据集"><a href="#9-9-2-Pascal-VOC2012语义分割数据集" class="headerlink" title="9.9.2 Pascal VOC2012语义分割数据集"></a>9.9.2 Pascal VOC2012语义分割数据集</h4>woc，2GB的训练集<br>数据集特点：</li>
<li>ImageSets/Segmentation路径包含了指定训练和测试样本的文本文件</li>
<li>而JPEGImages和Segmentationlass路径下分别包含了样本的输入图像和标签<br>这里的标签也是<strong>图像格式</strong>，其尺寸和它所标注的输入图像的尺寸相同。标签中颜色相同的像素属于同一个语义类别。 用颜色块块区分做标签。<br>不同的颜色代表不同的类别。</li>
</ul>
<ol>
<li>预处理数据<br>之前通过缩放从图像使符合模型的输入形状，然而在语义分割里面，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射难以做到准确，尤其在不同语义的分割区域。为了避免这个问题，将图像裁剪成固定尺寸而不是缩放，使用图像增广的随机裁剪，裁剪出相同区域。</li>
<li>自定义语义分割数据集类<br>继承Gluon的Dataset类自定义了一个语义分割数据集类VOCSegDataset通过实现<strong>getitem</strong>函数，我们可以任一访问数据集中索引为idx的输入图像及其每个像素的类别索引。还对RGB三个通道值分别做标准化。</li>
<li>读取数据集<br>我们通过自定义的VOCSegDataset类来分别创建训练集和测试集的实例。假设我们指定随机裁剪的输出图像行咋混过为320x480。下面我们可以查看训练集和测试集所保留的样本个数。？<br>未完，下面一节跟着训练网络。</li>
</ol>
<h3 id="9-10-全卷积网络（FCN）"><a href="#9-10-全卷积网络（FCN）" class="headerlink" title="9.10 全卷积网络（FCN）"></a>9.10 全卷积网络（FCN）</h3><p>上一节介绍，我们可以基于语义冯恩对图像中每个像素进行预测。全卷积网络采用卷积神经网络实现了图像从像素到像素类别的变换。<br>与之前介绍的卷积神经网络不同，全卷积神经网络通过转置卷积层将中间层特征图的高和宽变换回输入图像的尺寸，从而令预测结果与输入图像在空间维（高和宽）上一一对应：给定空间维上的位置，通道维的输出即该位置对应像素的类别预测。（大小相同）</p>
<h4 id="9-10-1-转置卷积层"><a href="#9-10-1-转置卷积层" class="headerlink" title="9.10.1 转置卷积层"></a>9.10.1 转置卷积层</h4><p>顾名思义，专置卷积层得名于矩阵的转置操作。事实上，卷积运算还可以通过矩阵乘法来实现（之前是互相关运算近似卷积运算）。<br>例子可以看到卷积运算与乘法运算结果一样，说明可以用乘法替代。<br>从矩阵乘法的角度来描述卷积运算。设输入向量为x，权重矩阵为W，卷积的前向计算函数的实现可以看作将函数输入乘以权重矩阵，并输出向量y=WX。我们直到，反向传播需要依据链式法则，卷积的烦心啊个传播函数的实现可以看作将函数输入乘以转置后的权重矩阵WT。而转置卷积层正好交换了卷积层的前向计算函数与反向传播函数：这两恶搞函数可以看作将函数输入向量分别乘以WT和W。<br>不难想象，转置卷积层可以用来交换卷积层输入和输出的形状。<br><strong>模型设计中，转置卷积层常用于将较小的特征图变换为更大的特征图。在全卷积网络中，当输入是高和宽较小的特征图时，转置卷积层可以用来将高和宽放大到输入图像的尺寸。</strong><br>就像相反操作。<br>有些文献，转置卷积也被称为分数步长卷积。</p>
<h4 id="9-10-2-构造模型"><a href="#9-10-2-构造模型" class="headerlink" title="9.10.2 构造模型"></a>9.10.2 构造模型</h4><p>我去666，先用卷积层提取特征，学会了之后再用转置卷积层变回原来的大小，输出类别预测。<br><img src="/2020/01/22/dive-into-deep-learning/fcn.png" alt="fcn"><br>全卷积网络先试用卷积神经网络抽取图像特征，然后通过1x1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。模型输出与输入图像的高和宽相同，并在空间位置上一一对应。<br>后面再说，我去要看也看得懂。</p>
<h4 id="9-10-3-初始化转置卷积层"><a href="#9-10-3-初始化转置卷积层" class="headerlink" title="9.10.3 初始化转置卷积层"></a>9.10.3 初始化转置卷积层</h4><h4 id="9-10-4-读取数据集"><a href="#9-10-4-读取数据集" class="headerlink" title="9.10.4 读取数据集"></a>9.10.4 读取数据集</h4><h4 id="9-10-5-训练模型"><a href="#9-10-5-训练模型" class="headerlink" title="9.10.5 训练模型"></a>9.10.5 训练模型</h4><h4 id="9-10-6-预测像素类型"><a href="#9-10-6-预测像素类型" class="headerlink" title="9.10.6 预测像素类型"></a>9.10.6 预测像素类型</h4><h3 id="9-11-样式迁移"><a href="#9-11-样式迁移" class="headerlink" title="9.11 样式迁移"></a>9.11 样式迁移</h3><p>样式迁移，贼酷 奥利给！！！<br>存在滤镜，他能改变照片的颜色样式，要实现好的效果需要大量的尝试不同的组合，复杂程度不亚于模型调参。<br>本节中，我们介绍使用卷积神经网络将某图像中的样式应用过在另一张图像之上，即样式迁移。我们输入两张输入图像，一张是内容图像，另一张是样式图像。利用神经网络修改图像内容使其在样式上更接近。</p>
<h4 id="9-11-1-方法"><a href="#9-11-1-方法" class="headerlink" title="9.11.1 方法"></a>9.11.1 方法</h4><h4 id="9-11-2-读取内容图像和样式图像"><a href="#9-11-2-读取内容图像和样式图像" class="headerlink" title="9.11.2 读取内容图像和样式图像"></a>9.11.2 读取内容图像和样式图像</h4><h4 id="9-11-3-预处理和后处理图像"><a href="#9-11-3-预处理和后处理图像" class="headerlink" title="9.11.3 预处理和后处理图像"></a>9.11.3 预处理和后处理图像</h4><h4 id="9-11-4-抽取特征"><a href="#9-11-4-抽取特征" class="headerlink" title="9.11.4 抽取特征"></a>9.11.4 抽取特征</h4><h4 id="9-11-5-定义损失函数"><a href="#9-11-5-定义损失函数" class="headerlink" title="9.11.5 定义损失函数"></a>9.11.5 定义损失函数</h4><ol>
<li>内容损失</li>
<li>样式损失</li>
<li>总变差损失</li>
<li>损失函数<h4 id="9-11-6-创建和初始化合成图像"><a href="#9-11-6-创建和初始化合成图像" class="headerlink" title="9.11.6 创建和初始化合成图像"></a>9.11.6 创建和初始化合成图像</h4><h4 id="9-11-7-训练模型"><a href="#9-11-7-训练模型" class="headerlink" title="9.11.7 训练模型"></a>9.11.7 训练模型</h4></li>
</ol>
<p>9.12 实战Kaggle比赛：图像分类（CIFAR-10）<br>9.12.1 读取和整理数据集</p>
<ol>
<li>下载数据集</li>
<li>解压数据集</li>
<li>整理数据集</li>
<li>12.2 图像增广</li>
<li>12.3 读取数据集</li>
<li>12.4 定义模型</li>
<li>12.5 定义训练函数</li>
<li>12.6 训练模型</li>
<li>12.7 对测试集分类并在Kaggle提交结果</li>
<li>13 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）</li>
<li>13.1 读取和整理数据集</li>
<li>下载数据集</li>
<li>整理数据集</li>
<li>13.2 图像增广</li>
<li>13.3 读取数据集</li>
<li>13.4 定义模型</li>
<li>13.5 定义训练函数</li>
<li>13.6  训练函数</li>
<li>13.7 对测试集分类并在kaggle提交结果</li>
</ol>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/12/06/前端开发总结/" rel="next" title="前端开发总结">
                  <i class="fa fa-chevron-left"></i> 前端开发总结
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/02/05/Summary-of-Pytorch-and-OpenCV/" rel="prev" title="Summary of Pytorch and OpenCV">
                  Summary of Pytorch and OpenCV <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#动手学深度学习"><span class="nav-number">1.</span> <span class="nav-text">动手学深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#第2章-预备知识"><span class="nav-number">1.1.</span> <span class="nav-text">第2章 预备知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-数据操作"><span class="nav-number">1.1.1.</span> <span class="nav-text">2.2 数据操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-自动求梯度"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.3 自动求梯度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第3章-深度学习基础"><span class="nav-number">1.2.</span> <span class="nav-text">第3章 深度学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-线性回归-（全概念）"><span class="nav-number">1.2.1.</span> <span class="nav-text">3.1 线性回归 （全概念）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-线性回归的基本要素"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">3.1.1 线性回归的基本要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-线性回归的表示方法"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">3.1.2 线性回归的表示方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-线性回归的从零开始实现"><span class="nav-number">1.2.2.</span> <span class="nav-text">3.2 线性回归的从零开始实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-生成数据集"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">3.2.1 生成数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-读取数据集"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">3.2.2 读取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-初始化模型参数"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">3.2.3 初始化模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-4-定义模型"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">3.2.4 定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-5-定义损失函数"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">3.2.5 定义损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-定义优化算法"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">3.2.6 定义优化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-训练模型"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">3.2.7 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-线性回归的简洁实现"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.3 线性回归的简洁实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-生成数据集"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">3.3.1 生成数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-读取数据集"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">3.3.2 读取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-定义模型"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">3.3.3 定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-4-初始化模型参数"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">3.3.4 初始化模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-5-定义损失函数"><span class="nav-number">1.2.3.5.</span> <span class="nav-text">3.3.5 定义损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-6-定义优化算法"><span class="nav-number">1.2.3.6.</span> <span class="nav-text">3.3.6 定义优化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-7-训练模型"><span class="nav-number">1.2.3.7.</span> <span class="nav-text">3.3.7 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-softmax回归"><span class="nav-number">1.2.4.</span> <span class="nav-text">3.4 softmax回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-分类问题"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">3.4.1 分类问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-softmax回归模型"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">3.4.2 softmax回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-单样本样本分类的矢量计算表达式"><span class="nav-number">1.2.4.3.</span> <span class="nav-text">3.4.3 单样本样本分类的矢量计算表达式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-4-小批量分类的矢量计算表达式"><span class="nav-number">1.2.4.4.</span> <span class="nav-text">3.4.4 小批量分类的矢量计算表达式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-5-交叉熵损失函数"><span class="nav-number">1.2.4.5.</span> <span class="nav-text">3.4.5 交叉熵损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-6-模型预测及评价"><span class="nav-number">1.2.4.6.</span> <span class="nav-text">3.4.6 模型预测及评价</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-图像分类数据集（Fashion-MNIST）"><span class="nav-number">1.2.5.</span> <span class="nav-text">3.5 图像分类数据集（Fashion-MNIST）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-1-获取数据集"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">3.5.1 获取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-获取小批量"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">3.5.2 获取小批量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-softmax回归的从零开始实现"><span class="nav-number">1.2.6.</span> <span class="nav-text">3.6 softmax回归的从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-7-softmax回归的简洁实现"><span class="nav-number">1.2.7.</span> <span class="nav-text">3.7 softmax回归的简洁实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-8-多层感知机"><span class="nav-number">1.2.8.</span> <span class="nav-text">3.8 多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-8-1-隐藏层"><span class="nav-number">1.2.8.1.</span> <span class="nav-text">3.8.1 隐藏层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-8-2-激活函数"><span class="nav-number">1.2.8.2.</span> <span class="nav-text">3.8.2 激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-8-3-多层感知机"><span class="nav-number">1.2.8.3.</span> <span class="nav-text">3.8.3 多层感知机</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-9-多层感知机的从零开始实现"><span class="nav-number">1.2.9.</span> <span class="nav-text">3.9 多层感知机的从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-10-多层感知机的简洁实现"><span class="nav-number">1.2.10.</span> <span class="nav-text">3.10 多层感知机的简洁实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-11-模型选择、欠拟合和过拟合"><span class="nav-number">1.2.11.</span> <span class="nav-text">3.11 模型选择、欠拟合和过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-1-训练误差和泛化误差"><span class="nav-number">1.2.11.1.</span> <span class="nav-text">3.11.1 训练误差和泛化误差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-2-模型选择"><span class="nav-number">1.2.11.2.</span> <span class="nav-text">3.11.2 模型选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-3-欠拟合和过拟合"><span class="nav-number">1.2.11.3.</span> <span class="nav-text">3.11.3 欠拟合和过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-4-多项式函数拟合实验"><span class="nav-number">1.2.11.4.</span> <span class="nav-text">3.11.4 多项式函数拟合实验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-12-权重衰减"><span class="nav-number">1.2.12.</span> <span class="nav-text">3.12 权重衰减</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-12-1-方法"><span class="nav-number">1.2.12.1.</span> <span class="nav-text">3.12.1 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-12-2-高维线性回归实验"><span class="nav-number">1.2.12.2.</span> <span class="nav-text">3.12.2 高维线性回归实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-12-3-从零开始实现"><span class="nav-number">1.2.12.3.</span> <span class="nav-text">3.12.3 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-12-4-简洁实现"><span class="nav-number">1.2.12.4.</span> <span class="nav-text">3.12.4 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-13-丢弃法"><span class="nav-number">1.2.13.</span> <span class="nav-text">3.13 丢弃法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-14-正向传播、反向传播和计算图"><span class="nav-number">1.2.14.</span> <span class="nav-text">3.14 正向传播、反向传播和计算图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-15-数值稳定性和模型初始化"><span class="nav-number">1.2.15.</span> <span class="nav-text">3.15 数值稳定性和模型初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-16-实战Kaggle比赛：房价预测"><span class="nav-number">1.2.16.</span> <span class="nav-text">3.16 实战Kaggle比赛：房价预测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第4章-深度学习计算"><span class="nav-number">1.3.</span> <span class="nav-text">第4章 深度学习计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-模型构造"><span class="nav-number">1.3.1.</span> <span class="nav-text">4.1 模型构造</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-继承Block类来构造模型"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">4.1.1 继承Block类来构造模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-Sequential类继承自Block类"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">4.1.2 Sequential类继承自Block类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-3-构造复杂的模型"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">4.1.3 构造复杂的模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-模型参数的访问、初始化和共享"><span class="nav-number">1.3.2.</span> <span class="nav-text">4.2 模型参数的访问、初始化和共享</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-访问模型参数"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">4.2.1 访问模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-初始化模型参数"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">4.2.2 初始化模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-3-自定义初始化方法"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">4.2.3 自定义初始化方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-4-共享模型参数"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">4.2.4 共享模型参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-模型参数的延后初始化"><span class="nav-number">1.3.3.</span> <span class="nav-text">4.3 模型参数的延后初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-自定义层"><span class="nav-number">1.3.4.</span> <span class="nav-text">4.4 自定义层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-不含模型参数的自定义层"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">4.4.1 不含模型参数的自定义层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-含模型参数的自定义层"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">4.4.2 含模型参数的自定义层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-读取和存储"><span class="nav-number">1.3.5.</span> <span class="nav-text">4.5 读取和存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-1-读写NDArry"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">4.5.1 读写NDArry</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-2-读写Gluon模型的参数"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">4.5.2 读写Gluon模型的参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-GPU计算"><span class="nav-number">1.3.6.</span> <span class="nav-text">4.6 GPU计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第5章-卷积神经网络"><span class="nav-number">1.4.</span> <span class="nav-text">第5章 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-二维卷积层"><span class="nav-number">1.4.1.</span> <span class="nav-text">5.1 二维卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-二维互相关运算"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">5.1.1 二维互相关运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-二维卷积层"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">5.1.2 二维卷积层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-3-图像中物体边缘检测"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">5.1.3 图像中物体边缘检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-4-通过数据学习核数组"><span class="nav-number">1.4.1.4.</span> <span class="nav-text">5.1.4 通过数据学习核数组</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-5-互相关运算和卷积运算"><span class="nav-number">1.4.1.5.</span> <span class="nav-text">5.1.5 互相关运算和卷积运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-6-特征图和感受野"><span class="nav-number">1.4.1.6.</span> <span class="nav-text">5.1.6 特征图和感受野</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-填充和步幅"><span class="nav-number">1.4.2.</span> <span class="nav-text">5.2 填充和步幅</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-填充"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">5.2.1 填充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-步幅"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">5.2.2 步幅</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-多输入通道和多输出通道"><span class="nav-number">1.4.3.</span> <span class="nav-text">5.3 多输入通道和多输出通道</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-1-多输入通道"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">5.3.1 多输入通道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-2-多输出通道"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">5.3.2 多输出通道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-3-1-x-1-卷积层"><span class="nav-number">1.4.3.3.</span> <span class="nav-text">5.3.3 1 x 1 卷积层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-池化层"><span class="nav-number">1.4.4.</span> <span class="nav-text">5.4 池化层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-1-二维最大池化层平均池化层"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">5.4.1 二维最大池化层平均池化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-2-填充和步幅"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">5.4.2 填充和步幅</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-3-多通道"><span class="nav-number">1.4.4.3.</span> <span class="nav-text">5.4.3 多通道</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-卷积神经网络（LeNet）"><span class="nav-number">1.4.5.</span> <span class="nav-text">5.5 卷积神经网络（LeNet）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-1-LeNet模型"><span class="nav-number">1.4.5.1.</span> <span class="nav-text">5.5.1 LeNet模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-2-训练模型"><span class="nav-number">1.4.5.2.</span> <span class="nav-text">5.5.2 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-深度卷积神经网络（AlexNet）"><span class="nav-number">1.4.6.</span> <span class="nav-text">5.6 深度卷积神经网络（AlexNet）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-1-学习特征表示"><span class="nav-number">1.4.6.1.</span> <span class="nav-text">5.6.1 学习特征表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-2-AlexNet"><span class="nav-number">1.4.6.2.</span> <span class="nav-text">5.6.2 AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-3-读取数据集"><span class="nav-number">1.4.6.3.</span> <span class="nav-text">5.6.3 读取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-4-训练模型"><span class="nav-number">1.4.6.4.</span> <span class="nav-text">5.6.4 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-7-使用重复元素的网络（VGG）"><span class="nav-number">1.4.7.</span> <span class="nav-text">5.7 使用重复元素的网络（VGG）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-1-VGG块"><span class="nav-number">1.4.7.1.</span> <span class="nav-text">5.7.1 VGG块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-2-VGG网络"><span class="nav-number">1.4.7.2.</span> <span class="nav-text">5.7.2 VGG网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-3-训练模型"><span class="nav-number">1.4.7.3.</span> <span class="nav-text">5.7.3 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-8-网络中的网络（NiN）"><span class="nav-number">1.4.8.</span> <span class="nav-text">5.8 网络中的网络（NiN）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-1-NiN块"><span class="nav-number">1.4.8.1.</span> <span class="nav-text">5.8.1 NiN块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-2-NiN模型"><span class="nav-number">1.4.8.2.</span> <span class="nav-text">5.8.2 NiN模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-3-训练模型"><span class="nav-number">1.4.8.3.</span> <span class="nav-text">5.8.3 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-9-含并行连结的网络（GoogLeNet）"><span class="nav-number">1.4.9.</span> <span class="nav-text">5.9 含并行连结的网络（GoogLeNet）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-1-Inception块"><span class="nav-number">1.4.9.1.</span> <span class="nav-text">5.9.1 Inception块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-2-GoogLeNet模型"><span class="nav-number">1.4.9.2.</span> <span class="nav-text">5.9.2 GoogLeNet模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-3-训练模型"><span class="nav-number">1.4.9.3.</span> <span class="nav-text">5.9.3 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-10-批量归一化"><span class="nav-number">1.4.10.</span> <span class="nav-text">5.10 批量归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-10-1-批量归一化层"><span class="nav-number">1.4.10.1.</span> <span class="nav-text">5.10.1 批量归一化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-10-2-从零开始实现"><span class="nav-number">1.4.10.2.</span> <span class="nav-text">5.10.2 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-10-3-使用批量归一化层的的LeNet"><span class="nav-number">1.4.10.3.</span> <span class="nav-text">5.10.3 使用批量归一化层的的LeNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-10-4-简洁实现"><span class="nav-number">1.4.10.4.</span> <span class="nav-text">5.10.4 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-11-残差网络（ResNet）"><span class="nav-number">1.4.11.</span> <span class="nav-text">5.11 残差网络（ResNet）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-11-1-残差块"><span class="nav-number">1.4.11.1.</span> <span class="nav-text">5.11.1 残差块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-11-2-ResNet模块"><span class="nav-number">1.4.11.2.</span> <span class="nav-text">5.11.2 ResNet模块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-11-3-训练模型"><span class="nav-number">1.4.11.3.</span> <span class="nav-text">5.11.3 训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-12-稠密连接网络（DenseNet）"><span class="nav-number">1.4.12.</span> <span class="nav-text">5.12 稠密连接网络（DenseNet）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-12-1-稠密块"><span class="nav-number">1.4.12.1.</span> <span class="nav-text">5.12.1 稠密块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-12-2-过渡层"><span class="nav-number">1.4.12.2.</span> <span class="nav-text">5.12.2 过渡层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-12-3-DenseNet模型"><span class="nav-number">1.4.12.3.</span> <span class="nav-text">5.12.3 DenseNet模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-12-4-训练模型"><span class="nav-number">1.4.12.4.</span> <span class="nav-text">5.12.4 训练模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第7章-优化算法"><span class="nav-number">1.5.</span> <span class="nav-text">第7章 优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-优化与深度学习"><span class="nav-number">1.5.1.</span> <span class="nav-text">7.1 优化与深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-1-优化与深度学习的关系"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">7.1.1 优化与深度学习的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-2-优化在深度学习中的挑战"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">7.1.2 优化在深度学习中的挑战</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-梯度下降和随机梯度下降"><span class="nav-number">1.5.2.</span> <span class="nav-text">7.2 梯度下降和随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-一维梯度下降"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">7.2.1 一维梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-学习率"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">7.2.2 学习率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-3-多维梯度下降"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">7.2.3 多维梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-4-随机梯度下降"><span class="nav-number">1.5.2.4.</span> <span class="nav-text">7.2.4 随机梯度下降</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-小批量随机梯度下降"><span class="nav-number">1.5.3.</span> <span class="nav-text">7.3 小批量随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-1-读取数据集"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">7.3.1 读取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-2-从零开始实现"><span class="nav-number">1.5.3.2.</span> <span class="nav-text">7.3.2 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-3-简洁实现"><span class="nav-number">1.5.3.3.</span> <span class="nav-text">7.3.3 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-动量法"><span class="nav-number">1.5.4.</span> <span class="nav-text">7.4 动量法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-1-梯度下降的问题"><span class="nav-number">1.5.4.1.</span> <span class="nav-text">7.4.1 梯度下降的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-2-动量法"><span class="nav-number">1.5.4.2.</span> <span class="nav-text">7.4.2 动量法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-3-从零开始实现"><span class="nav-number">1.5.4.3.</span> <span class="nav-text">7.4.3 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-4-简洁实现"><span class="nav-number">1.5.4.4.</span> <span class="nav-text">7.4.4 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-AdaGrad算法"><span class="nav-number">1.5.5.</span> <span class="nav-text">7.5 AdaGrad算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-1-算法"><span class="nav-number">1.5.5.1.</span> <span class="nav-text">7.5.1 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-2-特点"><span class="nav-number">1.5.5.2.</span> <span class="nav-text">7.5.2 特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-3-从零开始实现"><span class="nav-number">1.5.5.3.</span> <span class="nav-text">7.5.3 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-4-简洁实现"><span class="nav-number">1.5.5.4.</span> <span class="nav-text">7.5.4 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-RMSProp算法"><span class="nav-number">1.5.6.</span> <span class="nav-text">7.6 RMSProp算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-6-1-算法"><span class="nav-number">1.5.6.1.</span> <span class="nav-text">7.6.1 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-6-2-从零开始实现"><span class="nav-number">1.5.6.2.</span> <span class="nav-text">7.6.2 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-6-3-简洁实现"><span class="nav-number">1.5.6.3.</span> <span class="nav-text">7,6,3 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-AdaDelta算法"><span class="nav-number">1.5.7.</span> <span class="nav-text">7.7 AdaDelta算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-7-1-算法"><span class="nav-number">1.5.7.1.</span> <span class="nav-text">7.7.1 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-7-2-从零开始实现"><span class="nav-number">1.5.7.2.</span> <span class="nav-text">7.7.2 从零开始实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-7-3-简洁实现"><span class="nav-number">1.5.7.3.</span> <span class="nav-text">7.7.3 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-8-Adam算法"><span class="nav-number">1.5.8.</span> <span class="nav-text">7.8 Adam算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-8-1-算法"><span class="nav-number">1.5.8.1.</span> <span class="nav-text">7.8.1 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-8-2-从零开始实现"><span class="nav-number">1.5.8.2.</span> <span class="nav-text">7.8.2 从零开始实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第8章-计算性能"><span class="nav-number">1.6.</span> <span class="nav-text">第8章 计算性能</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-命令式和符号式混合编程"><span class="nav-number">1.6.1.</span> <span class="nav-text">8.1 命令式和符号式混合编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-1-混合式编程取两者之长"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">8.1.1 混合式编程取两者之长</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-2-使用HybridSequential类构造模型"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">8.1.2 使用HybridSequential类构造模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-3-使用HybridBlock类构造模型"><span class="nav-number">1.6.1.3.</span> <span class="nav-text">8.1.3 使用HybridBlock类构造模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-异步计算"><span class="nav-number">1.6.2.</span> <span class="nav-text">8.2 异步计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-1-MXNet中的异步计算"><span class="nav-number">1.6.2.1.</span> <span class="nav-text">8.2.1 MXNet中的异步计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-2-用同步函数让前端等待计算结果"><span class="nav-number">1.6.2.2.</span> <span class="nav-text">8.2.2 用同步函数让前端等待计算结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-3-使用异步计算提升计算性能"><span class="nav-number">1.6.2.3.</span> <span class="nav-text">8.2.3 使用异步计算提升计算性能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-4-异步计算对内存的影响"><span class="nav-number">1.6.2.4.</span> <span class="nav-text">8.2.4 异步计算对内存的影响</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-自动并行计算"><span class="nav-number">1.6.3.</span> <span class="nav-text">8.3 自动并行计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-1-CPU和GPU的并行计算"><span class="nav-number">1.6.3.1.</span> <span class="nav-text">8.3.1 CPU和GPU的并行计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-2-计算和通信的并行计算"><span class="nav-number">1.6.3.2.</span> <span class="nav-text">8.3.2 计算和通信的并行计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-多GPU计算"><span class="nav-number">1.6.4.</span> <span class="nav-text">8.4 多GPU计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-1-数据并行"><span class="nav-number">1.6.4.1.</span> <span class="nav-text">8.4.1 数据并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-2-定义模型"><span class="nav-number">1.6.4.2.</span> <span class="nav-text">8.4.2 定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-3-多GPU之间同步数据"><span class="nav-number">1.6.4.3.</span> <span class="nav-text">8.4.3 多GPU之间同步数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-4-单个小批量上的多GPU训练"><span class="nav-number">1.6.4.4.</span> <span class="nav-text">8.4.4 单个小批量上的多GPU训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-5-定义训练函数"><span class="nav-number">1.6.4.5.</span> <span class="nav-text">8.4.5 定义训练函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-6-多GPU训练实验"><span class="nav-number">1.6.4.6.</span> <span class="nav-text">8.4.6 多GPU训练实验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-多GPU计算的简洁实现"><span class="nav-number">1.6.5.</span> <span class="nav-text">8.5 多GPU计算的简洁实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-1-多GPU上初始化模型参数"><span class="nav-number">1.6.5.1.</span> <span class="nav-text">8.5.1 多GPU上初始化模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-5-2-多GPU训练模型"><span class="nav-number">1.6.5.2.</span> <span class="nav-text">8.5.2 多GPU训练模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第9章-计算机视觉"><span class="nav-number">1.7.</span> <span class="nav-text">第9章 计算机视觉</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-图像增广"><span class="nav-number">1.7.1.</span> <span class="nav-text">9.1 图像增广</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-1-常用的图像增广方法"><span class="nav-number">1.7.1.1.</span> <span class="nav-text">9.1.1 常用的图像增广方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-2-使用图像增广训练模型"><span class="nav-number">1.7.1.2.</span> <span class="nav-text">9.1.2 使用图像增广训练模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-微调"><span class="nav-number">1.7.2.</span> <span class="nav-text">9.2 微调</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#热狗识别"><span class="nav-number">1.7.2.1.</span> <span class="nav-text">热狗识别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-目标检测和边界框"><span class="nav-number">1.7.3.</span> <span class="nav-text">9.3 目标检测和边界框</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-锚框"><span class="nav-number">1.7.4.</span> <span class="nav-text">9.4 锚框</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-1-生成多个锚框"><span class="nav-number">1.7.4.1.</span> <span class="nav-text">9.4.1 生成多个锚框</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-2-交并比"><span class="nav-number">1.7.4.2.</span> <span class="nav-text">9.4.2 交并比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-3-标注训练集的锚框"><span class="nav-number">1.7.4.3.</span> <span class="nav-text">9.4.3 标注训练集的锚框</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-4-输出预测边界框"><span class="nav-number">1.7.4.4.</span> <span class="nav-text">9.4.4 输出预测边界框</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-多尺度目标检测"><span class="nav-number">1.7.5.</span> <span class="nav-text">9.5 多尺度目标检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-6-目标检测数据集（皮卡丘）"><span class="nav-number">1.7.6.</span> <span class="nav-text">9.6 目标检测数据集（皮卡丘）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-6-1-获取数据集"><span class="nav-number">1.7.6.1.</span> <span class="nav-text">9.6.1 获取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-6-2-读取数据集"><span class="nav-number">1.7.6.2.</span> <span class="nav-text">9.6.2 读取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-6-3-图示数据"><span class="nav-number">1.7.6.3.</span> <span class="nav-text">9.6.3 图示数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-7-单发多框检测（SSD）"><span class="nav-number">1.7.7.</span> <span class="nav-text">9.7 单发多框检测（SSD）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-7-1-定义模型"><span class="nav-number">1.7.7.1.</span> <span class="nav-text">9.7.1 定义模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-7-2-训练模型"><span class="nav-number">1.7.7.2.</span> <span class="nav-text">9.7.2 训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-7-3-预测目标"><span class="nav-number">1.7.7.3.</span> <span class="nav-text">9.7.3 预测目标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-8-区域卷积神经网络（R-CNN）系列"><span class="nav-number">1.7.8.</span> <span class="nav-text">9.8 区域卷积神经网络（R-CNN）系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-8-1-R-CNN"><span class="nav-number">1.7.8.1.</span> <span class="nav-text">9.8.1 R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-8-2-Fast-R-CNN"><span class="nav-number">1.7.8.2.</span> <span class="nav-text">9.8.2 Fast R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-8-3-Faster-R-CNN"><span class="nav-number">1.7.8.3.</span> <span class="nav-text">9.8.3 Faster R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-8-4-Mask-R-CNN"><span class="nav-number">1.7.8.4.</span> <span class="nav-text">9.8.4 Mask R-CNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-9-语义分割和数据集"><span class="nav-number">1.7.9.</span> <span class="nav-text">9.9 语义分割和数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-9-1-图像分割和实例分割"><span class="nav-number">1.7.9.1.</span> <span class="nav-text">9.9.1 图像分割和实例分割</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-9-2-Pascal-VOC2012语义分割数据集"><span class="nav-number">1.7.9.2.</span> <span class="nav-text">9.9.2 Pascal VOC2012语义分割数据集</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-10-全卷积网络（FCN）"><span class="nav-number">1.7.10.</span> <span class="nav-text">9.10 全卷积网络（FCN）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-10-1-转置卷积层"><span class="nav-number">1.7.10.1.</span> <span class="nav-text">9.10.1 转置卷积层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-10-2-构造模型"><span class="nav-number">1.7.10.2.</span> <span class="nav-text">9.10.2 构造模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-10-3-初始化转置卷积层"><span class="nav-number">1.7.10.3.</span> <span class="nav-text">9.10.3 初始化转置卷积层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-10-4-读取数据集"><span class="nav-number">1.7.10.4.</span> <span class="nav-text">9.10.4 读取数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-10-5-训练模型"><span class="nav-number">1.7.10.5.</span> <span class="nav-text">9.10.5 训练模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-10-6-预测像素类型"><span class="nav-number">1.7.10.6.</span> <span class="nav-text">9.10.6 预测像素类型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-11-样式迁移"><span class="nav-number">1.7.11.</span> <span class="nav-text">9.11 样式迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-1-方法"><span class="nav-number">1.7.11.1.</span> <span class="nav-text">9.11.1 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-2-读取内容图像和样式图像"><span class="nav-number">1.7.11.2.</span> <span class="nav-text">9.11.2 读取内容图像和样式图像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-3-预处理和后处理图像"><span class="nav-number">1.7.11.3.</span> <span class="nav-text">9.11.3 预处理和后处理图像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-4-抽取特征"><span class="nav-number">1.7.11.4.</span> <span class="nav-text">9.11.4 抽取特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-5-定义损失函数"><span class="nav-number">1.7.11.5.</span> <span class="nav-text">9.11.5 定义损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-6-创建和初始化合成图像"><span class="nav-number">1.7.11.6.</span> <span class="nav-text">9.11.6 创建和初始化合成图像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-11-7-训练模型"><span class="nav-number">1.7.11.7.</span> <span class="nav-text">9.11.7 训练模型</span></a></li></ol></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1570163730249&di=dcd36b04d1066a90ddb1f132ae3a6bcc&imgtype=0&src=http%3A%2F%2Fhbimg.b0.upaiyun.com%2Ffe60497fd762440686b6d5702c2c9f19df71fb9911009-LVWEJj_fw658"
      alt="望星的太阳花">
  <p class="site-author-name" itemprop="name">望星的太阳花</p>
  <div class="site-description" itemprop="description">You are my JavaSript in my HTML.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">望星的太阳花</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.4.1</div>

        












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/muse.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  

  

</body>
</html>
